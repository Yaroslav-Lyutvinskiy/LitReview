{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dab2d10b-32ff-422e-976c-772ca73023d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from IPython.display import display, Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae8a4f5a-6560-40c9-ae53-c296d36a1a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Make a search of 3 recent publications and write a literature review on recent advances in deep learning optimizers with an accent on novel optimization algorythms\"\n",
    "params = {\"query\":query}\n",
    "url = \"http://localhost:8086/agent\"\n",
    "response = requests.get(url, params = params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98cfc5d9-efeb-498c-96e3-b6678f87a3fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "600e7239-7f7d-491c-a0fa-4a9765beec9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = json.loads(response.content)[\"message\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b683d8b-fe84-443e-a058-710311cb9e6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Literature Review on Recent Advances in Deep Learning Optimizers with Emphasis on Novel Optimization Algorithms\n",
       "\n",
       "1. Introduction\n",
       "\n",
       "Deep learning (DL) has dramatically transformed a range of scientific and engineering disciplines by enabling the modeling of complex, high-dimensional functions. At the heart of these successes lie optimization algorithms that drive the training of deep neural networks. While standard techniques—most notably stochastic gradient descent (SGD) and its variants—have been extremely successful, recent advances in both theory and practice have spurred the development of novel optimization algorithms. These advances often aim to address challenges such as the high computational cost of solving partial differential equation (PDE) constrained problems, improving the stability and convergence of training in highly nonconvex landscapes, and mitigating issues such as zero-gradient regions in uncertainty-aware models. In this literature review, we synthesize key contributions from three recent publications that exemplify this trend: (1) an active-learning–based algorithm for surrogate modeling in PDE constrained optimization, (2) a modern mathematical analysis of deep learning optimizers and the associated loss landscapes, and (3) novel regularization techniques for evidential deep learning that help overcome training deficiencies in uncertainty quantification.\n",
       "\n",
       "2. Active Learning and Surrogate-Based Optimization in PDE-Constrained Problems\n",
       "\n",
       "Lye et al. (2020) propose a novel active learning algorithm, termed Iterative Surrogate Model Optimization (ISMO), which is designed to improve the computational efficiency and robustness of solving PDE constrained optimization problems. Traditional surrogate models rely on fixed, a priori sampling of training data, which, while effective for global approximation, often fail to capture the fine-scale behavior near the optimal region. The ISMO algorithm overcomes this limitation by adopting an iterative strategy that actively refines the training dataset: after constructing an initial deep neural network (DNN) surrogate, the algorithm employs a standard gradient-based optimizer to locate approximate minimizers. These minimizers are then incorporated back into the training set, focusing the surrogate’s accuracy on regions of the design space that are most relevant to the optimization goal.\n",
       "\n",
       "The theoretical analysis provided by Lye et al. (2020) shows that while traditional methods achieve error decay at an algebraic rate with increasing training samples, the ISMO algorithm obtains an exponential decay in error and variance under appropriate regularity and convexity assumptions. The numerical experiments—including applications in projectile control for optimal design and the shape optimization of airfoils—demonstrate that ISMO not only converges more rapidly but also delivers a significant reduction in computational cost when PDE solves are expensive. This work underscores how integrating active learning with surrogate modeling can yield novel optimizers, particularly for high-dimensional, PDE-constrained scenarios.\n",
       "\n",
       "3. Mathematical Analysis of Deep Learning Optimization\n",
       "\n",
       "Another critical paradigm shift in deep learning optimization has been the rigorous mathematical quantification of loss landscapes, as explored by Berner, Grohs, Kutyniok, and Petersen (2021) in their review “The Modern Mathematics of Deep Learning.” This work focuses on addressing several open questions: Why do overparameterized neural networks generalize so well? What is the role of network depth in shaping the geometry of the loss function? And how can we reconcile the nonconvexity of deep learning loss surfaces with the observed success of gradient-based optimizers?\n",
       "\n",
       "Berner et al. (2021) outline several modern approaches that offer partial answers. For instance, they discuss statistical mechanics–inspired techniques where the loss surface is compared with the Hamiltonian of spin glass models. In this analogy, the global minima are associated with nearly flat regions exhibiting low Hessian index, a fact which helps explain why SGD often finds high-quality solutions despite the existence of numerous saddle points and high-index critical points. Furthermore, the overview touches upon the concept of “lazy training,” an analysis framework in which highly overparameterized networks operate almost linearly in a local neighborhood around their initialization. In the infinite-width limit, connections are drawn to the neural tangent kernel (NTK) framework. This perspective not only provides rigorous exponential convergence guarantees for gradient descent (provided that the NTK matrix is positive definite) but also offers insights into the implicit regularization effects inherent in SGD.\n",
       "\n",
       "These mathematical perspectives encourage rethinking optimizer design by suggesting that explicit curvature adjustments—such as those accomplished via batch normalization or other gradient-smoothing techniques—can further stabilize and accelerate training. In essence, the work of Berner et al. (2021) complements the development of novel optimizers by establishing a theoretical foundation that explains why current methods perform so effectively and inspires new algorithms that leverage the specific geometry of deep learning loss landscapes.\n",
       "\n",
       "4. Addressing Gradient Flow in Evidential Deep Learning\n",
       "\n",
       "A third important development in the field of deep learning optimization concerns evidential deep learning—a framework that seeks to endow deterministic neural networks with the ability to quantify uncertainty in a principled manner using evidence theory and subjective logic. Pandey and Yu (2023) investigate a critical challenge in this domain: many conventional evidential activation functions (e.g., ReLU, SoftPlus) inadvertently create regions of the output space where the network produces zero evidence. When this happens, the gradients of the loss with respect to the network parameters vanish, leading to a learning deficiency in which certain training samples do not contribute to parameter updates. This problem is especially significant when scaling to large, complex datasets.\n",
       "\n",
       "Pandey and Yu (2023) present a theoretical analysis that identifies the “zero-evidence” regions as the root cause of inferior performance in evidential models. Their work demonstrates that activations such as ReLU and SoftPlus often result in near-zero gradients, thereby arresting the learning process for inputs that fall into these regions. In response, the authors propose two synergistic improvements. First, they advocate for the use of the exponential activation function, which minimizes the zero-evidence region and yields nonvanishing gradients even in challenging conditions. Second, they introduce a “correct evidence” regularization term that explicitly boosts the gradient updates for the ground truth class. This combination ensures that every training sample actively contributes to learning, enhancing both predictive accuracy and robust uncertainty quantification.\n",
       "\n",
       "This line of work is reminiscent of other recent efforts in deep learning optimizer research, where detailed analyses of gradient flow have led to innovations such as Adam (Kingma & Ba, 2014) and curvature-based methods. By carefully addressing the gradient dynamics in evidential networks, Pandey and Yu (2023) contribute a novel optimization strategy that not only improves evidential deep learning performance but also exemplifies how theoretical insights into gradient behavior can drive the design of better learning algorithms.\n",
       "\n",
       "5. Discussion\n",
       "\n",
       "Collectively, these three works illustrate the rich interplay between theoretical insight and practical algorithm design in the realm of deep learning optimizers. Lye et al. (2020) push the envelope by integrating active learning with surrogate modeling for high-cost, PDE-constrained optimization problems, and their iterative scheme demonstrates how targeted training set refinement can yield exponential improvements in convergence rates. Meanwhile, Berner et al. (2021) provide a broad mathematical perspective that elucidates how properties of the loss landscape and linearized training regimes (via the NTK framework) underpin the success of standard optimizers, thereby laying the foundation for future innovations. Complementing these approaches, Pandey and Yu (2023) tackle a pressing challenge in uncertainty-aware learning by identifying and mitigating zero-gradient issues, ensuring that every training sample contributes effectively to the model’s learning process.\n",
       "\n",
       "The convergence of these themes—active and adaptive sampling, rigorous loss landscape analysis, and targeted modifications to gradient flow—signals the maturing of deep learning optimization research. As models continue to grow in complexity and scale, such innovative interventions will be crucial for achieving both computational efficiency and robust generalization.\n",
       "\n",
       "6. Conclusions\n",
       "\n",
       "Novel optimization algorithms in deep learning are emerging at the intersection of active learning, rigorous mathematical analysis, and insights into gradient dynamics. Advances like the ISMO algorithm (Lye et al., 2020) highlight the potential of active learning-based iterative refinement in surrogate models, particularly for challenging PDE-constrained scenarios. The modern mathematical frameworks described by Berner et al. (2021) shed light on why simple gradient-based methods succeed in high-dimensional, nonconvex settings and point the way toward optimization methods that leverage these geometric insights. Finally, efforts to correct gradient deficiencies in evidential deep learning (Pandey & Yu, 2023) illustrate how a fine-grained analysis of activation functions and regularization can yield significant improvements in both learning and uncertainty estimation.\n",
       "\n",
       "Together, these contributions underscore that the future of deep learning optimization lies in a deeper understanding of both the theoretical foundations and the practical implementations of training algorithms. As research continues to refine these techniques, novel optimization algorithms will play a critical role in overcoming existing challenges and unlocking new applications for deep neural networks.\n",
       "\n",
       "References\n",
       "\n",
       "Allen-Zhu, Z., Li, Y., Liu, W., & Song, Z. (2019). A convergence theory for deep learning via over-parameterization. In Proceedings of the 36th International Conference on Machine Learning (pp. 242–252).\n",
       "\n",
       "Arora, S., Cohen, N., Hazan, E., & Luo, Z. (2019). On the optimization of deep networks: Implicit acceleration by overparameterization. In Proceedings of the 36th International Conference on Machine Learning (pp. 372–389).\n",
       "\n",
       "Berner, J., Grohs, P., Kutyniok, G., & Petersen, P. (2021, May 9). The Modern Mathematics of Deep Learning.\n",
       "\n",
       "Choromanska, A., Henaff, M., Mathieu, M., Ben Arous, G., & LeCun, Y. (2015). The loss surfaces of multilayer networks. In Proceedings of the 18th International Conference on Artificial Intelligence and Statistics (pp. 192–204).\n",
       "\n",
       "Du, S. S., Lee, J. D., Li, H., Wang, L., & Zhai, X. (2018). Gradient descent finds global minima of deep neural networks. In Proceedings of the 35th International Conference on Machine Learning (pp. 1675–1685).\n",
       "\n",
       "Ioffe, S., & Szegedy, C. (2015). Batch normalization: Accelerating deep network training by reducing internal covariate shift. In Proceedings of the 32nd International Conference on Machine Learning (pp. 448–456).\n",
       "\n",
       "Kingma, D. P., & Ba, J. (2014). Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980.\n",
       "\n",
       "Lye, K. O., Mishra, S., Ray, D., & Chandrasekhar, P. (2020, August 13). Iterative Surrogate Model Optimization (ISMO): An active learning algorithm for PDE constrained optimization with deep neural networks.\n",
       "\n",
       "Pandey, D. S., & Yu, Q. (2023, June 19). Learn to Accumulate Evidence from All Training Samples: Theory and Practice.\n",
       "\n",
       "TERMINATE"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcbaae66-ea92-4fdb-bbb9-8fb2568f33ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
