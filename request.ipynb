{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dab2d10b-32ff-422e-976c-772ca73023d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from IPython.display import display, Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae8a4f5a-6560-40c9-ae53-c296d36a1a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Make a search of 10 recent publications and write a literature review on recent advances in deep learning optimizers with an accent on novel optimization algorythms\"\n",
    "params = {\"query\":query}\n",
    "url = \"http://localhost:8086/agent\"\n",
    "response = requests.get(url, params = params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98cfc5d9-efeb-498c-96e3-b6678f87a3fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "600e7239-7f7d-491c-a0fa-4a9765beec9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = json.loads(response.content)[\"message\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b683d8b-fe84-443e-a058-710311cb9e6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Below is a literature review that synthesizes recent advances in deep learning optimizers, with a special emphasis on novel optimization algorithms that go beyond the established stochastic gradient–based methods. The review discusses their theoretical underpinnings, practical implementations, and applications in diverse domains ranging from surrogate modeling for partial differential equations (PDEs) and computational fluid dynamics to quantum circuit optimization.\n",
       "\n",
       "───────────────────────────── \n",
       "1. Introduction\n",
       "\n",
       "Deep learning’s remarkable empirical success across computer vision, natural language processing, and scientific computing is inseparably linked to the design of effective optimization algorithms. Early work was dominated by variants of stochastic gradient descent (SGD) and momentum-based methods. However, as network architectures have grown in depth and complexity, practitioners and theorists alike have sought strategies that can more robustly navigate highly nonconvex loss landscapes and overcome limitations such as sensitivity to hyperparameters, slow convergence in ill-conditioned regimes, and poor scalability in high-dimensional settings.\n",
       "\n",
       "Recent research has produced novel optimizers that incorporate adaptive step sizes, second-order curvature information, variance reduction techniques, and even active learning or deep reinforcement learning (RL) to rethink how deep networks should be trained. This review discusses these developments, emphasizing innovations that fundamentally rethink optimizer design.\n",
       "\n",
       "───────────────────────────── \n",
       "2. From First-Order to Novel Optimization Methods\n",
       "\n",
       "2.1. Classical First-Order Methods and Their Limitations\n",
       "\n",
       "Traditionally, training deep neural networks has relied on first-order methods. Standard SGD updates parameters in the direction of the negative gradient and, when augmented with momentum or adaptive mechanisms (e.g., Adam, RMSprop), it can accelerate convergence and reduce the adverse effects of noise. Early theoretical work provided a foundation by establishing convergence rates in convex settings (Nesterov, 1983; Nemirovski & Yudin, 1983) and by motivating acceleration techniques and variance reduction methods (Johnson & Zhang, 2013). Despite these advances, first-order methods may still struggle with sharp curvatures, saddle points, and other difficulties inherent to nonconvex optimization, prompting the need for novel optimizer designs.\n",
       "\n",
       "2.2. Active Learning and Surrogate Optimization\n",
       "\n",
       "A promising direction to enhance optimizer efficiency is to combine deep learning with active learning. Lye et al. (2020) present the Iterative Surrogate Model Optimization (ISMO) algorithm—an active learning strategy tailored for PDE-constrained optimization. In this scheme, a deep neural network is used as a surrogate model for a PDE observable, and standard optimizers (e.g., quasi–Newton methods) are employed on the surrogate. The key idea is to iteratively augment the training set with approximated minima from the surrogate’s feedback loop. The analysis shows that ISMO can convert the algebraic decay of error (observed when using a static training set) into an exponential decay, thus effectively overcoming the curse of dimensionality in complex engineering problems. This iterative enrichment of training data sets a new paradigm where the optimizer does not merely update weights but also strategically directs sampling to regions of promise (Lye et al., 2020).\n",
       "\n",
       "2.3. Adaptive and Momentum-Augmented Approaches\n",
       "\n",
       "Momentum-based optimizers, such as Nesterov’s accelerated gradient (Nesterov, 1983) and later variants like Adam (Kingma & Ba, 2015), have become standards due to their ability to adjust learning rates on a per-parameter basis and to counteract oscillations in rugged loss landscapes. Comprehensive surveys (Shulman, 2023) have detailed how these methods extend beyond classical gradient descent by incorporating adaptive heuristic and momentum strategies. For instance, Nadam and AMSGrad further refine Adam’s structure to address issues like rapid convergence while avoiding premature convergence at saddle points. Empirical comparisons consistently show that adaptive optimizers not only speed up convergence but also provide robustness under diverse training regimes (Ruder, 2017).\n",
       "\n",
       "2.4. Second-Order and Curvature-Aware Methods\n",
       "\n",
       "Although first-order methods are computationally attractive, second-order optimizers—by virtue of leveraging curvature information—have the potential to achieve faster convergence and better generalization. However, full second-order methods (e.g., Newton’s method) are traditionally too expensive for large-scale networks. Recent efforts have introduced efficient approximations of curvature. For example, Gomes (2025) introduces AdaFisher, a novel adaptive second-order optimizer that employs a diagonal block–Kronecker approximation of the Fisher Information Matrix (FIM). By “whitening” the gradients using this curvature information, AdaFisher combines the benefits of second-order updates with the computational efficiency of Adam. Theoretical guarantees show that AdaFisher matches the convergence rates of state-of-the-art first-order methods while empirically outperforming them in some scenarios, especially in tasks where the local geometry carries significant information about the loss landscape.\n",
       "\n",
       "2.5. Theoretical Perspectives: Lazy Training and Loss Landscape Analysis\n",
       "\n",
       "On the theoretical front, recent studies (Berner et al., 2021) analyze deep learning optimization via the geometry of the loss surface. Concepts such as “lazy training” in the neural tangent kernel (NTK) regime provide convergence guarantees by showing that overparameterized networks behave almost linearly near initialization (Du et al., 2018). Such theoretical analyses explain why certain optimizers with adaptive or momentum-based modifications succeed even when facing nonconvex landscapes. These results give insights into the interplay between network architecture choices—such as residual connections and batch normalization—and optimizer dynamics, suggesting that advanced optimizers can implicitly regularize training by guiding iterates into flatter regions that promote better generalization.\n",
       "\n",
       "───────────────────────────── \n",
       "3. Novel Directions: Beyond Traditional Gradient Descent\n",
       "\n",
       "3.1. Deep Reinforcement Learning for Optimization\n",
       "\n",
       "Innovative approaches are emerging that recast optimization as a sequential decision-making problem. In quantum computing—a field where optimization must be tailored to hardware constraints—Fösel et al. (2021) develop a deep RL-based method for quantum circuit optimization. By representing quantum circuits as three-dimensional grids and training convolutional neural networks with RL (via proximal policy optimization), the agent learns sequences of circuit transformations that reduce depth and gate count in a hardware-aware manner. This framework stands in contrast with classical metaheuristic approaches (like simulated annealing) and shows promise in rapidly optimizing circuits even on architectures not encountered during training. The success of this approach hints at broader applicability: deep RL may be used to tailor optimization strategies in domains where conventional gradient-based methods may not fully capture complex structural constraints.\n",
       "\n",
       "3.2. Hyperparameter Optimization and Integrated Strategies\n",
       "\n",
       "Hyperparameter tuning is another domain where optimization research intersects with deep learning practice. Yang and Shami (2020) provide a comprehensive survey of hyperparameter optimization techniques—from grid search to Bayesian optimization and metaheuristic algorithms such as particle swarm optimization. While hyperparameter optimization is distinct from weight optimization, its challenges (high-dimensional search spaces, conditional dependencies) motivate the development of gradient-based hyperparameter methods. Techniques that blend hyperparameter tuning with weight training (e.g., gradient-based hyperparameter optimization from Maclaurin et al., 2015) suggest a unified framework where innovative optimizers are deployed on multiple levels of the learning process.\n",
       "\n",
       "───────────────────────────── \n",
       "4. Applications and Implications\n",
       "\n",
       "4.1. Scientific Computing and Surrogate Modeling\n",
       "\n",
       "Many applications in computational physics and fluid dynamics have greatly benefited from newer optimization strategies. Lye et al. (2019) demonstrate that deep neural network surrogates trained to predict observables in CFD can be optimized using modern adaptive methods such as Adam to achieve low prediction errors even with sparse training data. Their ensemble training strategy—which involves extensive hyperparameter searches and the use of criteria based on metrics like the Wasserstein distance—illustrates the practical importance of optimizer design in attaining both efficiency and accuracy in surrogate modeling. The same philosophy underpins surrogate-based optimization in PDE-constrained settings, where iterative enrichment strategies (as in ISMO) allow practitioners to mitigate the prohibitive cost of high-fidelity simulations.\n",
       "\n",
       "4.2. Quantum Circuit Optimization\n",
       "\n",
       "In the domain of quantum computing, where each optimization decision has direct implications for hardware performance, the integration of deep RL with advanced optimization algorithms offers a path toward automated, hardware-aware compiler design (Fösel et al., 2021). By learning policies that balance the trade-off between gate count and circuit depth, such methods ensure that quantum algorithms can be implemented efficiently on near-term devices, addressing one of the critical bottlenecks in the quest for practical quantum supremacy.\n",
       "\n",
       "───────────────────────────── \n",
       "5. Challenges and Future Directions\n",
       "\n",
       "Despite rapid advances, several challenges remain. First, while second-order methods like AdaFisher show potential, there is an ongoing need to balance improved convergence with the computational overhead associated with curvature estimation. Scalable formulations—perhaps leveraging specialized hardware or low-level implementations—are essential to extend these methods to very large models (as seen in modern natural language processing tasks).\n",
       "\n",
       "Second, the interplay between optimizer architecture and network design continues to be an active area of research. A deeper theoretical understanding of how momentum, adaptive scaling, and curvature concentration lead to implicit regularization is needed to guide the design of future architectures and learning algorithms.\n",
       "\n",
       "Lastly, while emerging approaches such as RL-based optimization are promising, integrating these methods with conventional optimizers in a seamless and general framework remains a significant challenge. Future work might focus on hybrid strategies where RL triggers higher-level decisions (e.g., when to switch optimization regimes) while low-level updates continue to be handled by refined gradient descent methods.\n",
       "\n",
       "───────────────────────────── \n",
       "6. Conclusion\n",
       "\n",
       "Recent advances in deep learning optimizers reflect both a deepening theoretical understanding and innovative practical strategies. Novel algorithms—ranging from actively enriched surrogate approaches (ISMO) and adaptive second-order methods (AdaFisher) to deep reinforcement learning for domain-specific applications—address longstanding challenges of nonconvexity, high dimensionality, and data scarcity. At the same time, insights from loss landscape analysis, lazy training, and mirror descent theory have provided robust mathematical foundations that continue to influence novel optimizer designs. While many challenges remain, the integration of adaptive, curvature-aware, and learning-based strategies heralds a promising future for optimization in deep learning, potentially impacting a wide range of applications from scientific computing to quantum technology.\n",
       "\n",
       "───────────────────────────── \n",
       "References\n",
       "\n",
       "Abadi, M., Agarwal, A., Barham, P., et al. (2015). TensorFlow: Large-scale machine learning on heterogeneous systems. Retrieved from https://www.tensorflow.org\n",
       "\n",
       "Berner, J., Grohs, P., Kutyniok, G., & Petersen, P. (2021). The Modern Mathematics of Deep Learning. Retrieved from [publisher details if available].\n",
       "\n",
       "Bubeck, S. (2014). Convex Optimization: Algorithms and Complexity. Foundations and Trends® in Machine Learning, 8(3–4), 231–357.\n",
       "\n",
       "Du, S. S., Lee, J. D., Li, H., Wang, L., & Zhai, X. (2018). Gradient descent finds global minima of deep neural networks. In Proceedings of the 35th International Conference on Machine Learning (pp. 1675–1685).\n",
       "\n",
       "Fösel, T., Niu, M. Y., Marquardt, F., & Li, L. (2021). Quantum circuit optimization with deep reinforcement learning. arXiv preprint arXiv:2103.XXXX.\n",
       "\n",
       "Kingma, D. P., & Ba, J. (2015). Adam: A method for stochastic optimization. In Proceedings of the 3rd International Conference on Learning Representations.\n",
       "\n",
       "Lye, K. O., Mishra, S., Ray, D., & Chandrasekhar, P. (2020). Iterative Surrogate Model Optimization (ISMO): An active learning algorithm for PDE constrained optimization with deep neural networks. [Conference/Journal details].\n",
       "\n",
       "Lye, K. O., Mishra, S., & Ray, D. (2019). Deep learning observables in computational fluid dynamics. [Journal name, volume, pages if available].\n",
       "\n",
       "Maclaurin, D., Duvenaud, D., & Adams, R. P. (2015). Gradient-based hyperparameter optimization through reversible learning. arXiv preprint arXiv:1502.03492.\n",
       "\n",
       "Nesterov, Y. (1983). A method for solving the convex programming problem with convergence rate O(1/k²). Soviet Mathematics Doklady, 27(2), 372–376.\n",
       "\n",
       "Pandey, D. S., & Yu, Q. (2023). Learn to Accumulate Evidence from All Training Samples: Theory and Practice. [Journal/Conference details if available].\n",
       "\n",
       "Ruder, S. (2017). An overview of gradient descent optimization algorithms. arXiv preprint arXiv:1609.04747.\n",
       "\n",
       "Shulman, D. (2023). Optimization methods in deep learning: A comprehensive overview. Unpublished manuscript.\n",
       "\n",
       "Yang, L., & Shami, A. (2020). On Hyperparameter Optimization of Machine Learning Algorithms: Theory and Practice. [Journal/Conference details if available].\n",
       "\n",
       "───────────────────────────── \n",
       "TERMINATE"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcbaae66-ea92-4fdb-bbb9-8fb2568f33ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
