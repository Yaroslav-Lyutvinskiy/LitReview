{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee4ca6ab-350d-404e-b369-43ff9d95aa17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen_agentchat.agents import AssistantAgent\n",
    "from autogen_agentchat.conditions import TextMentionTermination\n",
    "from autogen_agentchat.teams import MagenticOneGroupChat\n",
    "from autogen_agentchat.ui import Console\n",
    "from autogen_core.tools import FunctionTool\n",
    "from autogen_ext.models.openai import OpenAIChatCompletionClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "49d1107e-c0dc-41ce-84d1-064a16460e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from semanticscholar import SemanticScholar\n",
    "from markitdown import MarkItDown\n",
    "import requests\n",
    "import os\n",
    "from pathlib import Path\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import arxiv\n",
    "import ast \n",
    "from IPython.display import display, Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b5fe1f0c-6baa-430e-89d8-adc92defe4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(\"./.env\")\n",
    "openai_api_key = os.environ.get(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926685a4-e04e-4853-af54-d4ca46bd3555",
   "metadata": {},
   "source": [
    "## Plan for Literature review \n",
    "\n",
    "1. Determine larger area of research and focused area of research. Focused area of research is the area of particular interest of the query. Larger area of research is the more general area which includes focus area as a part.\n",
    "2. Extract systematic literature reviews of larger area of research. Out of article body extract paragraphs and references which characterize state of the art in focused area of research.\n",
    "3. Summarise state of the art for focus area of research.\n",
    "4. Define strongest research groups. To do it search in references for focus area of research often appearing author names with special attention to first and last article authors. Combine often caithoring authors in reserarch groups. \n",
    "5. Extract latest publications from strongest research groups and based on this publications describe scientific approach of every research group. \n",
    "6. Compare research group approach looking for similarities and differencies in approach. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "855de688-6621-4266-a8c5-b95173952a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def s2_search(query: str, max_results: int = 2) -> list:  # type: ignore[type-arg]\n",
    "    \"\"\"\n",
    "    Search Semantic scholar by keywords and return the results including abstracts.\n",
    "    \"\"\"\n",
    "    from semanticscholar import SemanticScholar\n",
    "\n",
    "    sch = SemanticScholar()\n",
    "    \n",
    "    search = sch.search_paper(query=query, limit = max_results)\n",
    "\n",
    "    results = []\n",
    "    for paper in search.items:\n",
    "        try:\n",
    "            results.append(\n",
    "                {\n",
    "                    \"title\": paper.title,\n",
    "                    \"authors\": [author.name for author in paper.authors],\n",
    "                    \"published\": paper.publicationDate.strftime(\"%Y-%m-%d\"),\n",
    "                    \"abstract\": paper.abstract,\n",
    "                    \"pdf_url\": paper.openAccessPdf[\"url\"]\n",
    "                }\n",
    "            )\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    # # Write results to a file\n",
    "    # with open('s2_search_results.json', 'w') as f:\n",
    "    #     json.dump(results, f, indent=2)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f1c86348-7a8f-437f-8587-7f361bd3bbb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_pdf_file(url: str) -> str:\n",
    "    \"\"\"\n",
    "    Download PDF from given URL to local directory.\n",
    "    :param url: The url of the PDF file to be downloaded\n",
    "    :return: path of the downloaded file, empty string if download failed\n",
    "    \"\"\"\n",
    "\n",
    "    # Request URL and get response object\n",
    "    response = requests.get(url, stream=True)\n",
    "\n",
    "    # isolate PDF filename from URL\n",
    "    pdf_file_name = os.path.basename(url)\n",
    "    if response.status_code == 200:\n",
    "        # Save in current working directory\n",
    "        filepath = os.path.join(os.getcwd(), pdf_file_name)\n",
    "        if not filepath.endswith(\".pdf\"):\n",
    "            filepath = filepath+\".pdf\"\n",
    "        with open(filepath, 'wb') as pdf_object:\n",
    "            pdf_object.write(response.content)\n",
    "            print(f'{pdf_file_name} was successfully saved!')\n",
    "            return filepath\n",
    "    else:\n",
    "        print(f'Could not download {pdf_file_name},')\n",
    "        print(f'HTTP response status code: {response.status_code}')\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fafd087a-aaa8-4d60-9379-6c1e0910bcc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_pdf_to_markdown(path: str) -> str:\n",
    "    \"\"\"\n",
    "    Convert provided pdf file to markdown text\n",
    "    :param path: Local path to pdf file\n",
    "    :return: path to output markdown file\n",
    "    \"\"\"\n",
    "    md = MarkItDown(\n",
    "        # llm_client=client,\n",
    "        # llm_model=\"gpt-4o\",\n",
    "        # llm_prompt=\"Extract text from pdf file with OCR and return well-formatted Markdown. Exclude footers, page numbers, watermarks.\",\n",
    "    )\n",
    "    result = md.convert(path)\n",
    "    outpath = Path(path).with_suffix(\".md\")\n",
    "    with open(outpath, 'w') as pdf_object:\n",
    "            pdf_object.write(result.markdown)\n",
    "    return result.markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "bde4ae38-a9cb-46fd-85a6-e7e7b4a3b3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#def arxiv_search(query: str, max_results: int = 2, min_year: int = 2020, max_year: int = 2025) -> list:  # type: ignore[type-arg]\n",
    "def arxiv_search(query: str, max_results: int = 2) -> list:  # type: ignore[type-arg]\n",
    "    \"\"\"\n",
    "    Search Arxiv for papers and return the results including abstracts and full text.\n",
    "    :param query: query to search in arxiv\n",
    "    :param max_results: maximum number of articles returned\n",
    "    :return: path to output markdown file\n",
    "    \"\"\"\n",
    "\n",
    "    client = arxiv.Client(\n",
    "        page_size=max_results,\n",
    "        delay_seconds=5,\n",
    "        num_retries=3\n",
    "    )\n",
    "    search = arxiv.Search(query=query, max_results=max_results, sort_by=arxiv.SortCriterion.Relevance)\n",
    "\n",
    "    results = []\n",
    "    for paper in client.results(search):\n",
    "\n",
    "        try:\n",
    "\n",
    "            #extract full text\n",
    "            md_result = None\n",
    "            if paper.pdf_url is not None : \n",
    "                pdf_file =  download_pdf_file(paper.pdf_url)\n",
    "                if pdf_file is not None : \n",
    "                    md = MarkItDown()\n",
    "                    md_result = md.convert(pdf_file).markdown\n",
    "                \n",
    "            \n",
    "            results.append(\n",
    "                {\n",
    "                    \"title\": paper.title,\n",
    "                    \"authors\": [author.name for author in paper.authors],\n",
    "                    \"published\": paper.published.strftime(\"%Y-%m-%d\"),\n",
    "                    \"abstract\": paper.summary,\n",
    "                    \"full_text\": md_result\n",
    "                }\n",
    "            )\n",
    "        # except:\n",
    "        #     # Here need to be some warning\n",
    "        #     pass\n",
    "        finally:\n",
    "            if pdf_file is not None:\n",
    "                os.remove(pdf_file)\n",
    "\n",
    "    # # Write results to a file\n",
    "    # with open('arxiv_search_results.json', 'w') as f:\n",
    "    #     json.dump(results, f, indent=2)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "24772c8d-34e4-417e-9f05-0de6f516d538",
   "metadata": {},
   "outputs": [],
   "source": [
    "# client = arxiv.Client(\n",
    "#     page_size=5,\n",
    "#     delay_seconds=5,\n",
    "#     num_retries=3\n",
    "# )\n",
    "\n",
    "# search = arxiv.Search(query=\"recent advances in deep learning optimizers with an accent on novel optimization algorythms\", max_results=5, sort_by=arxiv.SortCriterion.Relevance)\n",
    "\n",
    "# search_iter = client.results(search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5e0935-c2d6-4177-bb5f-f598713971e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# p1 = next(search_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ffbd295-b99d-4ad9-9a57-20b3ccb808db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b1c7f4d8-4a78-4116-84fb-ae1034fec5c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2012.06469v1 was successfully saved!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/home/jovyan/work/LitReview/2012.06469v1.pdf'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "download_pdf_file(\"https://arxiv.org/pdf/2012.06469v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0cdfd3b9-a1fa-4cbf-a6ff-c549405db4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = convert_pdf_to_markdown('/home/jovyan/work/LitReview/2012.06469v1.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "10ad7bc2-ed89-44a8-8e49-42061bd7894b",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = arxiv.Client(\n",
    "        page_size=5,\n",
    "        delay_seconds=5,\n",
    "        num_retries=3\n",
    "    )\n",
    "search = arxiv.Search(query=\"recent advances in deep learning optimizers\", max_results=5, sort_by=arxiv.SortCriterion.Relevance)\n",
    "res = client.results(search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "11ec59f6-6e9f-40c2-8df0-84bc08c463f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "s2_search_tool = FunctionTool(\n",
    "    s2_search, description=\"Search Semantic scholar for papers by keywords, returns found papers including abstracts\"\n",
    ")\n",
    "arxiv_search_tool = FunctionTool(\n",
    "    arxiv_search, description=\"Search Arxiv for papers related to a given topic, including abstracts\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "806de4f7-fac1-4633-8039-447061a8e0d9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2105.04026v2 was successfully saved!\n",
      "2306.11113v2 was successfully saved!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'title': 'The Modern Mathematics of Deep Learning',\n",
       "  'authors': ['Julius Berner',\n",
       "   'Philipp Grohs',\n",
       "   'Gitta Kutyniok',\n",
       "   'Philipp Petersen'],\n",
       "  'published': '2021-05-09',\n",
       "  'abstract': 'We describe the new field of mathematical analysis of deep learning. This field emerged around a list of research questions that were not answered within the classical framework of learning theory. These questions concern: the outstanding generalization power of overparametrized neural networks, the role of depth in deep architectures, the apparent absence of the curse of dimensionality, the surprisingly successful optimization performance despite the non-convexity of the problem, understanding what features are learned, why deep architectures perform exceptionally well in physical problems, and which fine aspects of an architecture affect the behavior of a learning task in which way. We present an overview of modern approaches that yield partial answers to these questions. For selected approaches, we describe the main ideas in more detail.',\n",
       "  'full_text': 'The Modern Mathematics of Deep Learning∗\\n\\nJulius Berner†\\n\\nPhilipp Grohs‡\\n\\nGitta Kutyniok§\\n\\nPhilipp Petersen‡\\n\\n3\\n2\\n0\\n2\\n\\nb\\ne\\nF\\n8\\n\\n]\\n\\nG\\nL\\n.\\ns\\nc\\n[\\n\\n2\\nv\\n6\\n2\\n0\\n4\\n0\\n.\\n5\\n0\\n1\\n2\\n:\\nv\\ni\\nX\\nr\\na\\n\\nAbstract\\n\\nWe describe the new ﬁeld of mathematical analysis of deep learning. This ﬁeld emerged around a list\\nof research questions that were not answered within the classical framework of learning theory. These\\nquestions concern: the outstanding generalization power of overparametrized neural networks, the role of\\ndepth in deep architectures, the apparent absence of the curse of dimensionality, the surprisingly successful\\noptimization performance despite the non-convexity of the problem, understanding what features are\\nlearned, why deep architectures perform exceptionally well in physical problems, and which ﬁne aspects\\nof an architecture aﬀect the behavior of a learning task in which way. We present an overview of modern\\napproaches that yield partial answers to these questions. For selected approaches, we describe the main\\nideas in more detail.\\n\\nContents\\n\\n1 Introduction\\n\\n1.1 Notation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n1.2 Foundations of learning theory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n1.3 Do we need a new theory? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n\\n2 Generalization of large neural networks\\n\\n2.1 Kernel regime . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n2.2 Norm-based bounds and margin theory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n2.3 Optimization and implicit regularization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n. . . . . . . . . . . . . . . . . . . . . . . . . . .\\n2.4 Limits of classical theory and double descent\\n\\n3 The role of depth in the expressivity of neural networks\\n\\n3.1 Approximation of radial functions\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n3.2 Deep ReLU networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n3.3 Alternative notions of expressivity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n\\n4 Deep neural networks overcome the curse of dimensionality\\n\\n4.1 Manifold assumption . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n4.2 Random sampling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n4.3 PDE assumption . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n\\n5 Optimization of deep neural networks\\n\\n5.1 Loss landscape analysis\\n5.2 Lazy training and provable convergence of stochastic gradient descent\\n\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n. . . . . . . . . . . . .\\n\\n2\\n4\\n4\\n17\\n\\n22\\n23\\n24\\n25\\n27\\n\\n29\\n29\\n31\\n32\\n\\n34\\n34\\n35\\n36\\n\\n39\\n39\\n41\\n\\n∗A version of this review paper appears as a chapter in the book “Mathematical Aspects of Deep Learning” by Cambridge\\n\\nUniversity Press.\\n\\n†Faculty of Mathematics, University of Vienna.\\n‡Faculty of Mathematics and Research Network DataScience@UniVienna, University of Vienna.\\n§Department of Mathematics, Ludwig Maximilian University of Munich, and Department of Physics and Technology,\\n\\nUniversity of Tromsø.\\n\\n1\\n\\n\\x0c6 Tangible eﬀects of special architectures\\n\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n6.1 Convolutional neural networks\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n6.2 Residual neural networks\\n6.3 Framelets and U-Nets\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n6.4 Batch normalization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n6.5 Sparse neural networks and pruning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n6.6 Recurrent neural networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n\\n7 Describing the features a deep neural network learns\\n\\nInvariances and the scattering transform . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n7.1\\n7.2 Hierarchical sparse representations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n\\n8 Eﬀectiveness in natural sciences\\n\\n8.1 Deep neural networks meet inverse problems . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n8.2 PDE-based models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n\\n44\\n45\\n46\\n47\\n49\\n50\\n52\\n\\n52\\n52\\n53\\n\\n55\\n55\\n56\\n\\n1\\n\\nIntroduction\\n\\nDeep learning has undoubtedly established itself as the outstanding machine learning technique of recent\\ntimes. This dominant position was claimed through a series of overwhelming successes in widely diﬀerent\\napplication areas.\\n\\nPerhaps the most famous application of deep learning and certainly one of the ﬁrst where these techniques\\nbecame state-of-the-art is image classiﬁcation [LBBH98, KSH12, SLJ+15, HZRS16].\\nIn this area, deep\\nlearning is nowadays the only method that is seriously considered. The prowess of deep learning classiﬁers\\ngoes so far that they often outperform humans in image labelling tasks [HZRS15].\\n\\nA second famous application area is the training of deep-learning-based agents to play board games or\\ncomputer games, such as Atari games [MKS+13]. In this context, probably the most prominent achievement\\nyet is the development of an algorithm that beat the best human player in the game of Go [SHM+16, SSS+17]—\\na feat that was previously unthinkable owing to the extreme complexity of this game. Besides, even in\\nmultiplayer, team-based games with incomplete information deep-learning-based agents nowadays outperform\\nworld-class human teams [BBC+19, VBC+19].\\n\\nIn addition to playing games, deep learning has also led to impressive breakthroughs in the natural\\nsciences. For example, it is used in the development of drugs [MSL+15], molecular dynamics [FHH+17], or in\\nhigh-energy physics [BSW14]. One of the most astounding recent breakthroughs in scientiﬁc applications\\nis the development of a deep-learning-based predictor for the folding behavior of proteins [SEJ+20]. This\\npredictor is the ﬁrst method to match the accuracy of lab-based methods.\\n\\nFinally, in the vast ﬁeld of natural language processing, which includes the subtasks of understanding,\\nsummarizing, or generating text, impressive advances were made based on deep learning. Here, we refer\\nto [YHPC18] for an overview. One technique that recently stood out is based on a so-called transformer neural\\nnetwork [BCB15, VSP+17]. This network structure gave rise to the impressive GPT-3 model [BMR+20]\\nwhich not only creates coherent and compelling texts but can also produce code, such as, for the layout of a\\nwebpage according to some instructions that a user inputs in plain English. Transformer neural networks\\nhave also been successfully employed in the ﬁeld of symbolic mathematics [SGHK18, LC19].\\n\\nIn this article, we present and discuss the mathematical foundations of the success story outlined above.\\nMore precisely, our goal is to outline the newly emerging ﬁeld of mathematical analysis of deep learning. To\\naccurately describe this ﬁeld, a necessary preparatory step is to sharpen our deﬁnition of the term deep\\nlearning. For the purposes of this article, we will use the term in the following narrow sense: Deep learning\\nrefers to techniques where deep neural networks1 are trained with gradient-based methods. This narrow\\n\\n1We will deﬁne the term neural network later but, for this deﬁnition, one can view it as a parametrized family of functions\\n\\nwith a diﬀerentiable parametrization.\\n\\n2\\n\\n\\x0cdeﬁnition is helpful to make this article more concise. We would like to stress, however, that we do not claim\\nin any way that this is the best or the right deﬁnition of deep learning.\\n\\nHaving ﬁxed a deﬁnition of deep learning, three questions arise concerning the aforementioned emerging\\nﬁeld of mathematical analysis of deep learning: To what extent is a mathematical theory necessary? Is it\\ntruly a new ﬁeld? What are the questions studied in this area?\\n\\nLet us start by explaining the necessity of a theoretical analysis of the tools described above. From a\\nscientiﬁc perspective, the primary reason why deep learning should be studied mathematically is simple\\ncuriosity. As we will see throughout this article, many practically observed phenomena in this context are\\nnot explained theoretically. Moreover, theoretical insights and the development of a comprehensive theory\\nare often the driving force underlying the development of new and improved methods. Prominent examples\\nof mathematical theories with such an eﬀect are the theory of ﬂuid mechanics which is an invaluable asset\\nto the design of aircraft or cars and the theory of information that aﬀects and shapes all modern digital\\ncommunication. In the words of Vladimir Vapnik2: “Nothing is more practical than a good theory”, [Vap13,\\nPreface]. In addition to being interesting and practical, theoretical insight may also be necessary. Indeed, in\\nmany applications of machine learning, such as medical diagnosis, self-driving cars, and robotics, a signiﬁcant\\nlevel of control and predictability of deep learning methods is mandatory. Also, in services, such as banking\\nor insurance, the technology should be controllable to guarantee fair and explainable decisions.\\n\\nLet us next address the claim that the ﬁeld of mathematical analysis of deep learning is a newly emerging\\narea. In fact, under the aforementioned deﬁnition of deep learning, there are two main ingredients of the\\ntechnology: deep neural networks and gradient-based optimization. The ﬁrst artiﬁcial neuron was already\\nintroduced in 1943 in [MP43]. This neuron was not trained but instead used to explain a biological neuron.\\nThe ﬁrst multi-layered network of such artiﬁcial neurons that was also trained can be found in [Ros58].\\nSince then, various neural network architectures have been developed. We will discuss these architectures in\\ndetail in the following sections. The second ingredient, gradient-based optimization, is made possible by the\\nobservation that due to the graph-based structure of neural networks the gradient of an objective function\\nwith respect to the parameters of the neural network can be computed eﬃciently. This has been observed in\\nvarious ways, see [Kel60, Dre62, Lin70, RHW86]. Again, these techniques will be discussed in the upcoming\\nsections. Since then, techniques have been improved and extended. As the rest of the manuscript is spent\\nreviewing these methods, we will keep the discussion of literature at this point brief. Instead, we refer to\\nsome overviews of the history of deep learning from various perspectives: [LBH15, Sch15, GBC16, HH19].\\n\\nGiven the fact that the two main ingredients of deep neural networks have been around for a long\\ntime, one would expect that a comprehensive mathematical theory has been developed that describes\\nwhy and when deep-learning-based methods will perform well or when they will fail. Statistical learning\\ntheory [AB99, Vap99, CS02, BBL03, Vap13] describes multiple aspects of the performance of general learning\\nmethods and in particular deep learning. We will review this theory in the context of deep learning in\\nSubsection 1.2 below. Hereby, we focus on classical, deep learning-related results that we consider well-known\\nin the machine learning community. Nonetheless, the choice of these results is guaranteed to be subjective.\\nWe will ﬁnd that the presented, classical theory is too general to explain the performance of deep learning\\nadequately. In this context, we will identify the following questions that appear to be diﬃcult to answer\\nwithin the classical framework of learning theory: Why do trained deep neural networks not overﬁt on the\\ntraining data despite the enormous power of the architecture? What is the advantage of deep compared to\\nshallow architectures? Why do these methods seemingly not suﬀer from the curse of dimensionality? Why\\ndoes the optimization routine often succeed in ﬁnding good solutions despite the non-convexity, non-linearity,\\nand often non-smoothness of the problem? Which aspects of an architecture aﬀect the performance of the\\nassociated models and how? Which features of data are learned by deep architectures? Why do these methods\\nperform as well as or better than specialized numerical tools in natural sciences?\\n\\nThe new ﬁeld of mathematical analysis of deep learning has emerged around questions like the ones listed\\nabove. In the remainder of this article, we will collect some of the main recent advances to answer these\\nquestions. Because this ﬁeld of mathematical analysis of deep learning is incredibly active and new material\\nis added at breathtaking speeds, a brief survey on recent advances in this area is guaranteed to miss not only\\n\\n2This claim can be found earlier in a non-mathematical context in the works of Kurt Lewin [Lew43].\\n\\n3\\n\\n\\x0ca couple of references but also many of the most essential ones. Therefore, we do not strive for a complete\\noverview, but instead, showcase several fundamental ideas on a mostly intuitive level. In this way, we hope to\\nallow the reader to familiarize themselves with some exciting concepts and provide a convenient entry-point\\nfor further studies.\\n\\n1.1 Notation\\n\\nWe denote by N the set of natural numbers, by Z the set of integers and by R the ﬁeld of real numbers.\\nFor N ∈ N, we denote by [N ] the set {1, . . . , N }. For two functions f, g : X → [0, ∞), we write f (cid:46) g, if\\nthere exists a universal constant c such that f (x) ≤ cg(x) for all x ∈ X . In a pseudometric space (X , dX ),\\nwe deﬁne the ball of radius r ∈ (0, ∞) around a point x ∈ X by BdX\\nr (x) or Br(x) if the pseudometric dX\\nis clear from the context. By (cid:107) · (cid:107)p, p ∈ [1, ∞], we denote the (cid:96)p-norm, and by (cid:104)·, ·(cid:105) the Euclidean inner\\nproduct of given vectors. By (cid:107) · (cid:107)op we denote the operator norm induced by the Euclidean norm and by\\n(cid:107) · (cid:107)F the Frobenius norm of given matrices. For p ∈ [1, ∞], s ∈ [0, ∞), d ∈ N, and X ⊂ Rd, we denote by\\nW s,p(X ) the Sobolev-Slobodeckij space, which for s = 0 is just a Lebesgue space, i.e., W 0,p(X ) = Lp(X ).\\nFor measurable spaces X and Y, we deﬁne M(X , Y) to be the set of measurable functions from X to Y.\\nWe denote by ˆg the Fourier transform3 of a tempered distribution g. For probabilistic statements, we will\\nassume a suitable underlying probability space with probability measure P. For an X -valued random variable\\nX, we denote by E[X] and V[X] its expectation and variance and by PX the image measure of X on X ,\\ni.e., PX (A) = P(X ∈ A) for every measurable set A ⊂ X . If possible, we use the corresponding lowercase\\nletter to denote the realization x ∈ X of the random variable X for a given outcome. We write Id for the\\nd-dimensional identity matrix and, for a set A, we write 1A for the indicator function of A, i.e., 1A(x) = 1 if\\nx ∈ A and 1A(x) = 0 else.\\n\\n1.2 Foundations of learning theory\\n\\nBefore we continue to describe recent developments in the mathematical analysis of deep learning methods,\\nwe start by providing a concise overview of the classical mathematical and statistical theory underlying\\nmachine learning tasks and algorithms which, in their most general form, can be formulated as follows.\\n\\nDeﬁnition 1.1 (Learning - informal). Let X , Y, and Z be measurable spaces. In a learning task, one is given\\ndata in Z and a loss function L : M(X , Y) × Z → R. The goal is to choose a hypothesis set F ⊂ M(X , Y)\\nand construct a learning algorithm, i.e., a mapping\\n\\nA :\\n\\n(cid:91)\\n\\nm∈N\\n\\nZ m → F,\\n\\nthat uses training data s = (z(i))m\\ni=1 ∈ Z m to ﬁnd a model fs = A(s) ∈ F that performs well on the training\\ndata s and also generalizes to unseen data z ∈ Z. Here, performance is measured via the loss function L\\nand the corresponding loss L(fs, z) and, informally speaking, generalization means that the out-of-sample\\nperformance of fs at z behaves similar to the in-sample performance on s.\\n\\nDeﬁnition 1.1 is deliberately vague on how to measure generalization performance. Later, we will often\\nstudy the expected out-of-sample performance. To talk about expected performance, a data distribution\\nneeds to be speciﬁed. We will revisit this point in Assumption 1.10 and Deﬁnition 1.11.\\n\\nFor simplicity, we focus on one-dimensional, supervised prediction tasks with input features in Euclidean\\n\\nspace as deﬁned in the following.\\n\\nDeﬁnition 1.2 (Prediction task). In a prediction task, we have that Z := X × Y, i.e., we are given training\\ndata s = ((x(i), y(i)))m\\ni=1 that consist of input features x(i) ∈ X and corresponding labels y(i) ∈ Y. For\\none-dimensional regression tasks with Y ⊂ R, we consider the quadratic loss L(f, (x, y)) = (f (x) − y)2 and,\\n\\n3Respecting common notation, we will also use the hat symbol to denote the minimizer of the empirical risk (cid:98)fs in Deﬁnition 1.8\\n\\nbut this clash of notation does not cause any ambiguity.\\n\\n4\\n\\n\\x0cfor binary classiﬁcation tasks with Y = {−1, 1}, we consider the 0-1 loss L(f, (x, y)) = 1(−∞,0)(yf (x)). We\\nassume that our input features are in Euclidean space, i.e., X ⊂ Rd with input dimension d ∈ N.\\n\\nIn a prediction task, we aim for a model fs : X → Y, such that, for unseen pairs (x, y) ∈ X × Y, fs(x) is\\na good prediction of the true label y. However, note that large parts of the presented theory can be applied\\nto more general settings.\\n\\nRemark 1.3 (Learning tasks). Apart from straightforward extensions to multi-dimensional prediction tasks\\nand other loss functions, we want to mention that unsupervised and semi-supervised learning tasks are\\noften treated as prediction tasks. More precisely, one transforms unlabeled training data z(i) into features\\nx(i) = T1(z(i)) ∈ X and labels y(i) = T2(z(i)) ∈ Y using suitable transformations T1 : Z → X , T2 : Z → Y. In\\ndoing so, one asks for a model fs approximating the transformation T2 ◦ T −1\\n: X → Y which is, e.g., done in\\norder to learn feature representations or invariances.\\n\\n1\\n\\nFurthermore, one can consider density estimation tasks, where X = Z, Y := [0, ∞], and F consists of\\nprobability densities with respect to some σ-ﬁnite reference measure µ on Z. One then aims for a probability\\ndensity fs that approximates the density of the unseen data z with respect to µ. One can perform L2(µ)-\\napproximation based on the discretization L(f, z) = −2f (z) + (cid:107)f (cid:107)2\\nL2(µ) or maximum likelihood estimation\\nbased on the surprisal L(f, z) = − log(f (z)).\\n\\nIn deep learning the hypothesis set F consists of realizations of neural networks Φa(·, θ), θ ∈ P, with\\na given architecture a and parameter set P. In practice, one uses the term neural network for a range of\\nfunctions that can be represented by directed acyclic graphs, where the vertices correspond to elementary\\nalmost everywhere diﬀerentiable functions parametrizable by θ ∈ P and the edges symbolize compositions\\nof these functions. In Section 6, we will review some frequently used architectures, in the other sections,\\nhowever, we will mostly focus on fully connected feedforward (FC) neural networks as deﬁned below.\\n\\nDeﬁnition 1.4 (FC neural network). A fully connected feedforward neural network is given by its architecture\\na = (N, (cid:37)), where L ∈ N, N ∈ NL+1, and (cid:37) : R → R. We refer to (cid:37) as the activation function, to L as the\\nnumber of layers, and to N0, NL, and N(cid:96), (cid:96) ∈ [L − 1], as the number of neurons in the input, output, and\\n(cid:96)-th hidden layer, respectively. We denote the number of parameters by\\n\\nP (N ) :=\\n\\nL\\n(cid:88)\\n\\n(cid:96)=1\\n\\nN(cid:96)N(cid:96)−1 + N(cid:96)\\n\\nand deﬁne the corresponding realization function Φa : RN0 × RP (N ) → RNL which satisﬁes for every input\\nx ∈ RN0 and parameters\\n\\nθ = (θ((cid:96)))L\\n\\n(cid:96)=1 = ((W ((cid:96)), b((cid:96))))L\\n\\n(cid:96)=1 ∈\\n\\nL×\\n\\n(cid:96)=1\\n\\n(RN(cid:96)×N(cid:96)−1 × RN(cid:96)) ∼= RP (N )\\n\\nthat Φa(x, θ) = Φ(L)(x, θ), where\\n\\nΦ(1)(x, θ) = W (1)x + b(1),\\n¯Φ((cid:96))(x, θ) = (cid:37)(cid:0)Φ((cid:96))(x, θ)(cid:1),\\n\\n(cid:96) ∈ [L − 1],\\n\\nand\\n\\n(1.1)\\n\\nΦ((cid:96)+1)(x, θ) = W ((cid:96)+1) ¯Φ((cid:96))(x, θ) + b((cid:96)+1),\\n\\n(cid:96) ∈ [L − 1],\\n\\nand (cid:37) is applied componentwise. We refer to W ((cid:96)) ∈ RN(cid:96)×N(cid:96)−1 and b((cid:96)) ∈ RN(cid:96) as the weight matrices and\\nbias vectors, and to ¯Φ((cid:96)) and Φ((cid:96)) as the activations and pre-activations of the N(cid:96) neurons in the (cid:96)-th layer.\\nThe width and depth of the architecture are given by (cid:107)N (cid:107)∞ and L and we call the architecture deep if L > 2\\nand shallow if L = 2.\\n\\nThe underlying directed acyclic graph of FC networks is given by compositions of the aﬃne linear maps\\nx (cid:55)→ W ((cid:96))x + b((cid:96)), (cid:96) ∈ [L], with the activation function (cid:37) intertwined, see Figure 1.1. Typical activation\\n\\n5\\n\\n\\x0cx1\\n\\nx2\\n\\nx3\\n\\nx (cid:55)→ W (1)x + b(1)\\n\\nΦ(1)\\n1\\n\\nΦ(1)\\n2\\n\\nΦ(1)\\n3\\n\\nΦ(1)\\n4\\n\\n(cid:37)\\n\\n¯Φ(1)\\n1\\n\\n¯Φ(1)\\n2\\n\\n¯Φ(1)\\n3\\n\\n¯Φ(1)\\n4\\n\\nx (cid:55)→ W (2)x + b(2)\\n\\nΦ(2)\\n1\\n\\nΦ(2)\\n2\\n\\nΦ(2)\\n3\\n\\nΦ(2)\\n4\\n\\nΦ(2)\\n5\\n\\nΦ(2)\\n6\\n\\n(cid:37)\\n\\n¯Φ(2)\\n1\\n\\n¯Φ(2)\\n2\\n\\n¯Φ(2)\\n3\\n\\n¯Φ(2)\\n4\\n\\n¯Φ(2)\\n5\\n\\n¯Φ(2)\\n6\\n\\nx (cid:55)→ W (3)x + b(3)\\n\\nΦa\\n\\nFigure 1.1: Graph (grey) and (pre-)activations of the neurons (white) of a deep fully connected feedforward\\nneural network Φa : R3 × R53 (cid:55)→ R with architecture a = ((3, 4, 6, 1), (cid:37)) and parameters θ = ((W ((cid:96)), b((cid:96)))3\\n(cid:96)=1.\\n\\nfunctions used in practice are variants of the rectiﬁed linear unit (ReLU) given by (cid:37)R(x) := max{0, x} and\\nsigmoidal functions (cid:37) ∈ C(R) satisfying (cid:37)(x) → 1 for x → ∞ and (cid:37)(x) → 0 for x → −∞, such as the logistic\\nfunction (cid:37)σ(x) := 1/(1 + e−x) (often referred to as the sigmoid function). See also Table 1 for a comprehensive\\nlist of widely used activation functions.\\n\\nRemark 1.5 (Neural networks). If not further speciﬁed, we will use the term (neural) network, or the\\nabbreviation NN, to refer to FC neural networks. Note that many of the architectures used in practice (see\\nSection 6) can be written as special cases of Deﬁnition 1.4 where, e.g., speciﬁc parameters are prescribed by\\nconstants or shared with other parameters. Furthermore, note that aﬃne linear functions are NNs with depth\\nL = 1. We will also consider biasless NNs given by linear mappings without bias vector, i.e., b((cid:96)) = 0, (cid:96) ∈ [L].\\nIn particular, any NN can always be written without bias vectors by redeﬁning\\n\\nx →\\n\\n(cid:21)\\n\\n(cid:20)x\\n1\\n\\n,\\n\\n(W ((cid:96)), b((cid:96))) →\\n\\n(cid:20)W ((cid:96))\\n0\\n\\n(cid:21)\\n\\n,\\n\\nb((cid:96))\\n1\\n\\n(cid:96) ∈ [L − 1],\\n\\nand\\n\\n(W (L), b(L)) → (cid:2)W (L)\\n\\nb(L)(cid:3) .\\n\\nTo enhance readability we will often not specify the underlying architecture a = (N, (cid:37)) or the parameters θ ∈\\nRP (N ) and use the term NN to refer to the architecture as well as the realization functions Φa(·, θ) : RN0 → RNL\\nor Φa : RN0 ×RP (N ) → RNL. However, we want to emphasize that one cannot infer the underlying architecture\\nor properties like magnitude of parameters solely from these functions as the mapping (a, θ) (cid:55)→ Φa(·, θ) is\\nhighly non-injective. As an example, we can set W (L) = 0 which implies Φa(·, θ) = b(L) for all architectures\\na = (N, (cid:37)) and all values of (W ((cid:96)), b((cid:96)))L−1\\n(cid:96)=1 .\\n\\nIn view of our considered prediction tasks in Deﬁnition 1.2, this naturally leads to the following hypothesis\\n\\nsets of neural networks.\\n\\nDeﬁnition 1.6 (Hypothesis sets of neural networks). Let a = (N, (cid:37)) be a NN architecture with input\\ndimension N0 = d, output dimension NL = 1, and measurable activation function (cid:37). For regression tasks the\\ncorresponding hypothesis set is given by\\n\\nFa = (cid:8)Φa(·, θ) : θ ∈ RP (N )(cid:9)\\n\\nand for classiﬁcation tasks by\\n\\nFa,sgn = (cid:8)sgn(Φa(·, θ)) : θ ∈ RP (N )(cid:9), where\\n\\nsgn(x) :=\\n\\n(cid:40)\\n\\n1,\\n−1,\\n\\nif x ≥ 0,\\nif x < 0.\\n\\n6\\n\\n\\x0cName\\n\\nlinear\\n\\nHeaviside / step function\\n\\nlogistic / sigmoid\\n\\nGiven as a function of x ∈ R by\\n\\nPlot\\n\\nx\\n\\n1(0,∞)(x)\\n\\n1\\n1+e−x\\n\\nrectiﬁed linear unit (ReLU)\\n\\nmax{0, x}\\n\\npower rectiﬁed linear unit\\n\\nmax{0, x}k for k ∈ N\\n\\nparametric ReLU (PReLU)\\n\\nmax{ax, x} for a ≥ 0, a (cid:54)= 1\\n\\nexponential linear unit (ELU)\\n\\nx · 1[0,∞)(x) + (ex − 1) · 1(−∞,0)(x)\\n\\nsoftsign\\n\\nx\\n1+|x|\\n\\ninverse square root linear unit\\n\\nx · 1[0,∞)(x) +\\n\\n1+ax2 · 1(−∞,0)(x) for a > 0\\nx√\\n\\ninverse square root unit\\n\\nx√\\n\\n1+ax2\\n\\nfor a > 0\\n\\ntanh\\n\\narctan\\n\\nsoftplus\\n\\nGaussian\\n\\nex−e−x\\nex+e−x\\n\\narctan(x)\\n\\nln(1 + ex)\\n\\ne−x2/2\\n\\nTable 1: List of commonly used activation functions.\\n\\n7\\n\\n\\x0cNote that we compose the output of the NN with the sign function in order to obtain functions mapping\\nto Y = {−1, 1}. This can be generalized to multi-dimensional classiﬁcation tasks by replacing the sign by an\\nargmax function. Given a hypothesis set, a popular learning algorithm is empirical risk minimization (ERM),\\nwhich minimizes the average loss on the given training data, as described in the next deﬁnitions.\\n\\nDeﬁnition 1.7 (Empirical risk). For training data s = (z(i))m\\ndeﬁne the empirical risk by\\n\\ni=1 ∈ Z m and a function f ∈ M(X , Y), we\\n\\n(cid:98)Rs(f ) :=\\n\\n1\\nm\\n\\nm\\n(cid:88)\\n\\ni=1\\n\\nL(f, z(i)).\\n\\nDeﬁnition 1.8 (ERM learning algorithm). Given a hypothesis set F, an empirical risk minimization\\nalgorithm Aerm chooses4 for training data s ∈ Z m a minimizer (cid:98)fs ∈ F of the empirical risk in F, i.e.,\\n\\nAerm(s) ∈ arg min\\n\\nf ∈F\\n\\n(cid:98)Rs(f ).\\n\\n(1.2)\\n\\nRemark 1.9 (Surrogate loss and regularization). Note that, for classiﬁcation tasks, one needs to optimize\\nover non-diﬀerentiable functions with discrete outputs in (1.2). For NN hypothesis sets Fa,sgn one typically\\nuses the corresponding hypothesis set for regression tasks Fa to ﬁnd an approximate minimizer (cid:98)f surr\\n∈ Fa of\\n\\ns\\n\\n1\\nm\\n\\nm\\n(cid:88)\\n\\ni=1\\n\\nLsurr(f, z(i)),\\n\\nwhere Lsurr : M(X , R) × Z → R is a surrogate loss guaranteeing that sgn( (cid:98)f surr\\nfrequently used surrogate loss is the logistic loss5 given by\\n\\ns\\n\\n) ∈ arg minf ∈Fa,sgn (cid:98)Rs(f ). A\\n\\nLsurr(f, z) = log\\n\\n(cid:16)\\n\\n1 + e−yf (x)(cid:17)\\n\\n.\\n\\nIn various learning tasks one also adds regularization terms to the minimization problem in (1.2), such as\\npenalties on the norm of the parameters of the NN, i.e.,\\n\\nmin\\nθ∈RP (N )\\n\\n(cid:98)Rs(Φa(·, θ)) + α(cid:107)θ(cid:107)2\\n2,\\n\\nwhere α ∈ (0, ∞) is a regularization parameter. Note that in this case the minimizer depends on the chosen\\nparameters θ and not only on the realization function Φa(·, θ), see also Remark 1.5.\\n\\nComing back to our initial, informal description of learning in Deﬁnition 1.1, we have now outlined\\npotential learning tasks in Deﬁnition 1.2, NN hypothesis sets in Deﬁnition 1.6, a metric for the in-sample\\nperformance in Deﬁnition 1.7, and a corresponding learning algorithm in Deﬁnition 1.8. However, we are still\\nlacking a mathematical concept to describe the out-of-sample (generalization) performance of our learning\\nalgorithm. This question has been intensively studied in the ﬁeld of statistical learning theory, see Section 1\\nfor various references.\\n\\nIn this ﬁeld one usually establishes a connection between unseen data z and the training data s = (z(i))m\\ni=1\\nby imposing that z and z(i), i ∈ [m], are realizations of independent samples drawn from the same distribution.\\n\\nAssumption 1.10 (Independent and identically distributed data). We assume that z(1), . . . , z(m), z are\\nrealizations of i.i.d. random variables Z (1), . . . , Z (m), Z.\\n\\n4For simplicity, we assume that the minimum is attained which, for instance, is the case if F is a compact topological space\\non which (cid:98)Rs is continuous. Hypothesis sets of NNs F(N,(cid:37)) constitute a compact space if, e.g., one chooses a compact parameter\\nset P ⊂ RP (N ) and a continuous activation function (cid:37). One could also work with approximate minimizers, see [AB99].\\n\\n5This can be viewed as cross-entropy between the label y and the output of f composed with a logistic function (cid:37)σ. In a\\n\\nmulti-dimensional setting one can replace the logistic function with a softmax function.\\n\\n8\\n\\n\\x0cIn this formal setting, we can compute the average out-of-sample performance of a model. Recall from\\nour notation in Section 1.1 that we denote by PZ the image measure of Z on Z, which is the underlying\\ndistribution of our training data S = (Z (i))m\\n\\nZ and unknown data Z ∼ PZ.\\n\\ni=1 ∼ Pm\\n\\nDeﬁnition 1.11 (Risk). For a function f ∈ M(X , Y), we deﬁne6 the risk by\\n\\nR(f ) := E(cid:2)L(f, Z)(cid:3) =\\n\\n(cid:90)\\n\\nZ\\n\\nL(f, z) dPZ(z).\\n\\nDeﬁning S := (Z (i))m\\n\\ni=1, the risk of a model fS = A(S) is thus given by R(fS) = E(cid:2)L(fS, Z)|S(cid:3).\\n\\nFor prediction tasks, we can write Z = (X, Y ), such that the input features and labels are given by an\\nX -valued random variable X and a Y-valued random variable Y , respectively. Note that for classiﬁcation\\ntasks the risk equals the probability of misclassiﬁcation\\n\\nR(f ) = E[1(−∞,0)(Y f (X))] = P[f (X) (cid:54)= Y ].\\n\\nFor noisy data, there might be a positive, lower bound on the risk, i.e., an irreducible error. If the lower\\n\\nbound on the risk is attained, one can also deﬁne the notion of an optimal solution to a learning task.\\n\\nDeﬁnition 1.12 (Bayes-optimal function). A function f ∗ ∈ M(X , Y) achieving the smallest risk, the\\nso-called Bayes risk\\n\\nis called a Bayes-optimal function.\\n\\nR∗ :=\\n\\ninf\\nf ∈M(X ,Y)\\n\\nR(f ),\\n\\nFor the prediction tasks in Deﬁnition 1.2, we can represent the risk of a function with respect to the\\n\\nBayes risk and compute the Bayes-optimal function, see, e.g., [CZ07, Propositions 1.8 and 9.3].\\n\\nLemma 1.1 (Regression and classiﬁcation risk). For a regression task with V[Y ] < ∞, the risk can be\\ndecomposed into\\n\\nR(f ) = E(cid:2)(f (X) − E[Y |X])2(cid:3) + R∗,\\n\\nf ∈ M(X , Y),\\n\\n(1.3)\\n\\nwhich is minimized by the regression function f ∗(x) = E[Y |X = x]. For a classiﬁcation task, the risk can be\\ndecomposed into\\n\\nR(f ) = E(cid:2)|E[Y |X]|1(−∞,0)(E[Y |X]f (X))(cid:3) + R∗,\\nwhich is minimized by the Bayes classiﬁer f ∗(x) = sgn(E[Y |X = x]).\\n\\nf ∈ M(X , Y),\\n\\nAs our model fS is depending on the random training data S, the risk R(fS) is a random variable and\\nwe might aim7 for R(fS) small with high probability or in expectation over the training data. The challenge\\nfor the learning algorithm A is to minimize the risk by only using training data but without knowing the\\nunderlying distribution. One can even show that for every learning algorithm there exists a distribution\\nwhere convergence of the expected risk of fS to the Bayes risk is arbitrarily slow with respect to the number\\nof samples m [DGL96, Theorem 7.2].\\n\\nTheorem 1.13 (No free lunch). Let am ∈ (0, ∞), m ∈ N, be a monotonically decreasing sequence with\\na1 ≤ 1/16. Then for every learning algorithm A of a classiﬁcation task there exists a distribution PZ such\\nthat for every m ∈ N and training data S ∼ Pm\\nZ it holds that\\nE(cid:2)R(A(S))(cid:3) ≥ R∗ + am.\\n\\n6Note that this requires z (cid:55)→ L(f, z) to be measurable for every f ∈ M(X , Y), which is the case for our considered prediction\\n\\ntasks.\\n\\n7In order to make probabilistic statements on R(fS ) we assume that R(fS ) is a random variable, i.e., measurable. This is,\\n\\ne.g., the case if F constitutes a measurable space and s (cid:55)→ A(s) and f → R|F are measurable.\\n\\n9\\n\\n\\x0cFigure 1.2: Illustration of the errors (A)–(C) in the decomposition of (1.4). It shows an exemplary risk (cid:98)R\\n(blue) and empirical risk (cid:98)Rs (red) with respect to the projected space of measurable functions M(X , Y).\\nNote that the empirical risk and thus εgen and εopt depend on the realization s = (z(i))m\\ni=1 of the training\\ndata S ∼ Pm\\nZ .\\n\\nTheorem 1.13 shows the non-existence of a universal learning algorithm for every data distribution PZ and\\nshows that useful bounds must necessarily be accompanied by a priori regularity conditions on the underlying\\ndistribution PZ. Such prior knowledge can then be incorporated in the choice of the hypothesis set F. To\\nillustrate this, let f ∗\\n\\nF ∈ arg minf ∈F R(f ) be a best approximation in F, such that we can bound the error\\n\\nR(fS) − R∗ = R(fS) − (cid:98)RS(fS) + (cid:98)RS(fS) − (cid:98)RS(f ∗\\n≤ εopt + 2εgen + εapprox\\n\\nF ) + (cid:98)RS(f ∗\\n\\nF ) − R(f ∗\\n\\nF ) + R(f ∗\\n\\nF ) − R∗\\n\\n(1.4)\\n\\nby\\n\\n(A) an optimization error εopt := (cid:98)RS(fS) − (cid:98)RS( (cid:98)fS) ≥ (cid:98)RS(fS) − (cid:98)RS(f ∗\\n\\nF ), with (cid:98)fS as in Deﬁnition 1.8,\\n\\n(B) a (uniform8) generalization error εgen := supf ∈F |R(f ) − (cid:98)RS(f )| ≥ max{R(fS) − (cid:98)RS(fS), (cid:98)RS(f ∗\\n\\nF ) −\\n\\nR(f ∗\\n\\nF )}, and\\n\\n(C) an approximation error εapprox := R(f ∗\\n\\nF ) − R∗,\\n\\nsee also Figure 1.2. The approximation error is decreasing when enlarging the hypothesis set, but taking\\nF = M(X , Y) prevents controlling the generalization error, see also Theorem 1.13. This suggests a sweet-spot\\nfor the complexity of our hypothesis set F and is usually referred to as the bias-variance trade-oﬀ, see\\nalso Figure 1.4 below. In the next sections, we will sketch mathematical ideas to tackle each of the errors\\nin (A)–(C) in the context of deep learning. Observe that we bound the generalization and optimization\\nerror with respect to the empirical risk (cid:98)RS and its minimizer (cid:98)fS which is motivated by the fact that in\\ndeep-learning-based applications one typically tries to minimize variants of (cid:98)RS.\\n\\n1.2.1 Optimization\\n\\nThe ﬁrst error in the decomposition of (1.4) is the optimization error: εopt. This error is primarily inﬂuenced\\nby the numerical algorithm A that is used to ﬁnd the model fs in a hypothesis set of NNs for given training\\ndata s ∈ Z m. We will focus on the typical setting where such an algorithm tries to approximately minimize\\nthe empirical risk (cid:98)Rs. While there are many conceivable methods to solve this minimization problem, by\\nfar the most common are gradient-based methods. The main reason for the popularity of gradient-based\\n\\n8Although this uniform deviation can be a coarse estimate it is frequently considered to allow for the application of uniform\\n\\nlaws of large numbers from the theory of empirical processes.\\n\\n10\\n\\n\\x0cmethods is that for FC networks as in Deﬁnition 1.4, the accurate and eﬃcient computation of pointwise\\nderivatives ∇θΦa(x, θ) is possible by means of automatic diﬀerentiation, a speciﬁc form of which is often\\nreferred to as the backpropagation algorithm [Kel60, Dre62, Lin70, RHW86, GW08]. This numerical scheme\\nis also applicable in general settings, such as, when the architecture of the NN is given by a general directed\\nacyclic graph. Using these pointwise derivatives, one usually attempts to minimize the empirical risk (cid:98)Rs by\\nupdating the parameters θ according to a variant of stochastic gradient descent (SGD), which we shall review\\nbelow in a general formulation:\\n\\nAlgorithm 1: Stochastic gradient descent\\n\\nInput\\n\\n: Diﬀerentiable function r : Rp → R, sequence of step-sizes ηk ∈ (0, ∞), k ∈ [K],\\n\\nRp-valued random variable Θ(0).\\n\\nOutput : Sequence of Rp-valued random variables (Θ(k))K\\nfor k = 1, . . . , K do\\n\\nk=1.\\n\\nLet D(k) be a random variable such that E[D(k)|Θ(k−1)] = ∇r(Θ(k−1));\\nSet Θ(k) := Θ(k−1) − ηkD(k);\\n\\nend\\n\\nIf D(k) is chosen deterministically in Algorithm 1, i.e., D(k) = ∇r(Θ(k−1)), then the algorithm is known as\\ngradient descent. To minimize the empirical loss, we apply SGD with r : RP (N ) → R set to r(θ) = (cid:98)Rs(Φa(·, θ)).\\nMore concretely, one might choose a batch-size m(cid:48) ∈ N with m(cid:48) ≤ m and consider the iteration\\n\\nΘ(k) := Θ(k−1) −\\n\\nηk\\nm(cid:48)\\n\\n(cid:88)\\n\\nz∈S(cid:48)\\n\\n∇θL(Φa(·, Θ(k−1)), z),\\n\\n(1.5)\\n\\nwhere S(cid:48) is a so-called mini-batch of size |S(cid:48)| = m(cid:48) chosen uniformly9 at random from the training data\\ns. The sequence of step-sizes (ηk)k∈N is often called learning rate in this context. Stopping at step K, the\\noutput of a deep learning algorithm A is then given by\\n\\nfs = A(s) = Φa(·, ¯θ),\\nwhere ¯θ can be chosen to be the realization of the last parameter Θ(K) of (1.5) or a convex combination of\\n(Θ(k))K\\n\\nk=1 such as the mean.\\n\\nAlgorithm 1 was originally introduced in [RM51] in the context of ﬁnding the root of a nondecreasing\\nfunction from noisy measurements. Shortly afterwards this idea was applied to ﬁnd a unique minimum of a\\nLipschitz-regular function that has no ﬂat regions away from the global minimum [KW52].\\n\\nIn some regimes, we can guarantee convergence of SGD at least in expectation, see [NY83, NJLS09,\\nSSSSS09], [SDR14, Section 5.9], [SSBD14, Chapter 14]. One prototypical convergence guarantee that is found\\nin the aforementioned references in various forms is stated below.\\nTheorem 1.14 (Convergence of SGD). Let p, K ∈ N and let r : Rp ⊃ B1(0) → R be diﬀerentiable and\\nk=1 be the output of Algorithm 1 with initialization Θ(0) = 0, step-sizes ηk = K −1/2,\\nconvex. Further let (Θ(k))K\\nk ∈ [K], and random variables (D(k))K\\n\\nk=1 satisfying that (cid:107)D(k)(cid:107)2 ≤ 1 almost surely for all k ∈ [K]. Then\\n\\nE[r( ¯Θ)] − r(θ∗) ≤\\n\\n1\\n√\\nK\\n\\n,\\n\\nwhere ¯Θ := 1\\nK\\n\\n(cid:80)K\\n\\nk=1 Θ(k) and θ∗ ∈ arg minθ∈B1(0) r(θ).\\n\\nTheorem 1.14 can be strengthened to yield a faster convergence rate if the convexity is replaced by strict\\nconvexity. If r is not convex, then convergence to a global minimum can in general not be guaranteed. In\\nfact, in that case, stochastic gradient descent may converge to a local, non-global minimum, see Figure 1.3\\nfor an example.\\n\\n9We remark that in practice one typically picks S(cid:48) by selecting a subset of training data in a way to cover the full training\\ndata after one epoch of (cid:100)m/m(cid:48)(cid:101) many steps. This, however, does not necessarily yield an unbiased estimator D(k) of ∇θr(Θ(k−1))\\ngiven Θ(k−1).\\n\\n11\\n\\n\\x0cFigure 1.3: Examples of the dynamics of gradient descent (left) and stochastic gradient descent (right) for an\\nobjective function with one non-global minimum next to the global minimum. We see that depending on the\\ninitial condition and also on ﬂuctuations in the stochastic part of SGD the algorithm can fail or succeed in\\nﬁnding the global minimum.\\n\\nMoreover, gradient descent, i.e., the deterministic version of Algorithm 1, will stop progressing if at any\\npoint the gradient of r vanishes. This is the case in every stationary point of r. A stationary point is either a\\nlocal minimum, a local maximum, or a saddle point. One would expect that if the direction of the step D(k)\\nin Algorithm 1 is not deterministic, then the random ﬂuctuations may allow the iterates to escape saddle\\npoints. Indeed, results guaranteeing convergence to local minima exist under various conditions on the type\\nof saddle points that r admits, [NJLS09, GL13, GHJY15, LSJR16, JKNvW20].\\n\\nIn addition, many methods that improve the convergence by, for example, introducing more elaborate\\nstep-size rules or a momentum term have been established. We shall not review these methods here, but\\ninstead refer to [GBC16, Chapter 8] for an overview.\\n\\n1.2.2 Approximation\\n\\nGenerally speaking, NNs, even FC NNs (see Deﬁnition 1.4) with only L = 2 layers, are universal approximators,\\nmeaning that under weak conditions on the activation function (cid:37) they can approximate any continuous\\nfunction on a compact set up to arbitrary precision [Cyb89, Fun89, HSW89, LLPS93].\\n\\nTheorem 1.15 (Universal approximation theorem). Let d ∈ N, let K ⊂ Rd be compact, and let (cid:37) ∈ L∞\\nloc(R)\\nbe an activation function such that the closure of the points of discontinuity of (cid:37) is a Lebesgue null set.\\nFurther let\\n\\n(cid:91)\\n\\n(cid:101)F :=\\n\\nF((d,n,1),(cid:37))\\n\\nn∈N\\n\\nbe the corresponding set of two-layer NN realizations. Then it holds that C(K) ⊂ cl( (cid:101)F) (where the closure is\\ntaken with respect to the topology induced by the L∞(K)-norm) if and only if there does not exist a polynomial\\np : R → R with p = (cid:37) almost everywhere.\\n\\nThe theorem can be proven by the theorem of Hahn–Banach, which implies that (cid:101)F being dense in some\\n\\n12\\n\\n\\x0creal normed vector space S is equivalent to the following condition: For all non-trivial functionals F ∈ S (cid:48) \\\\ {0}\\nfrom the topological dual space of S there exist parameters w ∈ Rd and b ∈ R such that\\n\\nF ((cid:37)((cid:104)w, ·(cid:105) + b)) (cid:54)= 0.\\n\\n(cid:90)\\n\\nIn case of S = C(K) we have by the Riesz–Markov–Kakutani representation theorem that S (cid:48) is the space of\\nsigned Borel measures on K, see [Rud06]. Therefore, Theorem 1.15 holds, if (cid:37) is such that, for a signed Borel\\nmeasure µ,\\n\\n(cid:37)((cid:104)w, x(cid:105) + b) dµ(x) = 0\\n\\n(1.6)\\n\\nK\\n\\nfor all w ∈ Rd and b ∈ R implies that µ = 0. An activation function (cid:37) satisfying this condition is called\\ndiscriminatory. It is not hard to see that any sigmoidal (cid:37) is discriminatory. Indeed, assume that (cid:37) satisﬁes (1.6)\\nfor all w ∈ Rd and b ∈ R. Since for every x ∈ Rd it holds that (cid:37)(ax + b) → 1(0,∞)(x) + (cid:37)(b)1{0}(x) for a → ∞,\\nwe conclude by superposition and passing to the limit that for all c1, c2 ∈ R and w ∈ Rd, b ∈ R\\n\\n(cid:90)\\n\\nK\\n\\n1[c1,c2]((cid:104)w, x(cid:105) + b) dµ(x) = 0.\\n\\nRepresenting the exponential function x (cid:55)→ e−2πix as the limit of sums of elementary functions yields that\\n(cid:82)\\nK e−2πi((cid:104)w,x(cid:105)+b) dµ(x) = 0 for all w ∈ Rd, b ∈ R. Hence, the Fourier transform of µ vanishes which implies\\nthat µ = 0.\\n\\nTheorem 1.15 addresses a uniform approximation problem on a general compact set. If we are given a\\nﬁnite number of points and only care about good approximation at these points, then one can ask if this\\napproximation problem is potentially simpler. Below we see that, if the number of neurons is larger or equal\\nto the number of data points, then one can always interpolate, i.e., exactly ﬁt the data on a given ﬁnite\\nnumber of points.\\n\\nProposition 1.1 (Interpolation). Let d, m ∈ N, let x(i) ∈ Rd, i ∈ [m], with x(i) (cid:54)= x(j) for i (cid:54)= j, let\\n(cid:37) ∈ C(R), and assume that (cid:37) is not a polynomial. Then, there exist parameters θ(1) ∈ Rm×d × Rm with the\\nfollowing property: For every k ∈ N and every sequence of labels y(i) ∈ Rk, i ∈ [m], there exist parameters\\nθ(2) = (W (2), 0) ∈ Rk×m × Rk for the second layer of the NN architecture a = ((d, m, k), (cid:37)) such that\\n\\nΦa(x(i), (θ(1), θ(2))) = y(i),\\n\\ni ∈ [m].\\n\\nLet us sketch the proof in the following. First, note that Theorem 1.15 also holds for functions g ∈ C(K, Rm)\\nwith multi-dimensional output by approximating each one-dimensional component x (cid:55)→ (g(x))i and stacking\\nthe resulting networks. Second, one can add an additional row containing only zeros to the weight matrix\\nW (1) of the approximating neural network as well as an additional entry to the vector b(1). The eﬀect of\\nthis is that we obtain an additional neuron with constant output. Since (cid:37) (cid:54)= 0, we can choose b(1) such that\\nthe output of this neuron is not zero. Therefore, we can include the bias vector b(2) of the second layer\\ninto the weight matrix W (2), see also Remark 1.5. Now choose g ∈ C(Rm, Rm) to be a function satisfying\\ng(x(i)) = e(i), i ∈ [m], where e(i) ∈ Rm denotes the i-th standard basis vector. By the discussion before there\\nexists a neural network architecture ˜a = ((d, n, m), (cid:37)) and parameters ˜θ = (((cid:102)W (1), ˜b(1)), ((cid:102)W (2), 0)) such that\\n\\n(cid:107)Φ˜a(·, ˜θ) − g(cid:107)L∞(K) <\\n\\n1\\nm\\n\\n,\\n\\n(1.7)\\n\\nwhere K is a compact set with x(i) ∈ K, i ∈ [m]. Let us abbreviate the output of the activations in the ﬁrst\\nlayer evaluated at the input features by\\n\\n(cid:101)A :=\\n\\n(cid:104)\\n(cid:37)((cid:102)W (1)(x(1)) + ˜b(1))) . . . (cid:37)((cid:102)W (1)(x(m)) + ˜b(1)))\\n\\n(cid:105)\\n\\n∈ Rn×m.\\n\\nThe equivalence of the max and operator norm and (1.7) establish that\\n\\n(cid:107)(cid:102)W (2) (cid:101)A − Im(cid:107)op ≤ m max\\ni,j∈[m]\\n\\n(cid:12)\\n(cid:12)((cid:102)W (2) (cid:101)A − Im)i,j\\n\\n(cid:12)\\n(cid:12) = m max\\nj∈[m]\\n\\n(cid:107)Φ˜a(x(j), ˜θ) − g(x(j))(cid:107)∞ < 1,\\n\\n13\\n\\n\\x0cwhere Im denotes the m × m identity matrix. Thus, the matrix (cid:102)W (2) (cid:101)A ∈ Rm×m needs to have full rank and\\nwe can extract m linearly independent rows from (cid:101)A resulting in an invertible matrix A ∈ Rm×m. Now, we\\ndeﬁne the desired parameters θ(1) for the ﬁrst layer by extracting the corresponding rows from (cid:102)W (1) and ˜b(1)\\nand the parameters θ(2) of the second layer by\\n\\nW (2) := (cid:2)y(1) . . . y(m)(cid:3) A−1 ∈ Rk×m.\\n\\nThis proves that with any discriminatory activation function we can interpolate arbitrary training data\\n(x(i), y(i)) ∈ Rd × Rk, i ∈ [m], using a two-layer NN with m hidden neurons, i.e., O(m(d + k)) parameters.\\nOne can also ﬁrst project the input features to a one-dimensional line where they are separated and then\\napply Proposition 1.1 with d = 1. For nearly all activation functions, this can be represented by a three-layer\\nNN using only O(d + mk) parameters10.\\n\\nBeyond interpolation results, one can obtain a quantitative version of Theorem 1.15 if one knows additional\\nregularity properties of the Bayes optimal function f ∗, such as smoothness, compositionality, and symmetries.\\nFor surveys on such results, we refer the reader to [DHP20, GRK20]. For instructive purposes, we review one\\nsuch result, which can be found in [Mha96, Theorem 2.1], below:\\nTheorem 1.16 (Approximation of smooth functions). Let d, k ∈ N and p ∈ [1, ∞]. Further let (cid:37) ∈ C∞(R)\\nand assume that (cid:37) is not a polynomial. Then there exists a constant c ∈ (0, ∞) with the following property: For\\nevery n ∈ N there exist parameters θ(1) ∈ Rn×d × Rn for the ﬁrst layer of the NN architecture a = ((d, n, 1), (cid:37))\\nsuch that for every g ∈ W k,p((0, 1)d) it holds that\\n\\ninf\\nθ(2)∈R1×n×R\\n\\n(cid:107)Φa(·, (θ(1), θ(2))) − g(cid:107)Lp((0,1)d) ≤ cn− d\\n\\nk (cid:107)g(cid:107)W k,p((0,1)d).\\n\\nTheorem 1.16 shows that NNs achieve the same optimal approximation rates that, for example, spline-\\nbased approximation yields for smooth functions. The idea behind this theorem is based on a strategy that is\\nemployed repeatedly throughout the literature. This is the idea of re-approximating classical approximation\\nmethods by NNs and thereby transferring the approximation rates of these methods to NNs. In the example\\nof Theorem 1.16, approximation by polynomials is used. The idea is that due to the non-vanishing derivatives\\nof the activation function11, one can approximate every univariate polynomial via divided diﬀerences of the\\nactivation function. Speciﬁcally, accepting unbounded parameter magnitudes, for any activation function\\n(cid:37) : R → R which is p-times diﬀerentiable at some point λ ∈ R with (cid:37)(p)(λ) (cid:54)= 0, one can approximate the\\nmonomial x (cid:55)→ xp on a compact set K ⊂ R up to arbitrary precision by a ﬁxed-size NN via rescaled p-th\\norder diﬀerence quotients as\\n\\nlim\\nh→0\\n\\nsup\\nx∈K\\n\\n(cid:12)\\n(cid:12)\\n(cid:12)\\n\\np\\n(cid:88)\\n\\ni=0\\n\\n(cid:1)\\n\\n(−1)i(cid:0)p\\nhp(cid:37)(p)(λ)\\n\\ni\\n\\n(cid:37)(cid:0)(p/2 − i)hx + λ(cid:1) − xp(cid:12)\\n(cid:12)\\n(cid:12) = 0.\\n\\n(1.8)\\n\\nLet us end this subsection by clarifying the connection of the approximation results above to the error\\ndecomposition of (1.4). Consider, for simplicity, a regression task with quadratic loss. Then, the approximation\\nerror εapprox equals a common L2-error\\n\\nεapprox = R(f ∗\\n\\nF ) − R∗ (∗)\\n=\\n\\n(cid:90)\\n\\nX\\n\\n(f ∗\\n\\nF (x) − f ∗(x))2 dPX (x)\\n\\n(∗)\\n= min\\nf ∈F\\n\\n(cid:107)f − f ∗(cid:107)2\\n\\nL2(PX )\\n\\n≤ min\\nf ∈F\\n\\n(cid:107)f − f ∗(cid:107)2\\n\\nL∞(X ),\\n\\nwhere the identities marked by (∗) follow from Lemma 1.1. Hence, Theorem 1.15 postulates that εapprox → 0\\nfor increasing NN sizes, whereas Theorem 1.16 additionally explains how fast εapprox converges to 0.\\n\\n10To avoid the m × d weight matrix (without using shared parameters as in [ZBH+17]) one interjects an approximate one-\\ndimensional identity [PV18, Deﬁnition 2.5], which can be arbitrarily well approximated by a NN with architecture a = ((1, 2, 1), (cid:37))\\ngiven that (cid:37)(cid:48)(λ) (cid:54)= 0 for some λ ∈ R, see (1.8) below.\\n\\n11The Baire category theorem ensures that for a non-polynomial (cid:37) ∈ C∞(R) there exists λ ∈ R with (cid:37)(p)(λ) (cid:54)= 0 for all p ∈ N,\\n\\nsee, e.g., [Don69, Chapter 10].\\n\\n14\\n\\n\\x0c1.2.3 Generalization\\n\\nTowards bounding the generalization error εgen = supf ∈F |R(f ) − (cid:98)RS(f )|, one observes that, for every f ∈ F,\\nAssumption 1.10 ensures that L(f, Z (i)), i ∈ [m], are i.i.d. random variables. Thus, one can make use of\\nconcentration inequalities to bound the deviation of the empirical risk (cid:98)RS(f ) = 1\\ni=1 L(f, Z (i)) from its\\nm\\nexpectation R(f ). For instance, assuming boundedness12 of the loss, Hoeﬀding’s inequality [Hoe63] and a\\nunion bound directly imply the following generalization guarantee for countable, weighted hypothesis sets F,\\nsee, e.g., [BBL03].\\n\\n(cid:80)m\\n\\nTheorem 1.17 (Generalization bound for countable, weighted hypothesis sets). Let m ∈ N, δ ∈ (0, 1) and\\nassume that F is countable. Further let p be a probability distribution on F and assume that L(f, Z) ∈ [0, 1]\\nalmost surely for every f ∈ F. Then with probability 1 − δ (with respect to repeated sampling of Pm\\nZ -distributed\\ntraining data S) it holds for every f ∈ F that\\n\\n|R(f ) − (cid:98)RS(f )| ≤\\n\\n(cid:114)\\n\\nln(1/p(f )) + ln(2/δ)\\n2m\\n\\n.\\n\\nWhile the weighting p needs to be chosen before seeing the training data, one could incorporate prior\\ninformation on the learning algorithm A. For ﬁnite hypothesis sets without prior information, setting\\np(f ) = 1/|F| for every f ∈ F, Theorem 1.17 implies that, with high probability, it holds that\\n\\nεgen (cid:46)\\n\\n(cid:114)\\n\\nln(|F|)\\nm\\n\\n.\\n\\n(1.9)\\n\\nAgain, one notices that, in line with the bias-variance trade-oﬀ, the generalization bound is increasing with\\nthe size of the hypothesis set |F|. Although in practice the parameters θ ∈ RP (N ) of a NN are discretized\\naccording to ﬂoating-point arithmetic, the corresponding quantities |Fa| or |Fa,sgn| would be huge and we\\nneed to ﬁnd a replacement for the ﬁniteness condition.\\n\\nWe will focus on binary classiﬁcation tasks and present a main result of VC theory which is to a great\\nextent derived from the work of Vladimir Vapnik and Alexey Chervonenkis [VC71]. While in (1.9) we counted\\nthe number of functions in F, we now reﬁne this analysis to the number of functions restricted to a ﬁnite\\nsubset of X , given by the growth function\\n\\ngrowth(m, F) :=\\n\\nmax\\ni=1∈X m\\n\\n(x(i))m\\n\\n|{f |(x(i))m\\n\\ni=1\\n\\n: f ∈ F}|.\\n\\nThe growth function can be interpreted as the maximal number of classiﬁcation patterns in {−1, 1}m which\\nfunctions in F can realize on m points and thus growth(m, F) ≤ 2m. The asymptotic behavior of the growth\\nfunction is determined by a single intrinsic dimension of our hypothesis set F, the so-called VC-dimension\\nVCdim(F) := sup (cid:8)m ∈ N ∪ {0} : growth(m, F) = 2m(cid:9),\\n\\nwhich deﬁnes the largest number of points such that F can realize any classiﬁcation pattern, see, e.g., [AB99,\\nBBL03]. There exist various results on VC-dimensions of NNs with diﬀerent activation functions, see,\\nfor instance, [BH89, KM97, BMM98, Sak99]. We present the result of [BMM98] for piecewise polynomial\\nactivation functions (cid:37). It establishes a bound on the VC-dimension of hypothesis sets of NNs for classiﬁcation\\ntasks F(N,(cid:37)),sgn that scales, up to logarithmic factors, linear in the number of parameters P (N ) and quadratic\\nin the number of layers L.\\n\\nTheorem 1.18 (VC-dimension of neural network hypothesis sets). Let (cid:37) be a piecewise polynomial activation\\nfunction. Then there exists a constant c ∈ (0, ∞) such that for every L ∈ N and N ∈ NL+1 it holds that\\n\\nVCdim(F(N,(cid:37)),sgn) ≤ c(cid:0)P (N )L log(P (N )) + P (N )L2(cid:1).\\n\\n12Note that for our classiﬁcation tasks in Deﬁnition 1.2 it holds that L(f, Z) ∈ {0, 1} for every f ∈ F . For the regression\\ntasks, one typically assumes boundedness conditions, such as |Y | ≤ c and supf ∈F |f (X)| ≤ c almost surely for some c ∈ (0, ∞),\\nwhich yields that supf ∈F |L(f, Z)| ≤ 4c2.\\n\\n15\\n\\n\\x0cGiven (x(i))m\\n\\ni=1 ∈ X m, there exists a partition of RP (N ) such that Φ(x(i), ·), i ∈ [m], are polynomials on\\neach region of the partition. The proof of Theorem 1.18 is based on bounding the number of such regions\\nand the number of classiﬁcation patterns of a set of polynomials.\\n\\nA ﬁnite VC-dimension ensures the following generalization bound [Tal94, AB99]:\\n\\nTheorem 1.19 (VC-dimension generalization bound). There exists a constant c ∈ (0, ∞) with the following\\nproperty: For every classiﬁcation task as in Deﬁnition 1.2, every Z-valued random variable Z, and every\\nm ∈ N, δ ∈ (0, 1) it holds with probability 1 − δ (with respect to repeated sampling of Pm\\nZ -distributed training\\ndata S) that\\n\\n|R(f ) − (cid:98)RS(f )| ≤ c\\n\\nsup\\nf ∈F\\n\\n(cid:114)\\n\\nVCdim(F) + log(1/δ))\\nm\\n\\n.\\n\\nIn summary, using NN hypothesis sets F(N,(cid:37)),sgn with a ﬁxed depth and piecewise polynomial activation\\n\\n(cid:37) for a classiﬁcation task, with high probability it holds that\\n\\n(cid:114)\\n\\nεgen (cid:46)\\n\\nP (N ) log(P (N ))\\nm\\n\\n.\\n\\n(1.10)\\n\\nIn the remainder of this section we will sketch a proof of Theorem 1.19 and, in doing so, present\\nfurther concepts and complexity measures connected to generalization bounds. We start by observing that\\nMcDiarmid’s inequality [McD89] ensures that εgen is sharply concentrated around its expectation, i.e., with\\nprobability 1 − δ it holds that13\\n\\n(cid:12)εgen − E(cid:2)εgen(cid:3)(cid:12)\\n(cid:12)\\n\\n(cid:12) (cid:46)\\n\\n(cid:114)\\n\\nlog(1/δ)\\nm\\n\\n.\\n\\n(1.11)\\n\\nTo estimate the expectation of the uniform generalization error we employ a symmetrization argu-\\ni=1 ∼ Pm\\nZ be a test data set independent\\n(cid:101)S(f )]. By properties of the conditional expectation and Jensen’s inequality\\n\\nment [GZ84]. Deﬁne G := L ◦ F := {L(f, ·) : f ∈ F}, let (cid:101)S = ( (cid:101)Z (i))m\\nof S, and note that R(f ) = E[ (cid:98)R\\nit holds that\\n\\nE(cid:2)εgen(cid:3) = E\\n\\n(cid:104)\\n\\nsup\\nf ∈F\\n\\n(cid:105)\\n|R(f ) − (cid:98)RS(f )|\\n\\n= E\\n\\n≤ E\\n\\n= E\\n\\n(cid:104)\\n\\n(cid:104)\\n\\n(cid:104)\\n\\nsup\\ng∈G\\n\\nsup\\ng∈G\\n\\nsup\\ng∈G\\n\\n1\\nm\\n\\n(cid:12)\\n(cid:12)\\n\\n1\\nm\\n\\n(cid:12)\\n(cid:12)\\n\\n1\\nm\\n\\n(cid:12)\\n(cid:12)\\n\\n(cid:104)\\n\\n≤ 2E\\n\\n1\\nm\\n\\nsup\\ng∈G\\n\\nm\\n(cid:88)\\n\\ni=1\\nm\\n(cid:88)\\n\\ni=1\\nm\\n(cid:88)\\n\\nE(cid:2)g( (cid:101)Z (i)) − g(Z (i))|S(cid:3)(cid:12)\\n(cid:12)\\n\\n(cid:105)\\n\\ng( (cid:101)Z (i)) − g(Z (i))(cid:12)\\n(cid:12)\\n\\n(cid:105)\\n\\n(cid:0)g( (cid:101)Z (i)) − g(Z (i))(cid:1)(cid:12)\\n(cid:12)\\n\\nτi\\n\\n(cid:105)\\n\\ni=1\\nm\\n(cid:88)\\n\\n(cid:12)\\n(cid:12)\\n\\ni=1\\n\\nτig(Z (i))(cid:12)\\n(cid:12)\\n\\n(cid:105)\\n,\\n\\nwhere we used that multiplications with Rademacher variables (τ1, . . . , τm) ∼ U({−1, 1}m) only amount\\nto interchanging Z (i) with (cid:101)Z (i) which has no eﬀect on the expectation, since Z (i) and (cid:101)Z (i) have the same\\ndistribution. The quantity\\n\\nRm(G) := E\\n\\n(cid:104)\\n\\n(cid:12)\\n(cid:12)\\n\\n1\\nm\\n\\nsup\\ng∈G\\n\\n(cid:105)\\nτig(Z (i))(cid:12)\\n(cid:12)\\n\\nm\\n(cid:88)\\n\\ni=1\\n\\nis called the Rademacher complexity 14 of G. One can also prove a corresponding lower bound [vdVW97], i.e.,\\n\\nRm(G) −\\n\\n1\\n√\\nm\\n\\n(cid:46) E(cid:2)εgen(cid:3) (cid:46) Rm(G).\\n\\n(1.12)\\n\\n13For precise conditions to ensure that the expectation of εgen is well-deﬁned, we refer the reader to [vdVW97, Dud14].\\n14Due to our decomposition in (1.4), we want to uniformly bound the absolute value of the diﬀerence between the risk and\\nthe empirical risk. It is also common to just bound supf ∈F R(f ) − (cid:98)RS (f ) leading to a deﬁnition of the Rademacher complexity\\nwithout the absolute values which can be easier to deal with.\\n\\n16\\n\\n\\x0cNow we use a chaining method to bound the Rademacher complexity of F by covering numbers on diﬀerent\\nscales. Speciﬁcally, Dudley’s entropy integral [Dud67, LT91] implies that\\n\\nwhere\\n\\nRm(G) (cid:46) E\\n\\n(cid:104) (cid:90) ∞\\n\\n(cid:114)\\n\\n0\\n\\nlog Nα(G, dS)\\nm\\n\\n(cid:105)\\n\\n,\\n\\ndα\\n\\nNα(G, dS) := inf\\n\\n(cid:110)\\n\\n|G| : G ⊂ G, G ⊂\\n\\n(cid:111)\\n\\nBdS\\n\\nα (g)\\n\\n(cid:91)\\n\\ng∈G\\n\\n(1.13)\\n\\ndenotes the covering number with respect to the (random) pseudometric given by\\n\\ndS(f, g) = d(Z(i))m\\n\\ni=1\\n\\n(f, g) :=\\n\\n(cid:118)\\n(cid:117)\\n(cid:117)\\n(cid:116)\\n\\n1\\nm\\n\\nm\\n(cid:88)\\n\\ni=1\\n\\n(cid:0)f (Z (i)) − g(Z (i))(cid:1)2\\n\\n.\\n\\nFor the 0-1 loss L(f, z) = 1(−∞,0)(yf (x)) = (1 − f (x)y)/2, we can get rid of the loss function by the fact that\\n\\nNα(G, dS) = N2α(F, d(X (i))m\\n\\ni=1\\n\\n).\\n\\n(1.14)\\n\\nThe proof is completed by combining the inequalities in (1.11), (1.12), (1.13) and (1.14) with a result of\\nDavid Haussler [Hau95] which shows that for α ∈ (0, 1) we have\\n\\nlog(Nα(F, d(X (i))m\\n\\ni=1\\n\\n)) (cid:46) VCdim(F) log(1/α).\\n\\n(1.15)\\n\\nWe remark that this resembles a typical behavior of covering numbers. For instance, the logarithm of the\\ncovering number log(Nα(M)) of a compact d-dimensional Riemannian manifold M essentially scales like\\nd log(1/α). Finally, note that there exists a similar bound to the one in (1.15) for bounded regression tasks\\nmaking use of the so-called fat-shattering dimension [MV03, Theorem 1].\\n\\n1.3 Do we need a new theory?\\n\\nDespite the already substantial insight that the classical theories provide, a lot of open questions remain. We\\nwill outline these questions below. The remainder of this article then collects modern approaches to explain\\nthe following issues:\\n\\nWhy do large neural networks not overﬁt? In Subsection 1.2.2, we have observed that three-layer\\nNNs with commonly used activation functions and only O(d + m) parameters can interpolate any training\\ndata (x(i), y(i)) ∈ Rd × R, i ∈ [m]. While this speciﬁc representation might not be found in practice, [ZBH+17]\\nindeed trained convolutional15 NNs with ReLU activation function and about 1.6 million parameters to\\nachieve zero empirical risk on m = 50000 training images of the CIFAR10 dataset [KH09] with 32 × 32 pixels\\nper image, i.e., d = 1024. For such large NNs, generalization bounds scaling with the number of parameters\\nP (N ) as the VC-dimension bound in (1.10) are vacuous. However, they observed close to state-of-the-art\\ngeneralization performance16.\\n\\nGenerally speaking, NNs in practice are observed to generalize well despite having more parameters than\\ntraining samples (usually referred to as overparametrization) and approximately interpolating the training data\\n(usually referred to as overﬁtting). As we cannot perform any better on the training data, there is no trade-oﬀ\\nbetween ﬁt to training data and complexity of the hypothesis set F happening, seemingly contradicting\\nthe classical bias-variance trade-oﬀ of statistical learning theory. This is quite surprising, especially given\\nthe following additional empirical observations in this regime, see [NTS14, ZBH+17, NBMS17, BHMM19,\\nNKB+20]:\\n\\n15The basic deﬁnition of a convolutional NN will be given in Section 6. In [ZBH+17] more elaborate versions such as an\\n\\nInception architecture [SLJ+15] are employed.\\n\\n16In practice one usually cannot measure the risk R(fs) and instead evaluates the performance of a trained model fs by\\n(cid:98)R˜s(fs) using test data ˜s, i.e., realizations of i.i.d. random variables distributed according to PZ and drawn independently of the\\ntraining data. In this context one often calls Rs(fs) the training error and R˜s(fs) the test error.\\n\\n17\\n\\n\\x0cFigure 1.4: The ﬁrst plot (and its semi-log inset) shows median and interquartile range of the test and training\\nerrors of ten independent linear regressions with m = 20 samples, polynomial input features X = (1, Z, . . . , Z d)\\nof degree d ∈ [40], and labels Y = f ∗(Z) + ν, where Z ∼ U([−0.5, 0.5]), f ∗ is a polynomial of degree three,\\nand ν ∼ N (0, 0.01). This clearly reﬂects the classical u-shaped bias-variance curve with a sweet-spot at d = 3\\nand drastic overﬁtting beyond the interpolation threshold at d = 20. However, the second plot shows that\\nwe can control the complexity of our hypothesis set of linear models by restricting the Euclidean norm of\\ntheir parameters using ridge regression with a small regularization parameter α = 10−3, i.e., minimizing\\nthe regularized empirical risk 1\\n2, where Φ(·, θ) = (cid:104)θ, ·(cid:105). Corresponding\\nm\\nexamples of (cid:98)fs are depicted in the last plot.\\n\\ni=1(Φ(X (i), θ) − Y (i))2 + α(cid:107)θ(cid:107)2\\n\\n(cid:80)m\\n\\n1. Zero training error on random labels: Zero empirical risk can also be achieved for random labels using\\nthe same architecture and training scheme with only slightly increased training time: This suggests\\nthat the considered hypothesis set of NNs F can ﬁt arbitrary binary labels, which would imply that\\nVCdim(F) ≈ m or Rm(F) ≈ 1 rendering our uniform generalization bounds in Theorem 1.19 and\\nin (1.12) vacuous.\\n\\n2. Lack of explicit regularization: The test error depends only mildly on explicit regularization like norm-\\nbased penalty terms or dropout (see [G´er17] for an explanation of diﬀerent regularization methods): As\\nsuch regularization methods are typically used to decrease the complexity of F, one might ask if there\\nis any implicit regularization (see Figure 1.4), constraining the range of our learning algorithm A to\\nsome smaller, potentially data-dependent subset, i.e., A(s) ∈ (cid:101)Fs (cid:40) F.\\n\\n3. Dependence on the optimization: The same NN trained to zero empirical risk using diﬀerent variants of\\nSGD or starting from diﬀerent initializations can exhibit diﬀerent test errors: This indicates that the\\ndynamics of gradient descent and properties of the local neighborhood around the model fs = A(s)\\nmight be correlated with generalization performance.\\n\\n4. Interpolation of noisy training data: One still observes low test error when training up to approximately\\nzero empirical risk using a regression (or surrogate) loss on noisy training data. This is particularly\\ninteresting, as the noise is captured by the model but seems not to hurt generalization performance.\\n\\n5. Further overparametrization improves generalization performance: Further increasing the NN size can\\nlead to even lower test error: Together with the previous item, this might ask for a diﬀerent treatment\\nof models complex enough to ﬁt the training data. According to the traditional lore “The training error\\ntends to decrease whenever we increase the model complexity, that is, whenever we ﬁt the data harder.\\nHowever with too much ﬁtting, the model adapts itself too closely to the training data, and will not\\ngeneralize well (i.e., have large test error)”, [HTF01]. While this ﬂawlessly describes the situation for\\ncertain machine learning tasks (see Figure 1.4), it seems not to be directly applicable here.\\n\\nIn summary, this suggests that the generalization performance of NNs depends on an interplay of the data\\ndistribution PZ combined with properties of the learning algorithm A, such as the optimization procedure\\nand its range. In particular, classical uniform bounds as in Item (B) of our error decomposition might only\\n\\n18\\n\\n061218d0.000.050.10linear regression (=0)testtrain02040d1010101108061218d0.000.050.10ridge regression (=0.001)testtrain02040d1021010.50.00.5x012d=40=0=0.001f*training data\\x0cdeliver insuﬃcient explanation, see also [NK19]. The mismatch between predictions of classical theory and\\nthe practical generalization performance of deep NNs is often referred to as generalization puzzle. In Section 2\\nwe will present possible explanations for this phenomenon.\\n\\nWhat is the role of depth? We have seen in Subsection 1.2.2 that NNs can closely approximate every\\nfunction if they are suﬃciently wide [Cyb89, Fun89, HSW89]. There are additional classical results that even\\nprovide a trade-oﬀ between the width and the approximation accuracy [CLM94, Mha96, MP99]. In these\\nresults, the central concept is the width of a NN. In modern applications, however at least as much focus if\\nnot more lies on the depth of the underlying architectures, which can have more than 1000 layers [HZRS16].\\nAfter all, the depth of NNs is responsible for the name of deep learning.\\n\\nThis consideration begs the question of whether there is a concrete mathematically quantiﬁable beneﬁt of\\ndeep architectures over shallow NNs. Indeed, we will see eﬀects of depth at many places throughout this\\nmanuscript. However, one of the aspects of deep learning that is most clearly aﬀected by deep architectures\\nis the approximation theoretical aspect. In this framework, we will discuss in Section 3 multiple approaches\\nthat describe the eﬀect of depth.\\n\\nWhy do neural networks perform well in very high-dimensional environments? We have seen\\nin Subsection 1.2.2 and will see in Section 3 that from the perspective of approximation theory deep NNs\\nmatch the performance of the best classical approximation tool in virtually every task. In practice, we\\nobserve something that is even more astounding. In fact, NNs seem to perform incredibly well on tasks that\\nno classical, non-specialized approximation method can even remotely handle. The approximation problem\\nthat we are talking about here is that of approximation of high-dimensional functions. Indeed, the classical\\ncurse of dimensionality [Bel52, NW09] postulates that essentially every approximation method deteriorates\\nexponentially fast with increasing dimension.\\n\\nFor example, for the uniform approximation error of 1-Lipschitz continuous functions on a d-dimensional\\nunit cube in the uniform norm, we have a lower bound of Ω(p−1/d), for p → ∞, when approximating with a\\ncontinuous scheme17 of p free parameters [DeV98].\\n\\nOn the other hand, in most applications, the input dimensions are massive. For example, the following\\ndatasets are typically used as benchmarks in image classiﬁcation problems: MNIST [LBBH98] with 28 × 28\\npixels per image, CIFAR-10/CIFAR-100 [KH09] with 32×32 pixels per image and ImageNet [DDS+09, KSH12]\\nwhich contains high-resolution images that are typically down-sampled to 256 × 256 pixels. Naturally, in\\nreal-world applications, the input dimensions may well exceed those of these test problems. However, already\\nfor the simplest of the test cases above, the input dimension is d = 784. If we use d = 784 in the aforementioned\\nlower bound for the approximation of 1-Lipschitz functions, then we require O(ε−784) parameters to achieve\\na uniform error of ε ∈ (0, 1). Already for moderate ε this value will quickly exceed the storage capacity\\nof any conceivable machine in this universe. Considering the aforementioned curse of dimensionality, it is\\npuzzling to see that NNs perform adequately in this regime. In Section 4, we describe three approaches that\\noﬀer explanations as to why deep NN-based approximation is not rendered meaningless in the context of\\nhigh-dimensional input dimensions.\\n\\nWhy does stochastic gradient descent converge to good local minima despite the non-convexity\\nof the problem? As mentioned in Subsection 1.2.1, a convergence guarantee of stochastic gradient descent\\nto a global minimum is typically only given if the underlying objective function admits some form of convexity.\\nHowever, the empirical risk of a NN, i.e., (cid:98)Rs(Φ(·, θ)), is typically not a convex function with respect to the\\nparameters θ. For a simple intuitive reason why this function fails to be convex, it is instructive to consider\\nthe following example.\\n\\n17One can achieve better rates at the cost of discontinuous (with respect to the function to be approximated) parameter\\nassignment. This can be motivated by the use of space-ﬁlling curves. In the context of NNs with piecewise polynomial activation\\nfunctions, a rate of p−2/d can be achieved by very deep architectures [Yar18a, YZ20].\\n\\n19\\n\\n\\x0cFigure 1.5: Two-dimensional projection of the loss landscape of a neural network with four layers and ReLU\\nactivation function on four diﬀerent scales. From top-left to bottom-right, we zoom into the global minimum\\nof the landscape.\\n\\nExample 1.20. Consider the NN\\n\\nΦ(x, θ) = θ1(cid:37)R(θ3x + θ5) + θ2(cid:37)R(θ4x + θ6),\\n\\nθ ∈ R6,\\n\\nx ∈ R,\\n\\nwith the ReLU activation function (cid:37)R(x) = max{0, x}. It is not hard to see that the two parameter values\\nθ = (1, −1, 1, 1, 1, 0) and ¯θ = (−1, 1, 1, 1, 0, 1) produce the same realization function18, i.e., Φ(·, θ) = Φ(·, ¯θ).\\nHowever, since (θ + ¯θ)/2 = (0, 0, 1, 1, 1/2, 1/2), we conclude that Φ(·, (θ + ¯θ)/2) = 0. Clearly, for the data\\ns = ((−1, 0), (1, 1)), we now have that\\n\\n(cid:98)Rs(Φ(·, θ)) = (cid:98)Rs(Φ(·, ¯θ)) = 0 and\\n\\n(cid:98)Rs\\n\\n(cid:0)Φ(·, (θ + ¯θ)/2)(cid:1) =\\n\\n1\\n2\\n\\n,\\n\\nshowing the non-convexity of (cid:98)Rs.\\n\\n18This corresponds to interchanging the two neurons in the hidden layer. In general it holds that the realization function of a\\n\\nFC NN is invariant under permutations of the neurons in a given hidden layer.\\n\\n20\\n\\n\\x0cGiven this non-convexity, Algorithm 1 faces serious challenges. Firstly, there may exist multiple suboptimal\\nlocal minima. Secondly, the objective may exhibit saddle points, some of which may be of higher order, i.e.,\\nthe Hessian vanishes. Finally, even if no suboptimal local minima exist, there may be extensive areas of the\\nparameter space where the gradient is very small, so that escaping these regions can take a very long time.\\nThese issues are not mere theoretical possibilities, but will almost certainly arise. For example, [AHW96,\\nSS18] show the existence of many suboptimal local minima in typical learning tasks. Moreover, for ﬁxed-sized\\nNNs, it has been shown in [BEG19, PRV20], that with respect to Lp-norms the set of NNs is generally a\\nvery non-convex and non-closed set. Also, the map θ (cid:55)→ Φa(·, θ) is not a quotient map, i.e., not continuously\\ninvertible when accounting for its non-injectivity. In addition, in various situations ﬁnding the global optimum\\nof the minimization problem is shown to be NP-hard in general [BR89, Jud90, ˇS´ım02]. In Figure 1.5 we\\nshow the two-dimensional projection of a loss landscape, i.e., the projection of the graph of the function\\nθ (cid:55)→ (cid:98)Rs(Φ(·, θ)). It is apparent from the visualization that the problem exhibits more than one minimum.\\nWe also want to add that in practice one neglects that the loss is only almost everywhere diﬀerentiable in\\ncase of piecewise smooth activation functions, such as the ReLU, although one could resort to subgradient\\nmethods [KL18].\\n\\nIn view of these considerations, the classical framework presented in Subsection 1.2.1 oﬀers no explanation\\nas to why deep learning works in practice. Indeed, in the survey [OM98, Section 1.4] the state of the art in\\n1998 was summarized by the following assessment: “There is no formula to guarantee that (1) the NN will\\nconverge to a good solution, (2) convergence is swift, or (3) convergence even occurs at all.”\\n\\nNonetheless, in applications, not only would an explanation of when and why SGD converges be extremely\\ndesirable, convergence is also quite often observed even though there is little theoretical explanation for it in\\nthe classical set-up. In Section 5, we collect modern approaches explaining why and when convergence occurs\\nand can be guaranteed.\\n\\nWhich aspects of a neural network architecture aﬀect the performance of deep learning? In\\nthe introduction to classical approaches to deep learning above, we have seen that in classical results, such as\\nin Theorem 1.16, only the eﬀect of few aspects of the NN architectures are considered. In Theorem 1.16 only\\nthe impact of the width of the NN was studied. In further approximation theorems below, e.g., in Theorems 2.1\\nand 3.2, we will additionally have a variable depth of NNs. However, for deeper architectures, there are\\nmany additional aspects of the architecture that could potentially aﬀect the performance of the model for\\nthe associated learning task. For example, even for a standard FC NN with L layers as in Deﬁnition 1.4,\\nthere is a lot of ﬂexibility in choosing the number of neurons (N1, . . . , NL−1) ∈ NL−1 in the hidden layers.\\nOne would expect that certain choices aﬀect the capabilities of the NNs considerably and some choices are\\npreferable over others. Note that, one aspect of the neural network architecture that can have a profound\\neﬀect on the performance, especially regarding approximation theoretical aspects of the performance, is the\\nchoice of the activation function. For example, in [MP99, Yar21] activation functions were found that allow\\nuniform approximation of continuous functions to arbitrary accuracy with ﬁxed-size neural networks. In the\\nsequel we will, however, focus on architectural aspects other than the activation function.\\n\\nIn addition, practitioners have invented an immense variety of NN architectures for speciﬁc problems. These\\ninclude NNs with convolutional blocks [LBBH98], with skip connections [HZRS16], sparse connections [ZAP16,\\nBBC17], batch normalization blocks [IS15], and many more. In addition, for sequential data, recurrent\\nconnections are used [RHW86] and these often have forget mechanisms [HS97] or other gates [CvMG+14]\\nincluded in their architectures.\\n\\nThe choice of an appropriate NN architecture is essential to the success of many deep learning tasks. This\\ngoes so far, that frequently an architecture search is applied to ﬁnd the most suitable one [ZL17, PGZ+18].\\nIn most cases, though, the design and choice of the architecture is based on the intuition of the practitioner.\\nNaturally, from a theoretical point of view, this situation is not satisfactory. Instead, it would be highly\\ndesirable to have a mathematical theory guiding the choice of NN architectures. More concretely, one\\nwould wish for mathematical theorems that identify those architectures that work for a speciﬁc problem and\\nthose that will yield suboptimal results. In Section 6, we discuss various results that explain theoretically\\nquantiﬁable eﬀects of certain aspects or building blocks of NN architectures.\\n\\n21\\n\\n\\x0cWhich features of data are learned by deep architectures? It is commonly believed that the neurons\\nof NNs constitute feature extractors in diﬀerent levels of abstraction that correspond to the layers. This\\nbelief is partially grounded in experimental evidence as well as in drawing connections to the human visual\\ncortex, see [GBC16, Chapter 9.10].\\n\\nUnderstanding the features that are learned can, in a way, be linked to understanding the reasoning\\nwith which a NN-based model ended up with its result. Therefore, analyzing the features that a NN learns\\nconstitutes a data-aware approach to understanding deep learning. Naturally, this falls outside of the scope of\\nthe classical theory, which is formulated in terms of optimization, generalization, and approximation errors.\\nOne central obstacle towards understanding these features theoretically is that, at least for practical\\nproblems, the data distribution is unknown. However, one often has partial knowledge. One example is that\\nin image classiﬁcation it appears reasonable to assume that any classiﬁer is translation and rotation invariant\\nas well as invariant under small deformations. In this context, it is interesting to understand under which\\nconditions trained NNs admit the same invariances.\\n\\nBiological NNs such as the visual cortex are believed to be evolved in a way that is based on sparse\\nmultiscale representations of visual information [OF96]. Again, a fascinating question is whether NNs trained\\nin practice can be shown to favor such multiscale representations based on sparsity or if the architecture\\nis theoretically linked to sparse representations. We will discuss various approaches studying the features\\nlearned by neural networks in Section 7.\\n\\nAre neural networks capable of replacing highly specialized numerical algorithms in natural\\nsciences? Shortly after their successes in various data-driven tasks in data science and AI applications,\\nNNs have been used also as a numerical ansatz for solving highly complex models from the natural sciences\\nwhich may be combined with data driven methods. This per se is not very surprising as many such models\\ncan be formulated as optimization problems where the common deep learning paradigm can be directly\\napplied. What might be considered surprising is that this approach seems to be applicable to a wide range of\\nproblems which have previously been tackled by highly specialized numerical methods.\\n\\nParticular successes include the data-driven solution of ill-posed inverse problems [AM ¨OS19] which have,\\nfor example, led to a fourfold speedup in MRI scantimes [ZKS+18] igniting the research project fastmri.org.\\nDeep-learning-based approaches have also been very successful in solving a vast array of diﬀerent partial\\ndiﬀerential equation (PDE) models, especially in the high-dimensional regime [EY18, RPK19, HSN20,\\nPSMF20] where most other methods would suﬀer from the curse of dimensionality.\\n\\nDespite these encouraging applications, the foundational mechanisms governing their workings and\\nlimitations are still not well understood. In Subsection 4.3 and Section 8 we discuss some theoretical and\\npractical aspects of deep learning methods applied to the solution of inverse problems and PDEs.\\n\\n2 Generalization of large neural networks\\n\\nIn the following, we will shed light on the generalization puzzle of NNs as described in Subsection 1.3. We\\nfocus on four diﬀerent lines of research which, of course, do not cover the wide range of available results.\\nIn fact, we had to omit a discussion of a multitude of important works, some of which we reference in the\\nfollowing paragraph.\\n\\nFirst, let us mention extensions of the generalization bounds presented in Subsection 1.2.3 making use\\nof local Rademacher complexities [BBM05] or dropping assumptions on boundedness or rapidly decaying\\ntails [Men14]. Furthermore, there are approaches to generalization which do not focus on the hypothesis set F,\\ni.e., the range of the learning algorithm A, but the way A chooses its model fs. For instance, one can assume\\nthat fs does not depend too strongly on each individual sample (algorithmic stability [BE02, PRMN04]),\\nonly on a subset of the samples (compression bounds [AGNZ18]), or satisﬁes local properties (algorithmic\\nrobustness [XM12]). Finally, we refer the reader to [JNM+20] and the references mentioned therein for an\\nempirical study of various measures related to generalization.\\n\\nNote that many results on generalization capabilities of NNs can still only be proven in simpliﬁed settings,\\ne.g., for deep linear NNs, i.e., (cid:37)(x) = x, or basic linear models, i.e., one-layer NNs. Thus, we start by\\n\\n22\\n\\n\\x0cemphasizing the connection of deep, nonlinear NNs to linear models (operating on features given by a suitable\\nkernel) in the inﬁnite width limit.\\n\\n2.1 Kernel regime\\n\\nWe consider a one-dimensional prediction setting where the loss L(f, (x, y)) depends on x ∈ X only through\\nf (x) ∈ Y, i.e., there exists a function (cid:96) : Y × Y → R such that\\n\\nL(f, (x, y)) = (cid:96)(f (x), y).\\n\\nFor instance, in case of the quadratic loss we have that (cid:96)(ˆy, y) = (ˆy − y)2. Further, let Φ be a NN with\\narchitecture (N, (cid:37)) = ((d, N1, . . . , NL−1, 1), (cid:37)) and let Θ0 be a RP (N )-valued random variable. For simplicity,\\nwe evolve the parameters of Φ according to the continuous version of gradient descent, so-called gradient ﬂow,\\ngiven by\\n\\ndΘ(t)\\ndt\\n\\n= −∇θ (cid:98)Rs(Φ(·, Θ(t))) = −\\n\\n1\\nm\\n\\nm\\n(cid:88)\\n\\ni=1\\n\\n∇θΦ(x(i), Θ(t))Di(t), Θ(0) = Θ0,\\n\\n(2.1)\\n\\nwhere Di(t) := ∂(cid:96)(ˆy,y(i))\\nfeature x(i) at time t ∈ [0, ∞). The chain rule implies the following dynamics of the NN realization\\n\\n|ˆy=Φ(x(i),Θ(t)) is the derivative of the loss with respect to the prediction at input\\n\\n∂ ˆy\\n\\ndΦ(·, Θ(t))\\ndt\\n\\n= −\\n\\n1\\nm\\n\\nm\\n(cid:88)\\n\\ni=1\\n\\nKΘ(t)(·, x(i))Di(t)\\n\\nand its empirical risk\\n\\nd (cid:98)Rs(Φ(·, Θ(t))\\ndt\\n\\n= −\\n\\n1\\nm2\\n\\nm\\n(cid:88)\\n\\nm\\n(cid:88)\\n\\ni=1\\n\\nj=1\\n\\nDi(t)KΘ(t)(x(i), x(j))Dj(t),\\n\\nwhere Kθ, θ ∈ RP (N ), is the so-called neural tangent kernel (NTK)\\n\\nKθ : Rd × Rd → R, Kθ(x1, x2) = (cid:0)∇θΦ(x1, θ)(cid:1)T\\n\\n∇θΦ(x2, θ).\\n\\n(2.2)\\n\\n(2.3)\\n\\n(2.4)\\n\\nw/N(cid:96) and σ2\\n\\nNow let σw, σb ∈ (0, ∞) and assume that the initialization Θ0 consists of independent entries, where entries\\ncorresponding to the weight matrix and bias vector in the (cid:96)-th layer follow a normal distribution with\\nzero mean and variances σ2\\nb , respectively. Under weak assumptions on the activation function,\\nthe central limit theorem implies that the pre-activations converge to i.i.d. centered Gaussian processes in\\nthe inﬁnite width limit N1, . . . , NL−1 → ∞, see [LBN+18, MHR+18]. Similarly, also KΘ0 converges to a\\ndeterministic kernel K∞ which stays constant in time and only depends on the activation function (cid:37), the\\ndepth L, and the initialization parameters σw and σb [JGH18, ADH+19, Yan19, LXS+20]. Thus, within the\\ninﬁnite width limit, gradient ﬂow on the NN parameters as in (2.1) is equivalent to functional gradient ﬂow\\nin the reproducing kernel Hilbert space (HK∞, (cid:107) · (cid:107)K∞) corresponding to K∞, see (2.2).\\n\\nBy (2.3), the empirical risk converges to a global minimum as long as the kernel evaluated at the input\\nfeatures, ¯K∞ := (K∞(x(i), x(j)))m\\ni,j=1 ∈ Rm×m, is positive deﬁnite (see, e.g., [JGH18, DLL+19] for suitable\\nconditions) and the (cid:96)(·, y(i)) are convex and lower bounded. For instance, in case of the quadratic loss the\\nsolution of (2.2) is then given by\\n\\nΦ(·, Θ(t)) = C(t)(y(i))m\\n\\ni=1 + (cid:0)Φ(·, Θ0) − C(t)(Φ(x(i), Θ0))m\\n\\ni=1\\n\\n(cid:1),\\n\\n(2.5)\\n\\nwhere C(t) := (cid:0)(K∞(·, x(i)))m\\nm ). As the initial realization Φ(·, Θ0) constitutes a\\ncentered Gaussian process, the second term in (2.5) follows a normal distribution with zero mean at each\\ninput. In the limit t → ∞, its variance vanishes on the input features x(i), i ∈ [m], and the ﬁrst term\\nconvergences to the minimum kernel-norm interpolator, i.e., to the solution of\\n\\n( ¯K∞)−1(Im − e− 2 ¯K∞t\\n\\n(cid:1)T\\n\\ni=1\\n\\nmin\\nf ∈HK∞\\n\\n(cid:107)f (cid:107)K∞ s.t.\\n\\nf (x(i)) = y(i).\\n\\n23\\n\\n\\x0cTherefore, within the inﬁnite width limit, the generalization properties of the NN could be described by the\\ngeneralization properties of the minimizer in the reproducing kernel Hilbert space corresponding to the kernel\\nK∞ [BMM18, LR20, LRZ20, GMMM21, Li21].\\n\\nThis so-called lazy training, where a NN essentially behaves like a linear model with respect to the nonlinear\\nfeatures x (cid:55)→ ∇θΦ(x, θ), can already be observed in the non-asymptotic regime, see also Subsection 5.2. For\\nsuﬃciently overparametrized (P (N ) (cid:29) m) and suitably initialized models, one can show that Kθ(0) is close\\nto K∞ at initialization and Kθ(t) stays close to Kθ(0) throughout training, see [DZPS18, ADH+19, COB19,\\nDLL+19]. The dynamics of the NN under gradient ﬂow in (2.2) and (2.3) can thus be approximated by the\\ndynamics of the linearization of Φ at initialization Θ0, given by\\n\\nwhich motivates to study the behavior of linear models in the overparametrized regime.\\n\\nΦlin(·, θ) := Φ(·, Θ0) + (cid:104)∇θΦ(·, Θ0), θ − Θ0(cid:105),\\n\\n2.2 Norm-based bounds and margin theory\\n\\nFor piecewise linear activation functions, one can improve upon the VC-dimension bounds in Theorem 1.18\\nand show that, up to logarithmic factors, the VC-dimension is asymptotically bounded both above and below\\nby P (N )L, see [BHLM19]. The lower bound shows that the generalization bound in Theorem 1.19 can only\\nbe non-vacuous if the number of samples m scales at least linearly with the number of NN parameters P (N ).\\nHowever, heavily overparametrized NNs used in practice seem to generalize well outside of this regime.\\n\\nOne solution is to bound other complexity measures of NNs taking into account various norms on the\\nparameters and avoid the direct dependence on the number of parameters [Bar98]. For instance, we can\\ncompute bounds on the Rademacher complexity of NNs with positively homogeneous activation function,\\nwhere the Frobenius norm of the weight matrices is bounded, see also [NTS15]. Note that, for instance,\\nthe ReLU activation is positively homogeneous, i.e., it satisﬁes that (cid:37)R(λx) = λ(cid:37)R(x) for all x ∈ R and\\nλ ∈ (0, ∞).\\n\\nTheorem 2.1 (Rademacher complexity of neural networks). Let d ∈ N, assume that X = B1(0) ⊂ Rd, and\\nlet (cid:37) be a positively homogeneous activation function with Lipschitz constant 1. We deﬁne the set of all\\nbiasless NN realizations with depth L ∈ N, output dimension 1, and Frobenius norm of the weight matrices\\nbounded by C ∈ (0, ∞) as\\n\\n(cid:101)FL,C := (cid:8)Φ(N,(cid:37))(·, θ) : N ∈ NL+1, N0 = d, NL = 1, θ = ((W ((cid:96)), 0))L\\n\\n(cid:96)=1 ∈ RP (N ), (cid:107)W ((cid:96))(cid:107)F ≤ C(cid:9).\\n\\nThen for every m ∈ N it holds that\\n\\nRm( (cid:101)FL,C) ≤\\n\\nC(2C)L−1\\n√\\nm\\n\\n.\\n\\nThe term 2L−1 depending exponentially on the depth can be reduced to\\n\\nL or completely omitted\\nby invoking also the spectral norm of the weight matrices [GRS18]. Further, observe that for L = 1,\\ni.e., linear classiﬁers with bounded Euclidean norm, this bound is independent of the input dimension d.\\nTogether with (1.12), this motivates why the regularized linear model in Figure 1.4 did perform well in the\\noverparametrized regime.\\n\\n√\\n\\nThe proof of Theorem 2.1 is based on the contraction property of the Rademacher complexity [LT91]\\n\\nwhich establishes that\\n\\nRm((cid:37) ◦ (cid:101)F(cid:96),C) ≤ 2Rm( (cid:101)F(cid:96),C),\\nWe can iterate this together with the fact that for every τ ∈ {−1, 1}m, and x ∈ RN(cid:96)−1 it holds that\\n\\n(cid:96) ∈ N.\\n\\nsup\\n(cid:107)W ((cid:96))(cid:107)F ≤C\\n\\n(cid:13)\\n(cid:13)\\n\\nm\\n(cid:88)\\n\\ni=1\\n\\nτi(cid:37)(W ((cid:96))x)(cid:13)\\n\\n(cid:13)2 = C sup\\n(cid:107)w(cid:107)2≤1\\n\\n(cid:12)\\n(cid:12)\\n\\nm\\n(cid:88)\\n\\ni=1\\n\\nτi(cid:37)((cid:104)w, x(cid:105))(cid:12)\\n(cid:12).\\n\\n24\\n\\n\\x0cIn summary, one establishes that\\n\\nRm( (cid:101)FL,C) =\\n\\nE(cid:2)\\n\\nC\\nm\\n\\nsup\\nf ∈ (cid:101)FL−1,C\\n\\n(cid:13)\\n(cid:13)\\n\\nm\\n(cid:88)\\n\\ni=1\\n\\nτi(cid:37)(f (X (i)))(cid:13)\\n(cid:13)2\\n\\n(cid:3) ≤\\n\\nC(2C)L−1\\nm\\n\\nE(cid:2)(cid:13)\\n(cid:13)\\n\\nm\\n(cid:88)\\n\\ni=1\\n\\nτiX (i)(cid:13)\\n(cid:13)2\\n\\n(cid:3),\\n\\nwhich by Jensen’s inequality yields the claim.\\n\\nRecall that for classiﬁcation problems one typically minimizes a surrogate loss Lsurr, see Remark 1.9.\\nThis suggests that there could be a trade-oﬀ happening between complexity of the hypothesis class Fa and\\nthe corresponding regression ﬁt underneath, i.e., the margin M (f, z) := yf (x) by which a training example\\nz = (x, y) has been classiﬁed correctly by f ∈ Fa, see [BFT17, NBS18, JKMB19]. For simplicity, let us focus\\non the ramp surrogate loss with conﬁdence γ > 0, i.e., Lsurr\\n\\n(f, z) := (cid:96)γ(M (f, z)), where\\n\\nγ\\n\\n(cid:96)γ(t) := 1(−∞,γ](t) − t\\nγ\\n\\n1[0,γ](t),\\n\\nt ∈ R.\\n\\nNote that the ramp function (cid:96)γ is 1/γ-Lipschitz continuous. Using McDiarmid’s inequality and a sym-\\nmetrization argument similar to the proof of Theorem 1.19, combined with the contraction property of the\\nRademacher complexity, yields the following bound on the probability of misclassiﬁcation: With probability\\n1 − δ for every f ∈ Fa it holds that\\n\\nP[sgn(f (X)) (cid:54)= Y ] ≤ E(cid:2)Lsurr\\n\\nγ\\n\\n(f, Z)(cid:3) (cid:46) 1\\nm\\n\\n(cid:46) 1\\nm\\n\\n=\\n\\n1\\nm\\n\\nLsurr\\nγ\\n\\n(f, Z (i)) + Rm(Lsurr\\n\\nγ\\n\\n◦ Fa) +\\n\\n(cid:114)\\n\\nln(1/δ)\\nm\\n\\n1(−∞,γ)(Y (i)f (X (i))) +\\n\\n1(−∞,γ)(Y (i)f (X (i))) +\\n\\nRm(M ◦ Fa)\\nγ\\n\\n+\\n\\n(cid:114)\\n\\nln(1/δ)\\nm\\n\\nRm(Fa)\\nγ\\n\\n+\\n\\n(cid:114)\\n\\nln(1/δ)\\nm\\n\\n.\\n\\nm\\n(cid:88)\\n\\ni=1\\nm\\n(cid:88)\\n\\ni=1\\nm\\n(cid:88)\\n\\ni=1\\n\\nThis shows the trade-oﬀ between the complexity of Fa measured by Rm(Fa) and the fraction of training\\ndata that has been classiﬁed correctly with a margin of at least γ. In particular this suggests, that (even if\\nwe classify the training data correctly with respect to the 0-1 loss) it might be beneﬁcial to further increase\\nthe complexity of Fa to simultaneously increase the margins by which the training data has been classiﬁed\\ncorrectly and thus obtain a better generalization bound.\\n\\n2.3 Optimization and implicit regularization\\n\\nThe optimization algorithm, which is usually a variant of SGD, seems to play an important role for the\\ngeneralization performance. Potential indicators for good generalization performance are high speed of\\nconvergence [HRS16] or ﬂatness of the local minimum to which SGD converged, which can be characterized\\nby the magnitude of the eigenvalues of the Hessian (or approximately as the robustness of the minimizer\\nto adversarial perturbations on the parameter space), see [KMN+17]. In [DR17, NBMS17] generalization\\nbounds depending on a concept of ﬂatness are established by employing a PAC-Bayesian framework, which\\ncan be viewed as a generalization of Theorem 1.17, see [McA99]. Further, one can also unite ﬂatness and\\nnorm-based bounds by the Fisher–Rao metric of information geometry [LPRS19].\\n\\nLet us motivate the link between generalization and ﬂatness in the case of simple linear models: We\\n\\nassume that our model takes the form (cid:104)θ, ·(cid:105), θ ∈ Rd, and we will use the abbreviations\\n\\nr(θ) := (cid:98)Rs((cid:104)θ, ·(cid:105)) and γ(θ) := min\\ni∈[m]\\n\\nM ((cid:104)θ, ·(cid:105), z(i)) = min\\ni∈[m]\\n\\ny(i)(cid:104)θ, x(i)(cid:105)\\n\\nthroughout this subsection to denote the empirical risk and the margin for given training data s =\\n((x(i), y(i)))m\\ni=1. We assume that we are solving a classiﬁcation task with the 0-1 loss and that our training\\n\\n25\\n\\n\\x0cdata is linearly separable. This means that there exists a minimizer ˆθ ∈ Rd such that r(ˆθ) = 0. We observe\\nthat δ-robustness in the sense that\\n\\nmax\\nθ∈Bδ(0)\\n\\nr(ˆθ + θ) = r(ˆθ) = 0\\n\\nimplies that\\n\\n0 < min\\ni∈[m]\\n\\ny(i)(cid:104)ˆθ − δy(i) x(i)\\n(cid:107)x(i)(cid:107)2\\n\\n, x(i)(cid:105) ≤ γ(ˆθ) − δ min\\ni∈[m]\\n\\n(cid:107)x(i)(cid:107)2,\\n\\nsee also [PKL+17]. This lower bound on the margin γ(ˆθ) then ensures generalization guarantees as described\\nin Subsection 2.2.\\n\\nEven without explicit19 control on the complexity of Fa, there do exist results showing that SGD acts as\\nimplicit regularization [NTS14]. This is motivated by linear models where SGD converges to the minimal\\nEuclidean norm solution for the quadratic loss and in the direction of the hard margin support vector machine\\nsolution for the logistic loss on linearly separable data [SHN+18]. Note that convergence to minimum norm\\nor maximum margin solutions in particular decreases the complexity of our hypothesis set and thus improves\\ngeneralization bounds, see Subsection 2.2.\\n\\nWhile we have seen this behavior of gradient descent for linear regression already in the more general\\ncontext of kernel regression in Subsection 2.1, we want to motivate the corresponding result for classiﬁcation\\ntasks in the following. We focus on the exponential surrogate loss Lsurr(f, z) = (cid:96)(M (f, z)) = e−yf (x) with\\n(cid:96)(z) = e−z, but similar observations can be made for the logistic loss deﬁned in Remark 1.9. We assume that\\nthe training data is linearly separable, which guarantees the existence of ˆθ (cid:54)= 0 with γ(ˆθ) > 0. Then for every\\nlinear model (cid:104)θ, ·(cid:105), θ ∈ Rd, it holds that\\n\\n(cid:10)ˆθ, ∇θr(θ)(cid:105) =\\n\\n1\\nm\\n\\nm\\n(cid:88)\\n\\ni=1\\n\\n(cid:96)(cid:48)(y(i)(cid:104)θ, x(i)(cid:105))\\n(cid:124)\\n(cid:123)(cid:122)\\n(cid:125)\\n<0\\n\\ny(i)(cid:104)ˆθ, x(i)(cid:105)\\n(cid:123)(cid:122)\\n(cid:125)\\n(cid:124)\\n>0\\n\\n.\\n\\nA critical point ∇θr(θ) = 0 can therefore be approached if and only if for every i ∈ [m] we have\\n\\nwhich is equivalent to (cid:107)θ(cid:107)2 → ∞ and γ(θ) > 0. Let us now deﬁne\\n\\n(cid:96)(cid:48)(y(i)(cid:104)θ, x(i)(cid:105)) = −e−y(i)(cid:104)θ,x(i)(cid:105) → 0,\\n\\nrβ(θ) :=\\n\\n(cid:96)−1(r(βθ))\\nβ\\n\\n,\\n\\nθ ∈ Rd, β ∈ (0, ∞),\\n\\nand observe that\\n\\nrβ(θ) = −\\n\\nlog(r(βθ))\\nβ\\n\\n→ γ(θ),\\n\\nβ → ∞.\\n\\n(2.6)\\n\\nDue to this property, rβ is often referred to as the smoothed margin [LL19, JT19b]. We evolve θ according to\\ngradient ﬂow with respect to the smoothed margin r1, i.e.,\\n\\ndθ(t)\\ndt\\n\\n= ∇θr1(θ(t)) = −\\n\\n1\\nr(θ(t))\\n\\n∇θr(θ(t)),\\n\\nwhich produces the same trajectory as gradient ﬂow with respect to the empirical risk r under a rescaling\\nof the time t. Looking at the evolution of the normalized parameters ˜θ(t) = θ(t)/(cid:107)θ(t)(cid:107)2, the chain rule\\nestablishes that\\n\\nd˜θ(t)\\ndt\\n\\n= P˜θ(t)\\n\\n∇θrβ(t)(˜θ(t))\\nβ(t)\\n\\nwith β(t) := (cid:107)θ(t)(cid:107)2\\n\\nand Pθ := Id − θθT ,\\n\\nθ ∈ Rd.\\n\\n19Note that also diﬀerent architectures can exhibit vastly diﬀerent inductive biases [ZBH+20] and also within the architecture\\n\\ndiﬀerent parameters have diﬀerent importance, see [FC18, ZBS19] and Proposition 6.2.\\n\\n26\\n\\n\\x0cThis shows that the normalized parameters perform projected gradient ascent with respect to the function\\nrβ(t), which converges to the margin due to (2.6) and the fact that β(t) = (cid:107)θ(t)(cid:107)2 → ∞ when approaching a\\ncritical point. This motivates that during gradient ﬂow the normalized parameters implicitly maximize the\\nmargin. See [GLSS18a, GLSS18b, LL19, NLG+19, CB20, JT20] for a precise analysis and various extensions,\\ne.g., to homogeneous or two-layer NNs and other optimization geometries.\\n\\nTo illustrate one research direction, we present an exemplary result in the following. Let Φ = Φ(N,(cid:37)) be\\na biasless NN with parameters θ = ((W ((cid:96)), 0))L\\n(cid:96)=0 and output dimension NL = 1. For given input features\\nx ∈ RN0, the gradient ∇W ((cid:96))Φ = ∇W ((cid:96)) Φ(x, θ) ∈ RN(cid:96)−1×N(cid:96) with respect to the weight matrix in the (cid:96)-th\\nlayer satisﬁes that\\n\\n∇W ((cid:96))Φ = (cid:37)(Φ((cid:96)−1))\\n\\n∂Φ\\n∂Φ((cid:96)+1)\\n\\n∂Φ((cid:96)+1)\\n∂Φ((cid:96))\\n\\n= (cid:37)(Φ((cid:96)−1))\\n\\n∂Φ\\n∂Φ((cid:96)+1)\\n\\nW ((cid:96)+1) diag (cid:0)(cid:37)(cid:48)(Φ((cid:96)))(cid:1),\\n\\nwhere the pre-activations (Φ((cid:96)))L\\nas in (2.1) and using an activation function (cid:37) with (cid:37)(x) = (cid:37)(cid:48)(x)x, such as the ReLU, this implies that\\n\\n(cid:96)=1 are given as in (1.1). Evolving the parameters according to gradient ﬂow\\n\\ndiag (cid:0)(cid:37)(cid:48)(Φ((cid:96)))(cid:1)W ((cid:96))(t)\\n\\n(cid:16) dW ((cid:96))(t)\\ndt\\n\\n(cid:17)T\\n\\n=\\n\\n(cid:16) dW ((cid:96)+1)(t)\\ndt\\n\\n(cid:17)T\\n\\nW ((cid:96)+1)(t) diag (cid:0)(cid:37)(cid:48)(Φ((cid:96)))(cid:1).\\n\\n(2.7)\\n\\nNote that this ensures the conservation of balancedness between weight matrices of adjacent layers, i.e.,\\n\\nd\\ndt\\n\\n(cid:0)(cid:107)W ((cid:96)+1)(t)(cid:107)2\\n\\nF − (cid:107)W ((cid:96))(t)(cid:107)2\\nF\\n\\n(cid:1) = 0,\\n\\nsee [DHL18]. Furthermore, for deep linear NNs, i.e., (cid:37)(x) = x, the property in (2.7) implies conservation of\\nalignment of left and right singular spaces of W ((cid:96)) and W ((cid:96)+1). This can then be used to show implicit pre-\\nconditioning and convergence of gradient descent [ACH18, ACGH19] and that, under additional assumptions,\\ngradient descent converges to a linear predictor that is aligned with the maximum margin solution [JT19a].\\n\\n2.4 Limits of classical theory and double descent\\n\\nThere is ample evidence that classical tools from statistical learning theory alone, such as Rademacher\\naverages, uniform convergence, or algorithmic stability may be unable to explain the full generalization\\ncapabilities of NNs [ZBH+17, NK19]. It is especially hard to reconcile the classical bias-variance trade-oﬀ with\\nthe observation of good generalization performance when achieving zero empirical risk on noisy data using a\\nregression loss. On top of that, this behavior of overparametrized models in the interpolation regime turns out\\nnot to be unique to NNs. Empirically, one observes for various methods (decision trees, random features, linear\\nmodels) that the test error decreases even below the sweet-spot in the u-shaped bias-variance curve when\\nfurther increasing the number of parameters [BHMM19, GJS+20, NKB+20]. This is often referred to as the\\ndouble descent curve or benign overﬁtting, see Figure 2.1. For special cases, e.g., linear regression or random\\nfeature regression, such behavior can even be proven, see [HMRT19, MM19, BLLT20, BHX20, MVSS20].\\n\\nIn the following we analyze this phenomenon in the context of linear regression. Speciﬁcally, we focus\\non a prediction task with quadratic loss, input features given by a centered Rd-valued random variable\\nX, and labels given by Y = (cid:104)θ∗, X(cid:105) + ν, where θ∗ ∈ Rd and ν is a centered random variable independent\\ni=1, we consider the empirical risk minimizer (cid:98)fS = (cid:104)ˆθ, ·(cid:105) with\\nof X. For training data S = ((X (i), Y (i)))m\\nminimum Euclidean norm of its parameters ˆθ or, equivalently, the limit of gradient ﬂow with zero initialization.\\nUsing (1.3) and a bias-variance decomposition we can write\\n\\nE[R( (cid:98)fS)|(X (i))m\\n\\ni=1] − R∗ = E[(cid:107) (cid:98)fS − f ∗(cid:107)L2(PX )|(X (i))m\\n\\ni=1]\\n\\nwhere Σ := (cid:80)m\\ni=1 X (i)(X (i))T , Σ+ denotes the Moore–Penrose inverse of Σ, and P := Id − Σ+Σ is the\\northogonal projector onto the kernel of Σ. For simplicity, we focus on the variance Tr(cid:0)Σ+E[XX T ](cid:1), which can\\n\\n= (θ∗)T P E[XX T ]P θ∗ + E[ν2]Tr(cid:0)Σ+E[XX T ](cid:1),\\n\\n27\\n\\n\\x0cFigure 2.1: This illustration shows the classical, underparametrized regime in green, where the u-shaped curve\\ndepicts the bias-variance trade-oﬀ as explained in Section 1.2. Starting with complexity of our algorithm\\nA larger than the interpolation threshold we can achieve zero empirical risk (cid:98)Rs(fs) (training error), where\\nfs = A(s). Within this modern interpolation regime, the risk R(fs) (test error) might be even lower than\\nat the classical sweet spot. Whereas complexity(A) traditionally refers to the complexity of the hypothesis\\nset F, there is evidence that also the optimization scheme and the data is inﬂuencing the complexity\\nleading to deﬁnitions like complexity(A) := max (cid:8)m ∈ N : E(cid:2)\\n(cid:9), for suitable\\nε > 0 [NKB+20]. This illustration is based on [BHMM19].\\n\\n(cid:98)RS(A(S))(cid:3) ≤ ε with S ∼ Pm\\n\\nZ\\n\\nbe viewed as setting θ∗ = 0 and E[ν2] = 1. Assuming that X has i.i.d. entries with unit variance and bounded\\nﬁfth moment, the distribution of the eigenvalues of 1\\nm → κ ∈ (0, ∞) can be\\ndescribed via the Marchenko–Pastur law. Therefore, the asymptotic variance can be computed explicitly as\\n\\nm Σ+ in the limit d, m → ∞ with d\\n\\nTr(cid:0)Σ+E[XX T ](cid:1) →\\n\\n1 − max{1 − κ, 0}\\n|1 − κ|\\n\\nfor d, m → ∞ with\\n\\nd\\nm\\n\\n→ κ,\\n\\nalmost surely, see [HMRT19]. This shows that despite interpolating the data we can decrease the risk in\\nthe overparametrized regime κ > 1. In the limit d, m → ∞, such benign overﬁtting can also be shown for\\nmore general settings (including lazy training of NNs), some of which even achieve their optimal risk in the\\noverparametrized regime [MM19, MZ20, LD21].\\n\\nFor normally distributed input features X such that E[XX T ] has rank larger than m, one can also\\n\\ncompute the behavior of the variance in the non-asymptomatic regime [BLLT20]. Deﬁne\\n\\nk∗ := min{k ≥ 0 :\\n\\n(cid:80)\\n\\ni>k λi\\nλk+1\\n\\n≥ cm},\\n\\nwhere λ1 ≥ λ2 ≥ · · · ≥ λd ≥ 0 are the eigenvalues of E[XX T ] in decreasing order and c ∈ (0, ∞) is a universal\\nconstant. Assuming that k∗/m is suﬃciently small, with high probability it holds that\\n\\nTr(cid:0)Σ+E[XX T ](cid:1) ≈\\n\\nk∗\\nm\\n\\n+\\n\\nm (cid:80)\\n((cid:80)\\n\\ni>k∗ λ2\\ni\\ni>k∗ λi)2 .\\n\\n28\\n\\n\\x0cThis precisely characterizes the regimes for benign overﬁtting in terms\\nof the eigenvalues of the covariance matrix E[XX T ]. Furthermore, it\\nshows that adding new input feature coordinates and thus increasing\\nthe number of parameters d can lead to either an increase or decrease\\nof the risk.\\n\\nTo motivate this phenomenon, which is considered in much more\\ndepth in [CMBK20], let us focus on a single sample m = 1 and\\nfeatures X that take values in X = {−1, 1}d. Then it holds that\\nΣ+ = X (1)(X (1))T\\n\\n(cid:107)X (1)(cid:107)4 = X (1)(X (1))T\\n\\nand thus\\n\\nd2\\n\\nE(cid:2)Tr(cid:0)Σ+E[XX T ](cid:1)(cid:3) =\\n\\n1\\nd2\\n\\n(cid:13)E(cid:2)XX T (cid:3)(cid:13)\\n(cid:13)\\n2\\nF .\\n(cid:13)\\n\\n(2.8)\\n\\nIn particular, this shows that incrementing the input feature dimen-\\nsions d (cid:55)→ d + 1 one can increase or decrease the risk depending on\\nthe correlation of the coordinate Xd+1 with respect to the previous\\ncoordinates (Xi)d\\n\\ni=1, see also Figure 2.2.\\n\\nGenerally speaking, overparametrization and perfectly ﬁtting\\nnoisy data does not exclude good generalization performance, see\\nalso [BRT19]. However, the risk crucially depends on the data\\ndistribution and the chosen algorithm.\\n\\nFigure 2.2: The expected variance\\nof linear regression in (2.8) with d ∈\\n[150] and Xi ∼ U ({−1, 1}), i ∈ [150],\\nwhere Xi = X1 for i ∈ {10, . . . , 20} ∪\\n{30, . . . , 50} and all other coordinates\\nare independent.\\n\\n3 The role of depth in the expressivity of neural networks\\n\\nThe approximation theoretical aspect of a NN architecture, responsible for the approximation component\\nF ) − R∗ of the error R(fS) − R∗ in (1.4), is probably one of the most well-studied parts of the\\nεapprox := R(f ∗\\ndeep learning pipe-line. The achievable approximation error of an architecture most directly describes the\\npower of the architecture.\\n\\nAs mentioned in Subsection 1.3, many classical approaches only study the approximation theory of NNs\\nwith few layers, whereas modern architectures are typically very deep. A ﬁrst observation into the eﬀect of\\ndepth is that it can often compensate for insuﬃcient width. For example, in the context of the universal\\napproximation theorem, it was shown that very narrow NNs are still universal if instead of increasing the\\nwidth, the number of layers can be chosen arbitrarily [HS17, Han19, KL20]. However, if the width of a NN\\nfalls below a critical number, then the universality will not hold any longer.\\n\\nBelow, we discuss three additional observations that shed light on the eﬀect of depth on the approximation\\n\\ncapacities or alternative notions of expressivity of NNs.\\n\\n3.1 Approximation of radial functions\\n\\nOne technique to study the impact of depth relies on the construction of speciﬁc functions which can be\\nwell approximated by NNs of a certain depth, but require signiﬁcantly more parameters when approximated\\nto the same accuracy by NNs of smaller depth. In the following we present one example for this type of\\napproach, which can be found in [ES16].\\n\\nTheorem 3.1 (Power of depth). Let (cid:37) ∈ {(cid:37)R, (cid:37)σ, 1(0,∞)} be the ReLU, the logistic, or the Heaviside function.\\nThen there exist constants c, C ∈ (0, ∞) with the following property: For every d ∈ N with d ≥ C there exist a\\nprobability measure µ on Rd, a three-layer NN architecture a = (N, (cid:37)) = ((d, N1, N2, 1), (cid:37)) with (cid:107)N (cid:107)∞ ≤ Cd5,\\nand corresponding parameters θ∗ ∈ RP (N ) with (cid:107)θ∗(cid:107)∞ ≤ CdC and (cid:107)Φa(·, θ∗)(cid:107)L∞(Rd) ≤ 2 such that for every\\nn ≤ cecd it holds that\\n\\ninf\\nθ∈RP ((d,n,1))\\n\\n(cid:107)Φ((d,n,1),(cid:37))(·, θ) − Φa(·, θ∗)(cid:107)L2(µ) ≥ c.\\n\\n29\\n\\n050100150d0.00.51.0variance\\x0cIn fact, the activation function in Theorem 3.1 is only required to satisfy mild conditions and the result\\nholds, for instance, also for more general sigmoidal functions. The proof of Theorem 3.1 is based on the\\nconstruction of a suitable radial function g : Rd → R, i.e., g(x) = ˜g((cid:107)x(cid:107)2\\n2) for some ˜g : [0, ∞) → R, which\\ncan be eﬃciently approximated by three-layer NNs but approximation by only a two-layer NN requires\\nexponentially large complexity, i.e., the width being exponential in d.\\n\\nThe ﬁrst observation of [ES16] is that g can typically be well approximated on a bounded domain by\\na three-layer NN, if ˜g is Lipschitz continuous. Indeed, for the ReLU activation function it is not diﬃcult\\nto show that, emulating a linear interpolation, one can approximate a univariate C-Lipschitz function\\nuniformly on [0, 1] up to precision ε by a two-layer architecture of width O(C/ε). The same holds for smooth,\\nnon-polynomial activation functions due to Theorem 1.16. This implies that the squared Euclidean norm,\\nas a sum of d univariate functions, i.e., [0, 1]d (cid:51) x (cid:55)→ (cid:80)d\\ni , can be approximated up to precision ε by a\\ntwo-layer architecture of width O(d2/ε). Moreover, this shows that the third layer can eﬃciently approximate\\n˜g, establishing approximation of g on a bounded domain up to precision ε using a three-layer architecture\\nwith number of parameters polynomial in d/ε.\\n\\ni=1 x2\\n\\nThe second step in [ES16] is to choose g in such a way that the realization of any two-layer neural network\\nΦ = Φ((d,n,1),(cid:37))(·, θ) with width n not being exponential in d is on average (with respect to the probability\\nmeasure µ) a constant distance away from g. Their argument is heavily based on ideas from Fourier analysis\\nand will be outlined below. In this context, let us recall that we denote by ˆf the Fourier transform of a\\nsuitable function or, more generally, tempered distribution f .\\n\\nAssuming that the square-root ϕ of the density function associated with the probability measure µ as\\n\\nwell as Φ and g are well-behaved, the Plancherel theorem yields that\\n\\n(cid:107)Φ − g(cid:107)2\\n\\nL2(µ) = (cid:107)Φϕ − gϕ(cid:107)2\\n\\nL2(Rd) = (cid:13)\\n\\n(cid:13) (cid:99)Φϕ − (cid:99)gϕ(cid:13)\\n2\\nL2(Rd).\\n(cid:13)\\n\\n(3.1)\\n\\nNext, the speciﬁc structure of two-layer NNs is used, which implies that for every j ∈ [n] there exists\\nwj ∈ Rd with (cid:107)wj(cid:107)2 = 1 and (cid:37)j : R → R (subsuming the activation function (cid:37), the norm of wj, and the\\nremaining parameters corresponding to the j-th neuron in the hidden layer) such that Φ is of the form\\n\\nn\\n(cid:88)\\n\\nΦ =\\n\\n(cid:37)j((cid:104)wj, ·(cid:105)) =\\n\\nn\\n(cid:88)\\n\\n((cid:37)j ⊗ 1Rd−1) ◦ Rwj .\\n\\nj=1\\n\\nj=1\\nThe second equality follows by viewing the action of the j-th neuron\\nas a tensor product of (cid:37)j and the indicator function 1Rd−1 (x) = 1,\\nx ∈ Rd−1, composed with a d-dimensional rotation Rwj ∈ SO(d)\\nwhich maps wj to the ﬁrst standard basis vector e(1) ∈ Rd. Noting\\nthat the Fourier transform respects linearity, rotations, and tensor\\nproducts, we can compute\\n\\nˆΦ =\\n\\nn\\n(cid:88)\\n\\n(ˆ(cid:37)j ⊗ δRd−1 ) ◦ Rwj ,\\n\\nj=1\\n\\nwhere δRd−1 denotes the Dirac distribution on Rd−1. In particular, the\\nsupport of ˆΦ has a particular star-like shape, namely (cid:83)n\\nj=1 span{wj},\\nwhich are in fact lines passing through the origin.\\n\\nNow we choose ϕ to be the inverse Fourier transform of the\\nindicator function of a ball Br(0) ⊂ Rd with vol(Br(0)) = 1, ensuring\\nthat ϕ2 is a valid probability density for µ as\\n\\nµ(Rd) = (cid:107)ϕ2(cid:107)L1(Rd) = (cid:107)ϕ(cid:107)2\\n\\nL2(Rd) = (cid:107) ˆϕ(cid:107)2\\n\\nL2(Rd) = (cid:107)1Br(0)(cid:107)2\\n\\nL2(Rd) = 1.\\n\\nUsing the convolution theorem, this choice of ϕ yields that\\n\\nsupp( (cid:99)Φϕ) = supp( ˆΦ ∗ ˆϕ) ⊂\\n\\nn\\n(cid:91)\\n\\nj=1\\n\\n(span{wj} + Br(0)) .\\n\\n30\\n\\nFigure 3.1: This illustration shows\\nthe largest possible support (blue) of\\n(cid:99)Φϕ, where ˆϕ = 1Br(0) and Φ is a\\nshallow neural network with architec-\\nture N = (2, 4, 1) and weight matrix\\nW (1) = [w1 . . . w4]T in the ﬁrst layer.\\nAny radial function with enough of its\\nL2-mass located at high frequencies\\n(indicated by the red area) cannot be\\nwell approximated by Φϕ.\\n\\n\\x0cThus the lines passing through the origin are enlarged to tubes. It is this particular shape which allows the\\nconstruction of some g so that (cid:107) (cid:99)Φϕ− (cid:99)gϕ(cid:107)2\\nL2(Rd) can be suitably lower bounded, see also Figure 3.1. Intriguingly,\\nthe peculiar behavior of high-dimensional sets now comes into play. Due to the well known concentration of\\nmeasure principle, the variable n needs to be exponentially large for the set (cid:83)n\\nj=1 (span{wj} + Br(0)) to be\\nnot sparse. If it is smaller, one can construct a function g so that the main energy content of (cid:99)gϕ has a certain\\ndistance from the origin, yielding a lower bound for (cid:107) (cid:99)Φϕ − (cid:99)gϕ(cid:107)2 and hence (cid:107)Φ − g(cid:107)2\\nL2(µ), see (3.1). One key\\ntechnical problem is the fact that such a behavior for ˆg does not immediately imply a similar behavior of (cid:99)gϕ,\\nrequiring a quite delicate construction of g.\\n\\n3.2 Deep ReLU networks\\n\\nMaybe for no activation function is the eﬀect of depth clearer than\\nfor the ReLU activation function (cid:37)R(x) = max{0, x}. We refer to\\ncorresponding NN architectures (N, (cid:37)R) as ReLU (neural) networks\\n(ReLU NNs). A two-layer ReLU NN with one-dimensional input and\\noutput is a function of the form\\n\\nΦ(x) =\\n\\nn\\n(cid:88)\\n\\ni=1\\n\\ni (cid:37)R(w(1)\\nw(2)\\n\\ni x + b(1)\\n\\ni\\n\\n) + b(2),\\n\\nx ∈ R,\\n\\ni\\n\\n, b(1)\\ni\\n\\n, w(2)\\ni\\n\\nwhere w(1)\\n, b(2) ∈ R for i ∈ [n]. It is not hard to see that Φ\\nis a continuous piecewise aﬃne linear function. Moreover, Φ has at\\nmost n + 1 aﬃne linear pieces. On the other hand, notice that the\\nhat function\\n\\nh : [0, 1] → [0, 1],\\n\\nx (cid:55)→ 2(cid:37)R(x) − 4(cid:37)R(x − 1\\n\\n2 ) =\\n\\n(cid:40)\\n\\n2x,\\n2(1 − x),\\n\\nif 0 ≤ x < 1\\n2 ,\\nif 1\\n2 ≤ x ≤ 1,\\n\\nis a NN with two layers and two neurons. Telgarsky observed that the\\nn-fold convolution hn(x) := h ◦ · · · ◦ h produces a sawtooth function\\nwith 2n spikes [Tel15]. In particular, hn admits 2n aﬃne linear pieces\\nwith only 2n many neurons. In this case, we see that deep ReLU NNs\\nare in some sense exponentially more eﬃcient in generating aﬃne\\nlinear pieces.\\n\\nFigure 3.2: Interpolation In of [0, 1] (cid:51)\\nx (cid:55)→ g(x) := x − x2 on 2n + 1 equidis-\\ntant points, which can be represented\\nas a sum In = (cid:80)n\\nk=1 Ik − Ik−1 =\\n(cid:80)n\\nhk\\n22k of n sawtooth functions.\\nEach sawtooth function hk = hk−1 ◦h\\nin turn can be written as a k-fold\\ncomposition of a hat function h. This\\nillustration is based on [EPGB19].\\n\\nk=1\\n\\nMoreover, it was noted in [Yar17] that the diﬀerence of interpolations of [0, 1] (cid:51) x (cid:55)→ x − x2 at 2n + 1\\nand 2n−1 + 1 equidistant points equals the scaled sawtooth function hn\\n22n , see Figure 3.2. This allows to\\neﬃciently implement approximative squaring and, by polarization, also approximative multiplication using\\nReLU NNs. Composing these simple functions one can approximate localized Taylor polynomials and thus\\nsmooth functions, see [Yar17]. We state below a generalization [GKP20] of the result of [Yar17] which includes\\nmore general norms, but for p = ∞ and s = 0 coincides with the original result of Dmitry Yarotsky.\\n\\nTheorem 3.2 (Approximation of Sobolev-regular functions). Let d, k ∈ N with k ≥ 2, let p ∈ [1, ∞], s ∈ [0, 1],\\nB ∈ (0, ∞), and let (cid:37) be a piecewise linear activation function with at least one break-point. Then there\\nexists a constant c ∈ (0, ∞) with the following property: For every ε ∈ (0, 1/2) there exists a NN architecture\\na = (N, (cid:37)) with\\n\\nP (N ) ≤ cε−d/(k−s) log(1/ε)\\n\\nsuch that for every function g ∈ W k,p((0, 1)d) with (cid:107)g(cid:107)W k,p((0,1)d) ≤ B it holds that\\n\\ninf\\nθ∈RP (N )\\n\\n(cid:107)Φa(θ, ·) − g(cid:107)W s,p((0,1)d) ≤ ε.\\n\\n31\\n\\n01828384858687810132232332432532632732832gI1gI1I2I1gI2I3I2\\x0cThe ability of deep ReLU neural networks to emulate multiplication has also been employed to reap-\\nproximate wide ranges of high-order ﬁnite element spaces. In [OPS20] and [MOPS20] it was shown that\\ndeep ReLU neural networks are capable of achieving the approximation rates of hp-ﬁnite element methods.\\nConcretely, this means that for piecewise analytic functions, which appear, for example, as solutions of elliptic\\nboundary and eigenvalue problems with analytic data, exponential approximation rates can be achieved. In\\nother words, the number of parameters of neural networks to approximate such a function in the W 1,2-norm\\nup to an error of ε is logarithmic in ε.\\n\\nTheorem 3.2 requires the depth of the NN to grow. In fact, it can be shown that the same approximation\\nrate cannot be achieved with shallow NNs. Indeed, there exists a certain optimal number of layers and, if\\nthe architecture has fewer layers than optimal, then the NNs need to have signiﬁcantly more parameters, to\\nachieve the same approximation ﬁdelity. This has been observed in many diﬀerent settings in [LS17, SS17,\\nYar17, PV18, EPGB19]. We state here the result of [Yar17]:\\n\\nTheorem 3.3 (Depth-width approximation trade-oﬀ). Let d, L ∈ N with L ≥ 2 and let g ∈ C 2([0, 1]d) be a\\nfunction which is not aﬃne linear. Then there exists a constant c ∈ (0, ∞) with the following property: For\\nevery ε ∈ (0, 1) and every ReLU NN architecture a = (N, (cid:37)R) = ((d, N1, . . . , NL−1, 1), (cid:37)R) with L layers and\\n(cid:107)N (cid:107)1 ≤ cε−1/(2(L−1)) neurons it holds that\\n\\ninf\\nθ∈RP (N )\\n\\n(cid:107)Φa(·, θ) − g(cid:107)L∞([0,1]d) ≥ ε.\\n\\nThis results is based on the observation that\\nReLU NNs are piecewise aﬃne linear. The number\\nof pieces they admit is linked to their capacity of\\napproximating functions that have non-vanishing\\ncurvature. Using a construction similar to the ex-\\nample at the beginning of this subsection, it can be\\nshown that the number of pieces that can be gener-\\nated using an architecture ((1, N1, . . . , NL−1, 1), (cid:37)R)\\nscales roughly like (cid:81)L−1\\n(cid:96)=1 N(cid:96).\\n\\nIn the framework of the aforementioned results,\\nwe can speak of a depth-width trade-oﬀ, see also Fig-\\nure 3.3. A ﬁne-grained estimate of achievable rates\\nfor freely varying depths has also been established\\nin [She20].\\n\\n3.3 Alternative notions of expressivity\\n\\ndepth\\n\\ni\\n\\nw\\nd\\nt\\nh\\n\\nFigure 3.3: Standard feed-forward neural network. For\\ncertain approximation results, depth and width need\\nto be in a ﬁxed relationship to achieve optimal results.\\n\\nConceptual approaches to study the approximation power of deep NNs besides the classical approximation\\nframework usually aim to relate structural properties of the NN to the “richness” of the set of possibly\\nexpressed functions. One early result in this direction is [MPCB14] which describes bounds on the number of\\naﬃne linear regions of a ReLU NN Φ(N,(cid:37)R)(·, θ). In a simpliﬁed setting, we have seen estimates on the number\\nof aﬃne linear pieces already at the beginning of Subsection 3.2. Aﬃne linear regions can be deﬁned as the\\nconnected components of RN0 \\\\ H, where H is the set of non-diﬀerentiability of the realization20 Φ(N,(cid:37)R)(·, θ).\\nA reﬁned analysis on the number of such regions was, for example, conducted by [HvdG19]. It is found that\\ndeep ReLU neural networks can exhibit signiﬁcantly more regions than their shallow counterparts.\\n\\n20One can also study the potentially larger set of activation regions given by the connected components of RN0 \\\\ (cid:0) ∪L−1\\n(cid:96)=1\\ni=1Hi,(cid:96)\\n\\n(cid:1), where\\n\\n∪N(cid:96)\\n\\nHi,(cid:96) := {x ∈ RN0 : Φ((cid:96))\\n\\ni\\n\\n(x, θ) = 0},\\n\\nwith Φ((cid:96))\\nlinear regions, the activation regions are necessarily convex [RPK+17, HR19].\\n\\ni\\n\\nas in (1.1), is the set of non-diﬀerentiability of the activation of the i-th neuron in the (cid:96)-th layer. In contrast to the\\n\\n32\\n\\n\\x0cFigure 3.4: Shape of the trajectory t (cid:55)→ Φ((2,n,...,n,2),(cid:37)R)(γ(t), θ) of the output of a randomly initialized\\nnetwork with 0, 3, 10 hidden layers. The input curve γ is the circle given in the leftmost image. The hidden\\nlayers have n = 20 neurons and the variance of the initialization is taken as 4/n.\\n\\nThe reason for this eﬀectiveness of depth is described by the following analogy: Through the ReLU each\\nneuron Rd (cid:51) x (cid:55)→ (cid:37)R((cid:104)x, w(cid:105) + b), w ∈ Rd, b ∈ R, splits the space into two aﬃne linear regions separated by\\nthe hyperplane\\n\\n{x ∈ Rd : (cid:104)x, w(cid:105) + b = 0}.\\n\\nA shallow ReLU NN Φ((d,n,1),(cid:37)R)(·, θ) with n neurons in the hidden layer therefore produces a number of\\nregions deﬁned through n hyperplanes. Using classical bounds on the number of regions deﬁned through\\n(cid:1). Deepening\\nhyperplane arrangements [Zas75], one can bound the number of aﬃne linear regions by (cid:80)d\\nneural networks then corresponds to a certain folding of the input space. Through this interpretation it can\\nbe seen that composing NNs can lead to a multiplication of the number of regions of the individual NNs\\nresulting in an exponential eﬃciency of deep neural networks in generating aﬃne linear regions21.\\n\\n(cid:0)n\\nj\\n\\nj=0\\n\\nThis approach was further developed in [RPK+17] to a framework to study expressivity that to some extent\\nallows to include the training phase. One central object studied in [RPK+17] are so-called trajectory lengths.\\nIn this context, one analyzes how the length of a non-constant curve in the input space changes in expectation\\nthrough the layers of a NN. The authors ﬁnd an exponential dependence of the expected curve length on the\\ndepth. Let us motivate this in the special case of a ReLU NN with architecture a = ((N0, n, . . . , n, NL), (cid:37)R)\\nand depth L ∈ N.\\n\\nGiven a non-constant continuous curve γ : [0, 1] → RN0 in the input space, the length of the trajectory in\\n\\nthe (cid:96)-th layer of the NN Φa(·, θ) is then given by\\n\\nLength( ¯Φ((cid:96))(γ(·), θ)),\\n\\n(cid:96) ∈ [L − 1],\\n\\nwhere ¯Φ((cid:96))(·, θ) is the activation in the (cid:96)-th layer, see (1.1). Here the length of the curve is well-deﬁned since\\n¯Φ((cid:96))(·, θ)) is continuous and therefore ¯Φ((cid:96))(γ(·), θ) is continuous. Now, let the parameters Θ1 of the NN Φa\\nbe initialized independently so that the entries corresponding to the weight matrices and bias vectors follow\\na normal distribution with zero mean and variances 1/n and 1, respectively. It is not hard to see, e.g., by\\nProposition 1.1, that the probability that ¯Φ((cid:96))(·, Θ1) will map γ to a non-constant curve is positive and hence,\\nfor ﬁxed (cid:96) ∈ [L − 1],\\n\\nE(cid:2) Length( ¯Φ((cid:96))(γ(·), Θ1))(cid:3) = c > 0.\\nLet σ ∈ (0, ∞) and consider a second initialization Θσ, where we change the variances of the entries\\ncorresponding to the weight matrices and bias vectors to σ2/n and σ2, respectively. Recall that the ReLU is\\npositively homogeneous, i.e., we have that (cid:37)R(λx) = λ(cid:37)R(x) for all λ ∈ (0, ∞). Then it is clear that\\n\\n21However, to exploit this eﬃciency with respect to the depth, one requires highly oscillating pre-activations which in turn can\\nonly be achieved with a delicate selection of parameters. In fact, it can be shown that through random initialization the expected\\nnumber of activation regions per unit cube depends mainly on the number of neurons in the NN, rather than its depth [HR19].\\n\\n¯Φ((cid:96))(·, Θσ) ∼ σ(cid:96) ¯Φ((cid:96))(·, Θ1),\\n\\n33\\n\\n\\x0ci.e., the activations corresponding to the two initialization strategies are identically distributed up to the\\nfactor σ(cid:96). Therefore, we immediately conclude that\\n\\nE(cid:2) Length( ¯Φ((cid:96))(γ(·), Θσ))(cid:3) = σ(cid:96)c.\\n\\nThis shows that the expected trajectory length depends exponentially on the depth of the NN, which is in\\nline with the behavior of other notions of expressivity [PLR+16]. In [RPK+17] this result is also extended to\\nthe tanh activation function and the constant c is more carefully resolved. Empirically one also ﬁnds that the\\nshapes of the trajectories become more complex in addition to becoming longer on average, see Figure 3.4.\\n\\n4 Deep neural networks overcome the curse of dimensionality\\n\\nIn Subsection 1.3, one of the main puzzles of deep learning that we\\nidentiﬁed was the surprising performance of deep architectures on\\nproblems where the input dimensions are very high. This perfor-\\nmance cannot be explained in the framework of classical approx-\\nimation theory, since such results always suﬀer from the curse of\\ndimensionality [Bel52, DeV98, NW09].\\n\\nIn this section, we present three approaches that oﬀer explana-\\ntions of this phenomenon. As before, we had to omit certain ideas\\nwhich have been very inﬂuential in the literature to keep the length\\nof this section under control. In particular, an important line of\\nreasoning is that functions to be approximated often have composi-\\ntional structures which NNs may approximate very well as reviewed\\nin [PMR+17]. Note that also a suitable feature descriptor, factor-\\ning out invariances, might lead to a signiﬁcantly reduced eﬀective\\ndimension, see Subsection 7.1.\\n\\n4.1 Manifold assumption\\n\\nA ﬁrst remedy to the high-dimensional curse of dimensionality is\\nwhat we call the manifold assumption. Here it is assumed that we\\nare trying to approximate a function\\n\\ng : Rd ⊃ X → R,\\n\\nM\\n\\nFigure 4.1:\\nIllustration of a one-\\ndimensional manifold M embedded\\nin R3. For every point x ∈ M there\\nexists a neighborhood in which the\\nmanifold can be linearly projected\\nonto its tangent space at x such that\\nthe corresponding inverse function is\\ndiﬀerentiable.\\n\\nwhere d is very large. However, we are not seeking to optimize with respect to the uniform norm or a regular\\nLp space, but instead consider a measure µ which is supported on a d(cid:48)-dimensional manifold M ⊂ X . Then\\nthe error is measured in the Lp(µ)-norm. Here we consider the case where d(cid:48) (cid:28) d. This setting is appropriate\\nif the data z = (x, y) of a prediction task is generated from a measure supported on M × R.\\n\\nThis set-up or generalizations thereof have been fundamental in [CM18, SCC18, CJLZ19, SH19, CK20,\\nNI20]. Let us describe an exemplary approach, where we consider locally C k-regular functions and NNs with\\nReLU activation functions below:\\n\\n1. Describe the regularity of g on the manifold: Naturally, we need to quantify the regularity of the\\nfunction g restricted to M in an adequate way. The typical approach would be to make a deﬁnition\\nvia local coordinate charts. If we assume that M is an embedded submanifold of X , then locally,\\ni.e., in a neighborhood of a point x ∈ M, the orthogonal projection of M onto the d(cid:48)-dimensional\\ntangent space TxM is a diﬀeomorphism. The situation is depicted in Figure 4.1. Assuming M to\\nbe compact, we can choose a ﬁnite set of open balls (Ui)p\\ni=1 that cover M and on which the local\\nprojections γi onto the respective tangent spaces as described above exists and are diﬀeomorphisms.\\nNow we can deﬁne the regularity of g via classical regularity. In this example, we say that g ∈ C k(M)\\nif g ◦ γ−1\\n\\ni ∈ C k(γi(M ∩ Ui)) for all i ∈ [p].\\n\\n34\\n\\n\\x0c2. Construct localization and charts via neural networks: According to the construction of local coordinate\\n\\ncharts in Step 1, we can write g as follows:\\n\\ng(x) =\\n\\np\\n(cid:88)\\n\\ni=1\\n\\nφi(x) (cid:0)g ◦ γ−1\\n\\ni\\n\\n(γi(x))(cid:1) =:\\n\\np\\n(cid:88)\\n\\ni=1\\n\\n˜gi(γi(x), φi(x)),\\n\\nx ∈ M,\\n\\n(4.1)\\n\\nwhere φi is a partition of unity such that supp(φi) ⊂ Ui. Note that γi is a linear map, hence\\nrepresentable by a one-layer NN. Since multiplication is a smooth operation, we have that if g ∈ C k(M)\\nthen ˜gi ∈ C k(γi(M ∩ Ui) × [0, 1]).\\nThe partition of unity φi needs to be emulated by NNs. For example, if the activation function is the\\nReLU, then such a partition can be eﬃciently constructed. Indeed, in [HLXZ20] it was shown that\\nsuch NNs can represent linear ﬁnite elements exactly with ﬁxed-size NNs and hence a partition of unity\\nsubordinate to any given covering of M can be constructed.\\n\\n3. Use a classical approximation result on the localized functions: By some form of Whitney’s extension\\ntheorem [Whi34], we can extend each ˜gi to a function ¯gi ∈ C k(X ×[0, 1]) which by classical results can be\\napproximated up to an error of ε > 0 by NNs of size O(ε−(d(cid:48)+1)/k) for ε → 0, see [Mha96, Yar17, SCC18].\\n\\n4. Use the compositionality of neural networks to build the ﬁnal network: We have seen that every\\ncomponent in the representation (4.1), i.e., ˜gi, γi, and φi can be eﬃciently represented by NNs. In\\naddition, composition and summation are operations which can directly be implemented by NNs through\\nincreasing their depth and widening their layers. Hence (4.1) is eﬃciently—i.e., with a rate depending\\nonly on d(cid:48) instead of the potentially much larger d—approximated by a NN.\\n\\nOverall, we see that NNs are capable of learning local coordinate transformations and therefore reduce\\nthe complexity of a high-dimensional problem to the underlying low-dimensional problem given by the data\\ndistribution.\\n\\n4.2 Random sampling\\n\\nAlready in 1992, Andrew Barron showed that under certain seemingly very natural assumptions on the\\nfunction to approximate, a dimension-independent approximation rate by NNs can be achieved [Bar92, Bar93].\\nSpeciﬁcally, the assumption is formulated as a condition on the Fourier transform of a function and the result\\nis as follows.\\n\\nTheorem 4.1 (Approximation of Barron-regular functions). Let (cid:37) : R → R be the ReLU or a sigmoidal\\nfunction. Then there exists a constant c ∈ (0, ∞) with the following property: For every d, n ∈ N, every\\nprobability measure µ supported on B1(0) ⊂ Rd, and every g ∈ L1(Rd) with Cg := (cid:82)\\nRd (cid:107)ξ(cid:107)2|ˆg(ξ)| dξ < ∞ it\\nholds that\\n\\ninf\\nθ∈RP ((d,n,1))\\n\\n(cid:107)Φ((d,n,1),(cid:37))(·, θ) − g(cid:107)L2(µ) ≤\\n\\nc\\n√\\nn\\n\\nCg,\\n\\nNote that the L2-approximation error can be replaced by an L∞-estimate over the unit ball at the expense\\n\\n√\\n\\nof a factor of the order of\\n\\nd on the right-hand side.\\n\\nThe key idea behind Theorem 4.1 is the following application of the law of large numbers: First, we\\n\\nobserve that, per assumption, g can be represented via the inverse Fourier transform, as\\n\\n(cid:90)\\n\\ng − g(0) =\\n\\nˆg(ξ)(e2πi(cid:104)·,ξ(cid:105) − 1) dξ\\n\\nRd\\n(cid:90)\\n\\n= Cg\\n\\n= Cg\\n\\nRd\\n\\n(cid:90)\\n\\nRd\\n\\n(e2πi(cid:104)·,ξ(cid:105) − 1)\\n\\n1\\nCg\\n\\n(cid:107)ξ(cid:107)2ˆg(ξ) dξ\\n\\n(e2πi(cid:104)·,ξ(cid:105) − 1) dµg(ξ),\\n\\n1\\n(cid:107)ξ(cid:107)2\\n1\\n(cid:107)ξ(cid:107)2\\n\\n35\\n\\n(4.2)\\n\\n\\x0cwhere µg is a probability measure. Then it is further shown in [Bar92] that there exist (Rd × R)-valued\\nrandom variables (Ξ, (cid:101)Ξ) such that (4.2) can be written as\\n\\ng(x) − g(0) = Cg\\n\\n(cid:90)\\n\\nRd\\n\\n1\\n(cid:107)ξ(cid:107)2\\n\\n(e2πi(cid:104)x,ξ(cid:105) − 1) dµg(ξ) = CgE(cid:2)Γ(Ξ, (cid:101)Ξ)(x)(cid:3),\\n\\nx ∈ Rd,\\n\\n(4.3)\\n\\nwhere for every ξ ∈ Rd, ˜ξ ∈ R the function Γ(ξ, ˜ξ) : Rd → R is given by\\n\\nΓ(ξ, ˜ξ) := s(ξ, ˜ξ)(1(0,∞)(−(cid:104)ξ/(cid:107)ξ(cid:107)2, ·(cid:105) − ˜ξ) − 1(0,∞)((cid:104)ξ/(cid:107)ξ(cid:107)2, ·(cid:105) − ˜ξ)) with s(ξ, ˜ξ) ∈ {−1, 1}.\\n\\nNow, let ((Ξ(i), (cid:101)Ξ(i)))i∈N be i.i.d. random variables with (Ξ(1), (cid:101)Ξ(1)) ∼ (Ξ, (cid:101)Ξ). Then, Bienaym´e’s identity and\\nFubini’s theorem establish that\\n\\n(cid:34)\\n(cid:13)\\n(cid:13)\\n(cid:13)g − g(0) −\\n\\nE\\n\\nCg\\nn\\n\\nn\\n(cid:88)\\n\\ni=1\\n\\n(cid:13)\\n2\\nΓ(Ξ(i), (cid:101)Ξ(i))\\n(cid:13)\\n(cid:13)\\n\\nL2(µ)\\n\\n(cid:35)\\n\\n(cid:90)\\n\\n=\\n\\nV\\n\\n(cid:34)\\n\\nB1(0)\\n(cid:82)\\n\\nC 2\\ng\\n\\nB1(0)\\n\\n=\\n\\nn\\n(cid:88)\\n\\nΓ(Ξ(i), (cid:101)Ξ(i))(x)\\n\\nCg\\nn\\nV(cid:2)Γ(Ξ, (cid:101)Ξ)(x)(cid:3) dµ(x)\\n\\ni=1\\n\\nn\\n\\n(cid:35)\\n\\ndµ(x)\\n\\n(4.4)\\n\\n≤\\n\\n(2πCg)2\\nn\\n\\n,\\n\\nwhere the last inequality follows from combining (4.3) with the fact that |e2πi(cid:104)x,ξ(cid:105) − 1|/(cid:107)ξ(cid:107)2 ≤ 2π, x ∈ B1(0).\\nThis implies that there exists a realization ((ξ(i), ˜ξ(i)))i∈N of the random variables ((Ξ(i), (cid:101)Ξ(i)))i∈N that\\nachieves L2-approximation error of n−1/2. Therefore, it remains to show that NNs can well approximate\\nthe functions ((Γ(ξ(i), ˜ξ(i)))i∈N. Now it is not hard to see that the function 1(0,∞) and hence functions of\\nthe form Γ(ξ, ˜ξ), ξ ∈ Rd, ˜ξ ∈ R, can be arbitrarily well approximated with a ﬁxed-size, two-layer NN with a\\nsigmoidal or ReLU activation function. Thus, we obtain an approximation rate of n−1/2 when approximating\\nfunctions with one ﬁnite Fourier moment by two-layer NNs with n hidden neurons.\\n\\nIt was pointed out already in the dissertation of Emmanuel Cand`es [Can98] that the approximation rate\\nof NNs for Barron-regular functions is also achievable by n-term approximation with complex exponentials, as\\nis apparent by considering (4.2). However, for deeper NNs, the results also extend to high-dimensional non-\\nsmooth functions, where Fourier-based methods are certain to suﬀer from the curse of dimensionality [CPV20].\\nIn addition, the random sampling idea above was extended in [EMW19b, EMWW20, EW20b, EW20c]\\nto facilitate dimension-independent approximation of vastly more general function spaces. Basically, the\\nidea is to use (4.3) as an inspiration and deﬁne the generalized Barron space as all functions that may be\\nrepresented as\\n\\nE(cid:2)1(0,∞)((cid:104)Ξ, ·(cid:105) − (cid:101)Ξ)(cid:3)\\n\\nfor any random variable (Ξ, (cid:101)Ξ). In this context, deep and compositional versions of Barron spaces were\\nintroduced and studied in [BK18, EMW19a, EW20a], which considerably extend the original theory.\\n\\n4.3 PDE assumption\\n\\nAnother structural assumption that leads to the absence of the curse of dimensionality in some cases is that\\nthe function we are trying to approximate is given as the solution to a partial diﬀerential equation. It is by\\nno means clear that this assumption leads to approximation without the curse of dimensionality, since most\\nstandard methods, such as ﬁnite elements, sparse grids, or spectral methods typically suﬀer from the curse of\\ndimensionality.\\n\\nThis is not merely an abstract theoretical problem: Very recently, in [AHNB+20] it was shown that two\\ndiﬀerent gold standard methods for solving the multi-electron Schr¨odinger equation produce completely\\ndiﬀerent interaction energy predictions when applied to large delocalized molecules. Classical numerical\\nrepresentations are simply not expressive enough to accurately represent complicated high-dimensional\\nstructures such as wave functions with long-range interactions.\\n\\nInterestingly, there exists an emerging body of work that shows that NNs do not suﬀer from these\\nshortcomings and enjoy superior expressivity properties as compared to standard numerical representations.\\n\\n36\\n\\n\\x0cSuch results include, for example, [GHJVW20, GS20, HJKN20] for (linear and semilinear) parabolic evolution\\nequations, [GH22] for stationary elliptic PDEs, [GH21] for nonlinear Hamilton–Jacobi–Bellman equations,\\nor [KPRS19] for parametric PDEs. In all these cases, the absence of the curse of dimensionality in terms of\\nthe theoretical approximation power of NNs could be rigorously established.\\n\\nOne way to prove such results is via stochastic representations of the PDE solutions, as well as associated\\nsampling methods. We illustrate the idea for the simple case of linear Kolmogorov PDEs, that is the problem\\nof representing the function g : Rd × [0, ∞) → R satisfying22\\n\\n∂g\\n∂t\\n\\n(x, t) =\\n\\n1\\n2\\n\\nwhere the functions\\n\\nTr(cid:0)σ(x, t)[σ(x, t)]∗∇2\\n\\nxg(x, t)(cid:1) + (cid:104)µ(x, t), ∇xg(x, t)(cid:105),\\n\\ng(x, 0) = ϕ(x),\\n\\n(4.5)\\n\\nϕ : Rd → R (initial condition) and σ : Rd → Rd×d, µ : Rd → Rd\\n\\n(coeﬃcient functions)\\n\\nare continuous and satisfy suitable growth conditions. A stochastic representation of g is given via the Ito\\nprocesses (Sx,t)t≥0 satisfying\\n\\ndSx,t = µ(Sx,t)dt + σ(Sx,t)dBt, Sx,0 = x,\\n\\n(4.6)\\n\\nwhere (Bt)t≥0 is a d-dimensional Brownian motion. Then g is described via the Feynman–Kac formula which\\nstates that\\n\\ng(x, t) = E[ϕ(Sx,t)],\\n\\nx ∈ Rd, t ∈ [0, ∞).\\n\\n(4.7)\\n\\nRoughly speaking, a NN approximation result can be proven by ﬁrst approximating, via the law of large\\nnumbers,\\n\\ng(x, t) = E[ϕ(Sx,t)] ≈\\n\\n1\\nn\\n\\nn\\n(cid:88)\\n\\ni=1\\n\\nϕ(S (i)\\n\\nx,t),\\n\\n(4.8)\\n\\nx,t)n\\n\\ni=1 are i.i.d. random variables with S (1)\\n\\nwhere (S (i)\\nx,t ∼ Sx,t. Care has to be taken to establish such\\nan approximation uniformly in the computational domain, for example, for every (x, t) in the unit cube\\n[0, 1]d × [0, 1], see (4.4) for a similar estimate and [GHJVW20, GS20] for two general approaches to ensure\\nthis property. Aside from this issue, (4.8) represents a standard Monte Carlo estimator which can be shown\\nto be free of the curse of dimensionality.\\n\\nAs a next step, one needs to establish that realizations of the processes (x, t) (cid:55)→ Sx,t can be eﬃciently\\napproximated by NNs. This can be achieved by emulating a suitable time-stepping scheme for the SDE (4.6)\\nby NNs which, roughly speaking, can be done without incurring the curse of dimensionality whenever the\\ncoeﬃcient functions µ, σ can be approximated by NNs without incurring the curse of dimensionality and some\\ngrowth conditions hold true. In a last step one assumes that the initial condition ϕ can be approximated by\\nNNs without incurring the curse of dimensionality which, by the compositionality of NNs and the previous\\nstep, directly implies that realizations of the processes (x, t) (cid:55)→ ϕ(Sx,t) can be approximated by NNs without\\nincurring the curse of dimensionality. By (4.8) this implies a corresponding approximation result for the\\nsolution of the Kolmogorov PDE g in (4.5).\\n\\nInformally, we have discovered a regularity result for linear Kolmogorov equations, namely that (modulo\\nsome technical conditions on µ, σ), the solution g of (4.5) can be approximated by NNs without incurring the\\ncurse of dimensionality whenever the same holds true for the initial condition ϕ, as well as the coeﬃcient\\nfunctions µ, σ. In other words, the property of being approximable by NNs without curse of dimensionality is\\npreserved under the ﬂow induced by the PDE (4.5). Some comments are in order:\\n\\n22The natural solution concept to this type of PDEs is the viscosity solution concept, a thorough study of which can be found\\n\\nin [HHJ15].\\n\\n37\\n\\n\\x0cAssumption on the initial condition: One may wonder if the assumption that the initial condition\\nϕ can be approximated by NNs without incurring the curse of dimensionality is justiﬁed. This is at least\\nthe case in many applications in computational ﬁnance where the function ϕ typically represents an option\\npricing formula and (4.5) represents the famous Black–Scholes model. It turns out that nearly all common\\noption pricing formulas are constructed from iterative applications of linear maps and maximum/minimum\\nfunctions—in other words, in many applications in computational ﬁnance, the initial condition ϕ can be\\nexactly represented by a small ReLU NN.\\n\\nGeneralization and optimization error: The\\nFeynman–Kac representation (4.7) directly implies\\nthat g(·, t) can be computed as the Bayes opti-\\nmal function of a regression task with input fea-\\ntures X ∼ U([0, 1]d) and labels Y = ϕ(SX,t), which\\nallows for an analysis of the generalization error\\nas well as implementations based on ERM algo-\\nrithms [BGJ20, BBG+21].\\n\\nWhile it is in principle possible to analyze the\\napproximation and generalization error, the analysis\\nof the computational cost and/or convergence of\\ncorresponding SGD algorithms is completely open.\\nSome promising numerical results exist, see, for\\ninstance, Figure 4.2, but the stable training of NNs\\napproximating PDEs to very high accuracy (that\\nis needed in several applications such as quantum\\nchemistry) remains very challenging. The recent\\nwork [GV21] has even proven several impossibility\\nresults in that direction.\\n\\nFigure 4.2: Computational complexity as number of\\nneural network parameters times number of SGD steps\\nto solve heat equations of varying dimensions up to\\na speciﬁed precision. According to the ﬁt above, the\\nscaling is polynomial in the dimension [BDG20].\\n\\nExtensions and abstract idea: Similar techniques may be used to prove expressivity results for nonlinear\\nPDEs, for example, using nonlinear Feynman–Kac-type representations of [PP92] in place of (4.7) and\\nmultilevel Picard sampling algorithms of [EHJK19] in place of (4.8).\\n\\nWe can also formulate the underlying idea in an abstract setting (a version of which has also been used in\\nSubsection 4.2). Assume that a high-dimensional function g : Rd → R admits a probabilistic representation of\\nthe form\\n\\ng(x) = E[Yx],\\n\\nx ∈ Rd,\\n\\n(4.9)\\n\\nfor some random variable Yx which can be approximated by an iterative scheme\\n\\nY (L)\\n\\nx ≈ Yx\\n\\nand Y ((cid:96))\\n\\nx = T(cid:96)(Y ((cid:96)−1)\\n\\nx\\n\\n),\\n\\n(cid:96) = 1, . . . , L,\\n\\nwith dimension-independent convergence rate. If we can approximate realizations of the initial mapping\\nx (cid:55)→ Y 0\\nx and the maps T(cid:96), (cid:96) ∈ [L], by NNs and the numerical scheme is stable enough, then we can also\\napproximate Y (L)\\nusing compositionality. Emulating a uniform Monte-Carlo approximator of (4.9) then\\nleads to approximation results for g without curse of dimensionality. In addition, one can choose a Rd-valued\\nrandom variable X as input features and deﬁne the corresponding labels by YX to obtain a prediction task,\\nwhich can be solved by means of ERM.\\n\\nx\\n\\nOther methods: There exist a number of additional works related to the approximation capacities of\\nNNs for high-dimensional PDEs, for example, [EGJS18, LTY19, SZ19]. In most of these works, the proof\\ntechnique consists of emulating an existing method that does not suﬀer from the curse of dimensionality. For\\ninstance, in the case of ﬁrst-order transport equations, one can show in some cases that NNs are capable of\\nemulating the method of characteristics, which then also yields approximation results that are free of the\\ncurse of dimensionality [LP21].\\n\\n38\\n\\n101102input dimension10710810910101011xcx2.36#parameters  avg. #steps±2std.\\x0c5 Optimization of deep neural networks\\n\\nWe recall from Subsections 1.3 and 1.2.1 that the standard algorithm to solve the empirical risk minimization\\nproblem over the hypothesis set of NNs is stochastic gradient descent. This method would be guaranteed\\nto converge to a global minimum of the objective if the empirical risk were convex, viewed as a function of\\nthe NN parameters. However, this function is severely nonconvex, may exhibit (higher-order) saddle points,\\nseriously suboptimal local minima, and wide ﬂat areas where the gradient is very small.\\n\\nOn the other hand, in applications, excellent performance of SGD is observed. This indicates that the\\ntrajectory of the optimization routine somehow misses suboptimal critical points and other areas that may\\nlead to slow convergence. Clearly, the classical theory does not explain this performance. Below we describe\\nsome exemplary novel approaches that give partial explanations of this success.\\n\\nIn the ﬂavor of this article, the aim of this section is to present some selected ideas rather than giving an\\noverview of the literature. To give at least some detail about the underlying ideas and to keep the length of\\nthis section reasonable, a selection of results had to be made and some ground-breaking results had to be\\nomitted.\\n\\n5.1 Loss landscape analysis\\n\\nGiven a NN Φ(·, θ) and training data s ∈ Z m the function θ (cid:55)→ r(θ) := (cid:98)Rs(Φ(·, θ)) describes, in a natural\\nway, through its graph, a high-dimensional surface. This surface may have regions associated with lower\\nvalues of (cid:98)Rs which resemble valleys of a landscape if they are surrounded by regions of higher values. The\\nanalysis of the topography of this surface is called loss landscape analysis. Below we shall discuss a couple of\\napproaches that yield deep insights into the shape of this landscape.\\n\\nSpin glass interpretation: One of the ﬁrst discoveries about the\\nshape of the loss landscape comes from deep results in statistical\\nphysics. The Hamiltonian of the spin glass model is a random function\\non the (n − 1)-dimensional sphere of radius\\nn. Making certain\\nsimplifying assumptions, it was shown in [CHM+15] that the loss of\\na NN with random inputs can be considered as the Hamiltonian of a\\nspin glass model, where the inputs of the model are the parameters\\nof the NN.\\n\\n√\\n\\ns\\ns\\no\\nL\\n\\nNo negative curvature\\nat globally minimal\\nrisk.\\n\\nCritical points\\nwith high risk\\nare unstable.\\n\\nThis connection has far-reaching implications for the loss land-\\nscape of NNs because of the following surprising property of the\\nHamiltonian of spin glass models: Consider the set of critical points\\nof this set, and associate to each point an index that denotes the\\npercentage of the eigenvalues of the Hessian at that point which are\\nnegative. This index corresponds to the relative number of directions\\nin which the loss landscape has negative curvature. Then with high\\nprobability, a picture like we see in Figure 5.1 emerges [AA ˇC13].\\nMore precisely, the further away from the optimal loss we are, the more unstable the critical points become.\\nConversely, if one ﬁnds oneself in a local minimum, it is reasonable to assume that the loss is close to the\\nglobal minimum.\\n\\nFigure 5.1: Sketch of the distribution\\nof critical points of the Hamiltonian\\nof a spin glass model.\\n\\nIndex\\n\\n0.25\\n\\n0.5\\n\\n0\\n\\nWhile some of the assumptions establishing the connection between the spin glass model and NNs are\\nunrealistic in practice [CLA15], the theoretical distribution of critical points as in Figure 5.1 is visible in\\nmany practical applications [DPG+14].\\n\\nPaths and level sets: Another line of research is to understand the loss landscape by analyzing paths\\nthrough the parameter space. In particular, the existence of paths in parameter space, such that the associated\\nempirical risks are monotone along the path. Surely, should there exist a path of nonincreasing empirical risk\\nfrom every point to the global minimum, then we can be certain that no non-global minima exist, since no\\n\\n39\\n\\n\\x0csuch path can escape a minimum. An even stronger result holds. In fact, the existence of such paths shows\\nthat the loss landscape has connected level sets [FB17, VBB19].\\n\\nA crucial ingredient of the analysis of such paths are linear substructures. Consider a biasless two-layer\\n\\nNN Φ of the form\\n\\nRd (cid:51) x (cid:55)→ Φ(x, θ) :=\\n\\nn\\n(cid:88)\\n\\nj=1\\n\\nj (cid:37)(cid:0)(cid:104)θ(1)\\nθ(2)\\n\\nj\\n\\n,\\n\\n(cid:21)\\n(cid:105)(cid:1),\\n\\n(cid:20)x\\n1\\n\\n(5.1)\\n\\nwhere θ(1)\\nj ∈ Rd+1 for j ∈ [n], θ(2) ∈ Rn, (cid:37) is a Lipschitz continuous activation function, and we augment the\\nvector x by a constant 1 in the last coordinate as outlined in Remark 1.5. If we consider θ(1) to be ﬁxed,\\nthen it is clear that the space\\n\\n(cid:101)Fθ(1) := {Φ(·, θ) : θ = (θ(1), θ(2)), θ(2) ∈ Rn}\\n\\nis a linear space. If the risk23 is convex, as is the case for the widely used quadratic or logistic loss, then\\nthis implies that θ(2) (cid:55)→ r(cid:0)(θ(1), θ(2))(cid:1) is a convex map and hence, for every parameter set P ⊂ Rn this map\\nassumes its maximum on ∂P. Therefore, within the vast parameter space, there are many paths traveling\\nalong which does not increase the risk above the risk of the start and end points.\\n\\nThis idea was, for example, used in [FB17] in a way similar to the following simple sketch: Assume\\nthat, for two parameters θ and θmin there exists a linear subspace of NNs (cid:101)Fˆθ(1) such that there are paths γ1\\nand γ2 connecting Φ(·, θ) and Φ(·, θmin) to (cid:101)Fˆθ(1) respectively. Further assume that the paths are such that\\nalong γ1 and γ2 the risk does not signiﬁcantly exceed max{r(θ), r(θmin)}. Figure 5.2 shows a visualization of\\nthese paths. In this case, a path from θ to θmin not signiﬁcantly exceeding r(θ) along the way is found by\\nconcatenating the paths γ1, a path along (cid:101)Fˆθ(1), and γ2. By the previous discussion, we know that only γ1\\nand γ2 determine the extent to which the combined path exceeds r(θ) along its way. Hence, we need to ask\\nabout the existence of (cid:101)Fˆθ(1) that facilitates the construction of appropriate γ1 and γ2.\\n\\nTo understand why a good choice of (cid:101)Fˆθ(1), so that the risk along\\nγ1 and γ2 will not rise much higher than r(θ), is likely possible, we\\nset24\\n\\nˆθ(1)\\nj\\n\\n:=\\n\\n(cid:40)\\n\\nθ(1)\\nj\\n(θ(1)\\n\\nmin)j\\n\\nfor j ∈ [n/2],\\nfor j ∈ [n] \\\\ [n/2].\\n\\nj\\n\\nmin. If θ(1)\\n\\nIn other words, the ﬁrst half of ˆθ(1) is made from θ(1) and the\\nsecond from θ(1)\\n, j ∈ [N ], are realizations of random variables\\ndistributed uniformly on the d-dimensional unit sphere, then by\\ninvoking standard covering bounds of spheres (e.g., [Ver18, Corollary\\n4.2.13]), we expect that, for ε > 0 and a suﬃciently large number\\nof neurons n, the vectors (θ(1)\\n)n/2\\nj=1 already ε-approximate all vectors\\nj=1. Replacing all vectors (θ(1)\\n(θ(1)\\n)n\\n)n\\nj=1 by their nearest neighbor in\\nj\\n(θ(1)\\n)n/2\\nj=1 can be done with a linear path in the parameter space, and,\\nj\\ngiven that r is locally Lipschitz continuous and (cid:107)θ(2)(cid:107)1 is bounded,\\nthis operation will not increase the risk by more than O(ε). We\\ndenote the vector resulting from this replacement procedure by θ(1)\\n∗ .\\nSince for all j ∈ [n] \\\\ [n/2] we now have that\\n\\nj\\n\\nj\\n\\n(cid:37)(cid:0)(cid:104)(θ(1)\\n\\n∗ )j,\\n\\n(cid:26)\\n\\n(cid:21)\\n\\n(cid:20) ·\\n1\\n\\n(cid:105)(cid:1) ∈\\n\\n(cid:37)(cid:0)(cid:104)(θ(1)\\n\\n∗ )k,\\n\\n(cid:105)(cid:1) : k ∈ [n/2]\\n\\n(cid:27)\\n\\n,\\n\\n(cid:21)\\n\\n(cid:20) ·\\n1\\n\\nγ1\\n\\nΦ(·, θ∗)\\n\\nΦ(·, θ)\\n\\n(cid:101)F ˆθ(1)\\n\\nΦ(·, θmin)\\n\\nFigure 5.2: Construction of a path\\nfrom an initial point θ to the global\\nminimum θmin that does not have sig-\\nniﬁcantly higher risk than the initial\\npoint along the way. We depict here\\nthe landscape as a function of the\\nneural network realizations instead\\nof their parametrizations so that this\\nlandscape is convex.\\n\\n23As most statements in this subsection are valid for the empirical risk r(θ) = (cid:98)Rs(Φ(·, θ)) as well as the risk r(θ) = R(Φ(·, θ)),\\n\\ngiven a suitable distribution of Z, we will just call r the risk.\\n\\n24We assume w.l.o.g. that n is a multiple of 2.\\n\\n40\\n\\n\\x0cthere exists a vector θ(2)\\n\\n∗ with (θ(2)\\n\\n∗ )j = 0, j ∈ [n] \\\\ [n/2], so that\\n\\nΦ(·, (θ(1)\\n\\n∗ , θ(2))) = Φ(·, (θ(1)\\n\\n∗ , λθ(2)\\n\\n∗ + (1 − λ)θ(2))),\\n\\nλ ∈ [0, 1].\\n\\nIn particular, this path does not change the risk between (θ(1)\\n∗ )j = 0\\nfor j ∈ [n] \\\\ [n/2], the realization Φ(·, (θ(1)\\n∗ )) is computed by a sub-network consisting of the ﬁrst n/2\\nhidden neurons and we can replace the parameters corresponding to the other neurons without any eﬀect on\\nthe realization function. Speciﬁcally, it holds that\\n\\n∗ ). Now, since (θ(2)\\n\\n∗ , θ(2)) and (θ(1)\\n\\n∗ , θ(2)\\n\\n∗ , θ(2)\\n\\nΦ(·, (θ(1)\\n\\n∗ , θ(2)\\n\\n∗ )) = Φ(·, (λˆθ(1) + (1 − λ)θ(1)\\n\\n∗ , θ(2)\\n\\n∗ )),\\n\\nλ ∈ [0, 1],\\n\\nyielding a path of constant risk between (θ(1)\\n∗ ). Connecting these paths completes the\\nconstruction of γ1 and shows that the risk along γ1 does not exceed that at θ by more than O(ε). Of course,\\nγ2 can be constructed in the same way. The entire construction is depicted in Figure 5.2.\\n\\n∗ ) and (ˆθ(1), θ(2)\\n\\n∗ , θ(2)\\n\\nOverall, this derivation shows that for suﬃciently wide NNs (appropriately randomly initialized) it is very\\nlikely possible to almost connect a random parameter value to the global minimum with a path which along\\nthe way does not need to climb much higher than the initial risk.\\n\\nIn [VBB19], a similar approach is taken and the convexity in the last layer is used. However, the authors\\ninvoke the concept of intrinsic dimension to elegantly solve the non-linearity of r((θ(1), θ(2))) with respect to\\nθ(1). Additionally, [SS16] constructs a path of decreasing risk from random initializations. The idea here is\\nthat if one starts at a point of suﬃciently high risk, one can always ﬁnd a path to the global optimum with\\nstrictly decreasing risk. The intriguing insight behind this result is that if the initialization is suﬃciently\\nbad, i.e., worse than that of a NN outputting only zero, then there exist two operations that inﬂuence the\\nrisk directly. Multiplying the last layer with a number smaller than one will decrease the risk, whereas the\\nopposite will increase it. Using this tuning mechanism, any given potentially non-monotone path from the\\ninitialization to the global minimum can be modiﬁed so that it is strictly monotonically decreasing. In a\\nsimilar spirit, [NH17] shows that if a deep NN has a layer with more neurons than training data points, then\\nunder certain assumptions the training data will typically be mapped to linearly independent points in that\\nlayer. Of course, this layer could then be composed with a linear map that maps the linearly independent\\npoints to any desirable output, in particular one that achieves vanishing empirical risk, see also Proposition 1.1.\\nAs for two-layer NNs, the previous discussion on linear paths immediately shows that in this situation a\\nmonotone path to the global minimum exists.\\n\\n5.2 Lazy training and provable convergence of stochastic gradient descent\\n\\nWhen training highly overparametrized NNs, one often observes that the parameters of the NNs barely\\nchange during training. In Figure 5.3, we show the relative distance that the parameters travel through the\\nparameter space during the training of NNs of varying numbers of neurons per layer.\\n\\nThe eﬀect described above has been observed repeatedly and theoretically explained, see, e.g., [DZPS18,\\nLL18, AZLS19, DLL+19, ZCZG20]. In Subsection 2.1, we have already seen a high-level overview and, in\\nparticular, the function space perspective of this phenomenon in the inﬁnite width limit. Below we present a\\nshort and highly simpliﬁed derivation of this eﬀect and show how it leads to provable convergence of gradient\\ndescent for suﬃciently overparametrized deep NNs.\\n\\nA simple learning model: We consider again the simple NN model of (5.1) with a smooth activation\\ni=1 ∈ (Rd × R)m,\\nfunction (cid:37) which is not aﬃne linear. For the quadratic loss and training data s = ((x(i), y(i)))m\\nwhere xi (cid:54)= xj for all i (cid:54)= j, the empirical risk is given by\\n\\nr(θ) = (cid:98)Rs(θ) =\\n\\n1\\nm\\n\\nm\\n(cid:88)\\n\\ni=1\\n\\n(Φ(x(i), θ) − y(i))2.\\n\\nLet us further assume that Θ(1)\\nrandom variables.\\n\\nj ∼ N (0, 1/n)d+1, j ∈ [n], and Θ(2)\\n\\nj ∼ N (0, 1/n), j ∈ [n], are independent\\n\\n41\\n\\n\\x0cFigure 5.3: Four networks with architecture ((1, n, n, 1), (cid:37)R) and n ∈ {20, 100, 500, 2500} neurons per hidden\\nlayer were trained by gradient descent to ﬁt four points that are shown in the middle ﬁgure as black dots.\\nWe depict on the left the relative Euclidean distance of the parameters from the initialization through the\\ntraining process. In the middle, we show the ﬁnal trained NNs. On the right we show the behavior of the\\ntraining error.\\n\\nA peculiar kernel: Next, we would like to understand how the gradient ∇θr(Θ) looks like with high\\nprobability over the initialization Θ = (Θ(1), Θ(2)). Similar to (2.3), we have by restricting the gradient to\\nθ(2) and applying the chain rule that\\n\\n(cid:107)∇θr(Θ)(cid:107)2\\n\\n2 ≥\\n\\n4\\nm2\\n\\n(cid:13)\\n(cid:13)\\n(cid:13)\\n\\nm\\n(cid:88)\\n\\ni=1\\n\\n(cid:13)\\n2\\n∇θ(2)Φ(x(i), Θ)(Φ(x(i), Θ) − y(i))\\n(cid:13)\\n(cid:13)\\n2\\n\\n=\\n\\n4\\nm2\\n\\n(cid:0)(Φ(x(i), Θ) − y(i))m\\n\\ni=1\\n\\n(cid:1)T ¯KΘ(Φ(x(j), Θ) − y(j))m\\n\\nj=1,\\n\\n(5.2)\\n\\nwhere ¯KΘ is a random Rm×m-valued kernel given by\\n( ¯KΘ)i,j := (cid:0)∇θ(2)Φ(x(i), Θ)(cid:1)T\\n\\n∇θ(2)Φ(x(j), Θ),\\n\\ni, j ∈ [m].\\n\\nThis kernel is closely related to the neural tangent kernel in (2.4) evaluated at the features (x(i))m\\ni=1 and the\\nrandom initialization Θ. It is a slightly simpliﬁed version thereof, as in (2.4) the gradient is taken with respect\\nto the full vector θ. This can also be regarded as the kernel associated with a random features model [RR+07].\\n\\nNote that for our two-layer NN we have that\\n\\n(cid:0)∇θ(2)Φ(x, Θ)(cid:1)\\n\\nk = (cid:37)\\n\\n(cid:18)(cid:28)\\n\\nΘ(1)\\nk ,\\n\\n(cid:21)(cid:29)(cid:19)\\n\\n(cid:20)x\\n1\\n\\n,\\n\\nx ∈ Rd, k ∈ [n].\\n\\nThus, we can write ¯KΘ as the following sum of (random) rank one matrices:\\n\\n¯KΘ =\\n\\nn\\n(cid:88)\\n\\nk=1\\n\\nvkvT\\nk\\n\\n(cid:18)\\n\\n(cid:18)(cid:28)\\n\\nwith vk =\\n\\n(cid:37)\\n\\nΘ(1)\\nk ,\\n\\n(cid:20)x(i)\\n1\\n\\n(cid:21)(cid:29)(cid:19)(cid:19)m\\n\\ni=1\\n\\n∈ Rm,\\n\\nk ∈ [n].\\n\\n(5.3)\\n\\nThe kernel ¯KΘ are symmetric and positive semi-deﬁnite by construction.\\nIt is positive deﬁnite if it is\\nnon-singular, i.e., if at least m of the n vectors vk, k ∈ [n], are linearly independent. Proposition 1.1 shows\\nthat for n = m the probability of that event is not zero, say δ, and is therefore at least 1 − (1 − δ)(cid:98)n/m(cid:99) for\\narbitrary n. In other words, the probability increases rapidly with n. It is also clear from (5.3) that E[ ¯KΘ]\\nscales linearly with n.\\n\\nFrom this intuitive derivation, we conclude that for suﬃciently large n, with high probability ¯KΘ is\\na positive deﬁnite kernel with smallest eigenvalue λmin( ¯KΘ) scaling linearly with n. The properties of\\n¯KΘ, in particular its positive deﬁniteness, have been studied much more rigorously as already described in\\nSubsection 2.1.\\n\\n42\\n\\n\\x0cControl of the gradient: Applying the expected behavior of the smallest eigenvalue λmin( ¯KΘ) of ¯KΘ\\nto (5.2), we conclude that with high probability\\n\\n(cid:107)∇θr(Θ)(cid:107)2\\n\\n4\\nm2 λmin( ¯KΘ)(cid:107)(Φ(x(i), Θ) − y(i))m\\nTo understand what will happen when applying gradient descent, we ﬁrst need to understand how the\\nsituation changes in a neighborhood of Θ. We ﬁx x ∈ Rd and observe that by the mean value theorem for all\\n¯θ ∈ B1(0) we have\\n\\n(cid:38) n\\nm\\n\\ni=1(cid:107)2\\n2\\n\\nr(Θ).\\n\\n(5.4)\\n\\n2 ≥\\n\\n(cid:13)∇θΦ(x, Θ) − ∇θΦ(x, Θ + ¯θ)(cid:13)\\n(cid:13)\\n2\\n(cid:13)\\n2\\n\\n(cid:46) sup\\n\\nˆθ∈B1(0)\\n\\n(cid:13)\\n(cid:13)∇2\\n\\nθΦ(x, Θ + ˆθ)(cid:13)\\n2\\nop,\\n(cid:13)\\n\\n(5.5)\\n\\nwhere (cid:107)∇2\\nof (5.1), it is not hard to see that for all i, j ∈ [n] and k, (cid:96) ∈ [d + 1]\\n\\nθΦ(x, Θ + ˆθ)(cid:107)op denotes the operator norm of the Hessian of Φ(x, ·) at Θ + ˆθ. From inspection\\n\\nE\\n\\n(cid:34)\\n\\n(cid:16) ∂2Φ(x, Θ)\\ni ∂θ(2)\\n\\n∂θ(2)\\n\\nj\\n\\n(cid:17)2(cid:35)\\n\\n(cid:34)\\n\\n= 0, E\\n\\n(cid:16) ∂2Φ(x, Θ)\\ni ∂(θ(1)\\n\\n∂θ(2)\\n\\nj )k\\n\\n(cid:17)2(cid:35)\\n\\n(cid:46) δi,j,\\n\\nand E\\n\\n(cid:34)\\n\\n(cid:16)\\n\\n∂2Φ(x, Θ)\\n)k∂(θ(1)\\n\\n∂(θ(1)\\ni\\n\\nj )(cid:96)\\n\\n(cid:17)2(cid:35)\\n\\n(cid:46) δi,j\\nn\\n\\n,\\n\\nwhere δi,j = 0 if i (cid:54)= j and δi,i = 1 for all i, j ∈ [n]. For suﬃciently large n, we have that ∇2\\nθΦ(x, Θ) is\\nin expectation approximately a block band matrix with band-width d + 1. Therefore, we conclude that\\n(cid:3) (cid:46) 1. Hence, we obtain by concentration of Gaussian random variables that with high\\nE(cid:2)(cid:107)∇2\\nθΦ(x, Θ)(cid:107)2\\nop\\nprobability (cid:107)∇2\\nθΦ(x, Θ)(cid:107)2\\nθΦ(x, Θ) we have that, even after perturbation\\nop\\nof Θ by a vector ˆθ with norm bounded by 1, the term (cid:107)∇2\\nop is bounded, which yields that the\\nright-hand side of (5.5) is bounded with high probability.\\n\\n(cid:46) 1. By the block-banded form of ∇2\\n\\nθΦ(x, Θ + ˆθ)(cid:107)2\\n\\nUsing (5.5), we can extend (5.4), which holds with high probability, to a neighborhood of Θ by the\\n\\nfollowing argument: Let ¯θ ∈ B1(0), then\\n\\n(cid:107)∇θr(Θ + ¯θ)(cid:107)2\\n\\n2 ≥\\n\\n4\\nm2\\n\\n(cid:13)\\n(cid:13)\\n(cid:13)\\n\\nm\\n(cid:88)\\n\\n∇θ(2)Φ(x(i), Θ + ¯θ)(Φ(x(i), Θ + ¯θ) − y(i))\\n\\n(cid:13)\\n2\\n(cid:13)\\n(cid:13)\\n2\\n\\n=\\n(5.5)\\n\\n4\\nm2\\n\\ni=1\\nm\\n(cid:13)\\n(cid:88)\\n(cid:13)\\n(cid:13)\\n\\ni=1\\n\\n(∇θ(2)Φ(x(i), Θ) + O(1))(Φ(x(i), Θ + ¯θ) − y(i))\\n\\n(cid:13)\\n2\\n(cid:13)\\n(cid:13)\\n2\\n\\n(5.6)\\n\\n1\\nm2 (λmin( ¯KΘ) + O(1))(cid:107)(Φ(x(i), Θ + ¯θ) − y(i))m\\nr(Θ + ¯θ),\\n\\ni=1(cid:107)2\\n2\\n\\n(cid:38)\\n(∗)\\n\\n(cid:38) n\\nm\\n\\nwhere the estimate marked by (∗) uses the positive deﬁniteness of ¯KΘ again and only holds for suﬃciently\\nlarge n, so that the O(1) term is negligible.\\n\\nWe conclude that, with high probability over the initialization Θ, on a ball of ﬁxed radius around Θ the\\nm times the empirical risk.\\n\\nsquared Euclidean norm of the gradient of the empirical risk is lower bounded by n\\n\\nExponential convergence of gradient descent: For suﬃciently small step sizes η, the observation\\nin the previous paragraph yields the following convergence rate for gradient descent as in Algorithm 1,\\nspeciﬁcally (1.5), with m(cid:48) = m and Θ(0) = Θ: If (cid:107)Θ(k) − Θ(cid:107) ≤ 1 for all k ∈ [K + 1], then25\\n\\nr(Θ(K+1)) ≈ r(Θ(K)) − η(cid:107)∇θr(Θ(K))(cid:107)2\\n\\n2 ≤\\n\\n(cid:16)\\n\\n1 −\\n\\n(cid:17)\\n\\ncηn\\nm\\n\\nr(Θ(K)) (cid:46)\\n\\n(cid:16)\\n\\n1 −\\n\\n(cid:17)K\\n\\n,\\n\\ncηn\\nm\\n\\n(5.7)\\n\\nfor c ∈ (0, ∞) so that (cid:107)∇θr(Θ(k))(cid:107)2\\n\\n2 ≥ cn\\n\\nm r(Θ(k)) for all k ∈ [K].\\n\\n25Note that the step-size η needs to be small enough to facilitate the approximation step in (5.7). Hence, we cannot simply\\n\\nput η = m/(cn) in (5.7) and have convergence after one step.\\n\\n43\\n\\n\\x0cLet us assume without proof that the estimate (5.6) could be extended to an equivalence. In other words,\\nwe assume that we additionally have that (cid:107)∇θr(Θ + ¯θ)(cid:107)2\\nm r(Θ + ¯θ). This, of course, could be shown with\\n2\\nsimilar tools as were used for the lower bound. Then we have that (cid:107)Θ(k) − Θ(cid:107)2 ≤ 1 for all k (cid:46) (cid:112)m/(η2n).\\nSetting t = (cid:112)m/(η2n) and using the limit deﬁnition of the exponential function, i.e., limt→∞(1 − x/t)t = e−x,\\nyields for suﬃciently small η that (5.7) is bounded by e−c\\n\\nn/m.\\n\\n(cid:46) n\\n\\n√\\n\\nWe conclude that, with high probability over the initialization, gradient descent converges with an\\nexponential rate to an arbitrary small empirical risk if the width n is suﬃciently large. In addition, the iterates\\nof the descent algorithm even stay in a small ﬁxed neighborhood of the initialization during training. Because\\nthe parameters only move very little, this type of training has also been coined lazy training [COB19].\\n\\nSimilar ideas as above, have led to groundbreaking convergence results of SGD for overparametrized NNs\\n\\nin much more complex and general settings, see, e.g., [DZPS18, LL18, AZLS19].\\n\\nIn the inﬁnite width limit, NN training is practically equivalent to kernel regression, see Subsection 2.1. If\\nwe look at Figure 5.3 we see that the most overparametrized NN interpolates the data like a kernel-based\\ninterpolator would. In a sense, which was also highlighted in [COB19], this shows that, while overparametrized\\nNNs in the lazy training regime have very nice properties, they essentially act like linear methods.\\n\\n6 Tangible eﬀects of special architectures\\n\\nIn this section, we describe results that isolate the eﬀects of certain aspects of NN architectures. As we have\\ndiscussed in Subsection 1.3, typically only either the depth or the number of parameters are used to study\\ntheoretical aspects of NNs. We have seen instances of this throughout Sections 3 and 4. Moreover, also in\\nSection 5, we saw that wider NNs enjoy certain very favorable properties from an optimization point of view.\\nBelow, we introduce certain specialized NN architectures. We start with one of the most widely used\\ntypes of NNs, the convolutional neural network (CNN). In Subsection 6.2 we introduce skip connections and\\nin Subsection 6.3 we discuss a speciﬁc class of CNNs equipped with an encoder-decoder structure that are\\nfrequently used in image processing techniques. We introduce the batch normalization block in Subsection 6.4.\\nThen, we discuss sparsely connected NNs that typically result as an extraction from fully connected NNs in\\nSubsection 6.5. Finally, we brieﬂy comment on recurrent neural networks in Subsection 6.6.\\n\\nAs we have noted repeatedly throughout this manuscript, it is impossible to give a full account of the\\nliterature in a short introductory article. In this section, this issue is especially severe since the number of\\nspecial architectures studied in practice is enormous. Therefore, we had to omit many very inﬂuential and\\nwidely used neural network architectures. Among those are graph neural networks, which handle data from non-\\nEuclidean input spaces. We refer to the survey articles [BBL+17, WPC+21] for a discussion. Another highly\\nsuccessful type of architectures are (variational) autoencoders [AHS85, HZ94]. These are neural networks\\nwith a bottleneck that enforce a more eﬃcient representation of the data. Similarly, generative adversarial\\nnetworks [GPAM+14] which are composed of two neural networks, one generator and one discriminator,\\ncould not be discussed here. Another widely used component of architectures used in practice is the so-called\\ndropout layer. This layer functions through removing some neurons randomly during training. This procedure\\nempirically prevents overﬁtting. An in-detail discussion of the mathematical analysis behind this eﬀect is\\nbeyond the scope of this manuscript. We refer to [WZZ+13, SHK+14, HV17, MAV18] instead. Finally, the\\nvery successful attention mechanism [BCB15, VSP+17], that is the basis of transformer neural networks, had\\nto be omitted.\\n\\nBefore we start describing certain eﬀects of special NN architectures, a word of warning is required. The\\nspecial building blocks, which will be presented below, have been developed based on a speciﬁc need in\\napplications and are used and combined in a very ﬂexible way. To describe these tools theoretically without\\ncompletely inﬂating the notational load, some simplifying assumptions need to be made. It is very likely that\\nthe simpliﬁed building blocks do not accurately reﬂect the practical applications of these tools in all use cases.\\n\\n44\\n\\n\\x0c6.1 Convolutional neural networks\\n\\nEspecially for very high-dimensional inputs where the input dimensions are spatially related, fully connected\\nNNs seem to require unnecessarily many parameters. For example, in image classiﬁcation problems, neigh-\\nboring pixels very often share information and the spatial proximity should be reﬂected in the architecture.\\nBased on this observation, it appears reasonable to have NNs that have local receptive ﬁelds in the sense\\nthat they collect information jointly from spatially close inputs. In addition, in image processing, we are not\\nnecessarily interested in a universal hypothesis set. A good classiﬁer is invariant under many operations, such\\nas translation or rotation of images. It seems reasonable to hard-code such invariances into the architecture.\\nThese two principles suggest that the receptive ﬁeld of a NN should be the same on diﬀerent translated\\npatches of the input. In this sense, parameters of the architecture can be reused. Together, these arguments\\nmake up the three fundamental principles of convolutional NNs: local receptive ﬁelds, parameter sharing,\\nand equivariant representations, as introduced in [LBD+89]. We will provide a mathematical formulation of\\nconvolutional NNs below and then revisit these concepts.\\n\\nA convolutional NN corresponds to multiple convolutional blocks, which are special types of layers. For\\na group G, which typically is either [d] ∼= Z/(dZ) or [d]2 ∼= (Z/(dZ))2 for d ∈ N, depending on whether we\\nare performing one-dimensional or two-dimensional convolutions, the convolution of two vectors a, b ∈ RG is\\ndeﬁned as\\n\\n(a ∗ b)i =\\n\\najbj−1i,\\n\\ni ∈ G.\\n\\n(cid:88)\\n\\nj∈G\\n\\nNow we can deﬁne a convolutional block as follows: Let (cid:101)G be a subgroup of G, let p : G → (cid:101)G be a so-called\\npooling-operator, and let C ∈ N denote the number of channels. Then, for a series of kernels κi ∈ RG, i ∈ [C],\\nthe output of a convolutional block is given by\\n\\nRG (cid:51) x (cid:55)→ x(cid:48) := (p(x ∗ κi))C\\n\\ni=1 ∈ (R (cid:101)G)C.\\n\\n(6.1)\\n\\nA typical example of a pooling operator is for G = (Z/(2dZ))2 and (cid:101)G = (Z/(dZ))2 the 2 × 2 subsampling\\noperator\\n\\np : RG → R (cid:101)G,\\n\\nx (cid:55)→ (x2i−1,2j−1)d\\n\\ni,j=1.\\n\\nPopular alternatives are average pooling or max pooling. These operations then either pass the average or the\\nmaximum over patches of similar size. The convolutional kernels correspond to the aforementioned receptive\\nﬁelds. They can be thought of as local if they have small supports, i.e., few nonzero entries.\\n\\nAs explained earlier, a convolutional NN is built by stacking multiple convolutional blocks after another26.\\nAt some point, the output can be ﬂattened, i.e., mapped to a vector and is then fed into a FC NN (see\\nDeﬁnition 1.4). We depict this setup in Figure 6.1.\\n\\nOwing to the fact that convolution is a linear operation, depending on the pooling operation, one may\\nwrite a convolutional block (6.1) as a FC NN. For example, if G = (Z/(2dZ))2 and the 2 × 2 subsampling\\npooling operator is used, then the convolutional block could be written as x (cid:55)→ W x for a block circulant\\nmatrix W ∈ R(Cd2)×(2d)2\\n. Since we require W to have a special structure, we can interpret a convolutional\\nblock as a special, restricted feed-forward architecture.\\n\\nAfter these considerations, it is natural to ask how the restriction of a NN to a pure convolutional\\nstructure, i.e., consisting only of convolutional blocks, will aﬀect the resulting hypothesis set. The ﬁrst\\nnatural question is whether the set of such NNs is still universal in the sense of Theorem 1.15. The answer to\\nthis question depends strongly on the type of pooling and convolution that is allowed. If the convolution is\\nperformed with padding, then the answer is yes [OS19, Zho20b]. On the other hand, for circular convolutions\\nand without pooling, universality does not hold, but the set of translation equivariant functions can be\\nuniversally approximated [Yar18b, PV20]. Furthermore, [Yar18b] illuminates the eﬀect of subsample pooling\\nby showing that, if no pooling is applied, then universality cannot be achieved, whereas if pooling is applied\\n\\n26We assume that the deﬁnition of a convolutional block is suitably extended to input data in the Cartesian product (RG)C .\\nFor instance, one can take an aﬃne linear combination of C mappings as in (6.1) acting on each coordinate. Moreover, one may\\nalso interject an activation function between the blocks.\\n\\n45\\n\\n\\x0cConvolution\\n\\nPooling\\n\\nConvolution\\n\\nPooling\\n\\nFully connected NN\\n\\nFigure 6.1: Illustration of a convolutional neural network with two-dimensional convolutional blocks and\\n2 × 2 subsampling as pooling operation.\\n\\nthen universality is possible. The eﬀect of subsampling in CNNs from the viewpoint of approximation theory\\nis further discussed in [Zho20a]. The role of other types of pooling in enhancing invariances of the hypothesis\\nset will be discussed in Subsection 7.1 below.\\n\\n6.2 Residual neural networks\\n\\nLet us ﬁrst illustrate a potential obstacle when training deep NNs. Consider for L ∈ N the product operation\\n\\nIt is clear that\\n\\nRL (cid:51) x (cid:55)→ π(x) =\\n\\nL\\n(cid:89)\\n\\n(cid:96)=1\\n\\nx(cid:96).\\n\\n∂\\n∂xk\\n\\nπ(x) =\\n\\nL\\n(cid:89)\\n\\n(cid:96)(cid:54)=k\\n\\nx(cid:96),\\n\\nx ∈ RL.\\n\\nTherefore, for suﬃciently large L, we expect that (cid:12)\\n(cid:12)\\n(cid:12) ∂π\\n(cid:12) will be exponentially small, if |x(cid:96)| < λ < 1 for all\\n∂xk\\n(cid:96) ∈ [L] or exponentially large, if |x(cid:96)| > λ > 1 for all (cid:96) ∈ [L]. The output of a general NN, considered as a\\ndirected graph, is found by repeatedly multiplying the input with parameters in every layer along the paths\\nthat lead from the input to the output neuron. Due to the aforementioned phenomenon, it is often observed\\nthat training NNs suﬀers from either the exploding or the vanishing gradient problem, which may prevent\\nlower layers from training at all. The presence of an activation function is likely to exacerbate this eﬀect. The\\nexploding or vanishing gradient problem seems to be a serious obstacle towards eﬃcient training of deep NNs.\\nIn addition to the vanishing and exploding gradient problems, there is an empirically observed degradation\\nproblem [HZRS16]. This phrase describes the phenomenon that FC NNs seem to achieve lower accuracy on\\nboth the training and test data when increasing their depth.\\n\\nFrom an approximation theoretic perspective, deep NNs should always be superior to shallow NNs. The\\nreason for this is that NNs with two layers can either exactly represent the identity map or approximate it\\narbitrarily well. Concretely, for the ReLU activation function (cid:37)R we have that x = (cid:37)R(x + b) − b for x ∈ Rd\\nwith xi > −bi, where b ∈ Rd. In addition, for any activation function (cid:37) which is continuously diﬀerentiable on\\na neighborhood of some point λ ∈ R with (cid:37)(cid:48)(λ) (cid:54)= 0 one can approximate the identity arbitrary well, see (1.8).\\nBecause of this, extending a NN architecture by one layer can only enlarge the associated hypothesis set.\\n\\nTherefore, one may expect that the degradation problem is more associated with the optimization aspect\\nof learning. This problem is addressed by a small change to the architecture of a feed-forward NN in [HZRS16].\\n\\n46\\n\\n\\x0cidR3\\n\\nidR3\\n\\nidR3\\n\\nidR3\\n\\nFigure 6.2: Illustration of a neural network with residual blocks.\\n\\nInstead of deﬁning a FC NN Φ as in (1.1), one can insert a residual block in the (cid:96)-th layer by redeﬁning27\\n¯Φ((cid:96))(x, θ) = (cid:37)(Φ((cid:96))(x, θ)) + ¯Φ((cid:96)−1)(x, θ),\\n\\n(6.2)\\n\\nwhere we assume that N(cid:96) = N(cid:96)−1. Such a block can be viewed as the sum of a regular FC NN and the\\nidentity which is referred to as skip connection or residual connection. A sketch of a NN with residual blocks\\nis shown in Figure 6.2. Inserting a residual block in all layers leads to a so-called residual NN.\\n\\nA prominent approach to analyze residual NNs is by establishing a connection with optimal control\\nproblems and dynamical systems [E17, TvG18, EHL19, LLS19, RH19, LML+20]. Concretely, if each layer of\\na NN Φ is of the form (6.2), then we have that\\n\\n¯Φ((cid:96)) − ¯Φ((cid:96)−1) = (cid:37)(Φ((cid:96))) =: h((cid:96), Φ((cid:96))),\\nwhere we abbreviate ¯Φ((cid:96)) = ¯Φ((cid:96))(x, θ) and set ¯Φ(0) = x. Hence, ( ¯Φ((cid:96)))L−1\\nof the ODE\\n\\n˙φ(t) = h(t, φ(t)),\\n\\nφ(0) = x,\\n\\n(cid:96)=0 corresponds to an Euler discretization\\n\\nwhere t ∈ [0, L − 1] and h is an appropriate function.\\n\\nUsing this relationship, deep residual NNs can be studied in the framework of the well-established theory\\n\\nof dynamical systems, where strong mathematical guarantees can be derived.\\n\\n6.3 Framelets and U-Nets\\n\\nOne of the most prominent application areas of deep NNs are inverse problems, particularly those in the ﬁeld\\nof imaging science, see also Subsection 8.1. A speciﬁc architectural design of CNNs, namely so-called U-nets\\nintroduced in [RFB15], seems to perform best for this range of problems. We depict a sketch of a U-net in\\nFigure 6.3. However, a theoretical understanding of the success of this architecture was lacking.\\n\\nRecently, an innovative approach called deep convolutional framelets was suggested in [YHC18], which we\\nnow brieﬂy explain. The core idea is to take a frame-theoretic viewpoint, see, e.g., [CKP12], and regard the\\nforward pass of a CNN as a decomposition in terms of a frame (in the sense of a generalized basis). A similar\\napproach will be taken in Subsection 7.2 for understanding the learned kernels using sparse coding. However,\\nbased on the analysis and synthesis operators of the corresponding frame, the usage of deep convolutional\\nframelets naturally leads to a theoretical understanding of encoder-decoder architectures, such as U-nets.\\n\\nLet us describe this approach for one-dimensional convolutions on the group G := Z/(dZ) with kernels\\ndeﬁned on the subgroup H := Z/(nZ), where d, n ∈ N with n < d, see also Subsection 6.1. We deﬁne\\nthe convolution between u ∈ RG and v ∈ RH by zero-padding v, i.e., g ∗◦ v := g ∗ ¯v, where ¯v ∈ RG is\\ndeﬁned by ¯vi = vi for i ∈ H and ¯vi = 0 else. As an important tool, we consider the Hankel matrix\\nHn(x) = (xi+j)i∈G,j∈H ∈ Rd×n associated with x ∈ RG. As one key property, matrix-vector multiplications\\nwith Hankel matrices are translated to convolutions via28\\n\\n(cid:104)e(i), Hn(x)v(cid:105) =\\n\\n(cid:88)\\n\\nj∈H\\n\\nxi+jvj = (cid:104)x, e(i) ∗◦ v(cid:105),\\n\\ni ∈ G,\\n\\n(6.3)\\n\\n27One can also skip multiple layers, e.g., in [HZRS16] two or three layers skipped, use a simple transformation instead of the\\n\\nidentity [SGS15], or randomly drop layers [HSL+16].\\n\\n28Here and in the following we naturally identify elements in RG and RH with the corresponding vectors in Rd and Rn.\\n\\n47\\n\\n\\x0cFigure 6.3: Illustration of a simpliﬁed U-net neural network. Down-arrows stand for pooling, up arrows for\\ndeconvolution or upsampling, right arrows for convolution or fully connected steps. Dashed lines are skip\\nconnections.\\n\\nwhere e(i) := 1{i} ∈ RG and v ∈ RH , see [YGLD17]. Further, we can recover the k-th coordinate of x by the\\nFrobenius inner product between Hn(x) and the Hankel matrix associated with e(k), i.e.,\\n\\nTr(cid:0)Hn(e(k))T Hn(x)(cid:1) =\\n\\n1\\nn\\n\\n1\\nn\\n\\n(cid:88)\\n\\n(cid:88)\\n\\nj∈H\\n\\ni∈G\\n\\ne(k)\\ni+jxi+j =\\n\\n1\\nn\\n\\n|H|xk = xk.\\n\\n(6.4)\\n\\nThis allows us to construct global and local bases as follows: Let p, q ∈ N, let U = (cid:2)u1 · · · up\\nV = (cid:2)v1 · · · vq\\n\\n(cid:3) ∈ Rn×q, and assume that\\n\\n(cid:3) ∈ Rd×p, and (cid:101)V = (cid:2)˜v1 · · · ˜vq\\n\\n(cid:3) ∈ Rn×q, (cid:101)U = (cid:2)˜u1 · · · ˜up\\n\\nHn(x) = (cid:101)U U T Hn(x)V (cid:101)V T .\\n\\n(cid:3) ∈ Rd×p,\\n\\n(6.5)\\n\\nFor p ≥ d and q ≥ n, this is, for instance, satisﬁed if U and V constitute frames with (cid:101)U and (cid:101)V being their\\nrespective dual frames, i.e., (cid:101)U U T = Id and V (cid:101)V T = In. As a special case, one can consider orthonormal bases\\nU = (cid:101)U and V = (cid:101)V with p = d and q = n. In the case p = q = r ≤ n, where r is the rank of Hn(x), one can\\nestablish (6.5) by choosing the left and right singular vectors of Hn(x) as U = (cid:101)U and V = (cid:101)V , respectively.\\nThe identity in (6.5), in turn, ensures the following decomposition:\\n\\nx =\\n\\n1\\nn\\n\\np\\n(cid:88)\\n\\nq\\n(cid:88)\\n\\n(cid:104)x, ui ∗◦ vj(cid:105)˜ui ∗◦ ˜vj.\\n\\ni=1\\n\\nj=1\\n\\n(6.6)\\n\\nObserving that the vector vj ∈ RH interacts locally with x ∈ RG due to the fact that H ⊂ G, whereas\\nui ∈ RG acts on the entire vector x, we refer to (vj)q\\ni=1 as global bases. In the context of\\nCNNs, vi can be interpreted as local convolutional kernel and ui as pooling operation29. The proof of (6.6)\\n\\nj=1 as local and (ui)p\\n\\n29Note that (cid:104)x, ui ∗◦ vj (cid:105) can also be interpreted as (cid:104)ui, vj (cid:63) x(cid:105), where (cid:63) denotes the cross-correlation between the zero-padded\\nvj and x. This is in line with software implementations for deep learning applications, e.g., TensorFlow and PyTorch, where\\ntypically cross-correlations are used instead of convolutions.\\n\\n48\\n\\n\\x0cfollows directly from properties (6.3), (6.4), and (6.5) as\\n\\nxk =\\n\\n1\\nn\\n\\nTr(cid:0)Hn(e(k))T Hn(x)(cid:1) =\\n\\n1\\nn\\n\\nTr(cid:0)Hn(e(k))T (cid:101)U U T Hn(x)V (cid:101)V T (cid:1) =\\n\\n1\\nn\\n\\np\\n(cid:88)\\n\\nq\\n(cid:88)\\n\\ni=1\\n\\nj=1\\n\\n(cid:104)ui, Hn(x)vj(cid:105)(cid:104)˜ui, Hn(e(k))˜vj(cid:105).\\n\\nThe decomposition in (6.6) can now be interpreted as a composition of an encoder and a decoder,\\n\\nx (cid:55)→ C = ((cid:104)x, ui ∗◦ vj(cid:105))i∈[p],j∈[q]\\n\\nand\\n\\nC (cid:55)→\\n\\n1\\nn\\n\\np\\n(cid:88)\\n\\nq\\n(cid:88)\\n\\ni=1\\n\\nj=1\\n\\nCi,j ˜ui ∗◦ ˜vj,\\n\\n(6.7)\\n\\nwhich relates it to CNNs equipped with an encoder-decoder structure such as U-nets, see Figure 6.3.\\nGeneralizing this approach to multiple channels, it is possible to stack such encoders and decoders which\\nleads to a layered version of (6.6). In [YHC18] it is shown that one can make an informed decision on the\\nnumber of layers based on the rank of Hn(x), i.e., the complexity of the input features x. Moreover, also an\\nactivation function such as the ReLU or bias vectors can be included. The key question one can then ask is\\nhow the kernels can be chosen to obtain sparse coeﬃcients C in (6.7) and a decomposition such as (6.6), i.e.,\\nperfect reconstruction. If U and V are chosen as the left and right singular vectors of Hn(x), one obtains a\\nvery sparse, however input-dependent, representation in (6.6) due to the fact that\\n\\nCi,j = (cid:104)x, ui ∗◦ vj(cid:105) = (cid:104)ui, Hn(x)vj(cid:105) = 0,\\n\\ni (cid:54)= j.\\n\\nFinally, using the framework of deep convolutional framelets, theoretical reasons for including skip connections\\ncan be derived, since they aid to obtain a perfect reconstruction.\\n\\n6.4 Batch normalization\\n\\nBatch normalization is a building block of NNs that was invented in [IS15] with the goal to reduce so-called\\ninternal covariance shift. In essence, this phrase describes the (undesirable) situation where during training\\neach layer receives inputs with diﬀerent distribution. A batch normalization block is deﬁned as follows: For\\npoints b = (y(i))m\\n\\ni=1 ∈ (Rn)m and β, γ ∈ R, we deﬁne\\n\\nBN(β,γ)\\nb\\n\\n(y) := γ\\n\\ny − µb\\nσb\\n\\n+ β,\\n\\ny ∈ Rn, with µb =\\n\\n1\\nm\\n\\nm\\n(cid:88)\\n\\ni=1\\n\\ny(i)\\n\\nand σ2\\n\\nb =\\n\\n1\\nm\\n\\nm\\n(cid:88)\\n\\n(y(i) − µb)2,\\n\\ni=1\\n\\n(6.8)\\n\\nwhere all operations are to be understood componentwise, see Figure 6.4.\\n\\nSuch a batch normalization block can be added into a NN architecture. Then b is the output of the\\nprevious layer over a batch or the whole training data30. Furthermore, the parameters β, γ are variable and\\ncan be learned during training. Note that, if one sets β = µb and γ = σb, then BN(β,γ)\\n(y) = y for all y ∈ Rn.\\nTherefore, a batch normalization block does not negatively aﬀect the expressivity of the architecture. On the\\nother hand, batch normalization does have a tangible eﬀect on the optimization aspects of deep learning.\\nIndeed, in [STIM18, Theorem 4.1], the following observation was made:\\n\\nb\\n\\n30In practice, one typically uses a moving average to estimate the mean µ and the standard deviation σ of the output of the\\n\\nprevious layer over the whole training data by only using batches.\\n\\n49\\n\\n\\x0cµb σb\\n\\n(cid:98)y = y−µb\\nσb\\nz = γ(cid:98)y + β\\n\\nβ\\n\\nγ\\n\\nFigure 6.4: A batch normalization block after a fully connected neural network. The parameters µb, σb are\\nthe mean and the standard deviation of the output of the fully connected network computed over a batch s,\\ni.e., a set of inputs. The parameters β, γ are learnable parts of the batch normalization block.\\n\\nProposition 6.1 (Smoothening eﬀect of batch normalization). Let m ∈ N with m ≥ 2 and for every β, γ ∈ R\\ndeﬁne B(β,γ) : Rm → Rm by\\n\\nB(β,γ)(b) = (BN(β,γ)\\n\\nb\\n\\n(y(1)), . . . , BN(β,γ)\\n\\nb\\n\\n(y(m))),\\n\\nb = (y(i))m\\n\\ni=1 ∈ Rm,\\n\\nwhere BN(β,γ)\\nholds for every b ∈ Rm that\\n\\nb\\n\\nis given as in (6.8). Let β, γ ∈ R and let r : Rm → R be a diﬀerentiable function. Then it\\n\\n(cid:107)∇(r ◦ B(β,γ))(b)(cid:107)2\\n\\n2 =\\n\\nγ2\\nσ2\\nb\\n\\n(cid:0) (cid:107)∇r(b)(cid:107)2 −\\n\\n1\\nm\\n\\n(cid:104)1, ∇r(b)(cid:105)2 −\\n\\n1\\nm\\n\\n(cid:104)B(0,1)(b), ∇r(b)(cid:105)2(cid:1),\\n\\nwhere 1 = (1, . . . , 1) ∈ Rm and σ2\\n\\nb is given as in (6.8).\\n\\nFor multi-dimensional y(i) ∈ Rn, i ∈ [m], the same statement holds for all components as, by deﬁnition,\\nthe batch normalization block acts componentwise. Proposition 6.1 follows from a convenient representation\\nof the Jacobian of the mapping B(β,γ), given by\\n\\n∂B(β,γ)(b)\\n∂b\\n\\n=\\n\\n(cid:16)\\n\\nγ\\nσb\\n\\nIm −\\n\\n1\\nm\\n\\n11T −\\n\\n1\\nm\\n\\nB(0,1)(b)(B(0,1)(b))T (cid:17)\\n\\n,\\n\\nb ∈ Rm,\\n\\nand the fact that { 1√\\n\\nm , 1√\\n\\nm B(0,1)(b)} constitutes an orthonormal set.\\n\\nChoosing r to mimic the empirical risk of a learning task, Proposition 6.1 shows that, in certain situations—\\nfor instance, if γ is smaller than σb or if m is not too large—a batch normalization block can considerably\\nreduce the magnitude of the derivative of the empirical risk with respect to the input of the batch normalization\\nblock. By the chain rule, this implies that also the derivative of the empirical risk with respect to NN\\nparameters inﬂuencing the input of the batch normalization block is reduced.\\n\\nInterestingly, a similar result holds for second derivatives [STIM18, Theorem 4.2] if r is twice diﬀerentiable.\\nOne can conclude that adding a batch normalization block increases the smoothness of the optimization\\nproblem. Since the parameters β and γ were introduced, including a batch normalization block also increases\\nthe dimension of the optimization problem by two.\\n\\n6.5 Sparse neural networks and pruning\\n\\nFor deep FC NNs, the number of trainable parameters usually scales like the square of the number of neurons.\\nFor reasons of computational complexity and memory eﬃciency, it appears sensible to seek for techniques to\\nreduce the number of parameters or extract sparse subnetworks (see Figure 6.5) without aﬀecting the output\\n\\n50\\n\\n\\x0cof a NN much. One way to do this is by pruning [LDS89, HMD16].\\nHere, certain parameters of a NN are removed after training. This\\nis done, for example, by setting these parameters to zero.\\n\\nIn this context, the lottery ticket hypothesis was formulated\\nin [FC18]. It states: “A randomly-initialized, dense NN contains a\\nsubnetwork that is initialized such that—when trained in isolation—it\\ncan match the test accuracy of the original NN after training for at\\nmost the same number of iterations”. In [RWK+20] a similar hy-\\npothesis was made and empirically studied. There, it is claimed that,\\nfor a suﬃciently overparametrized NN, there exists a subnetwork\\nthat matches the performance of the large NN after training without\\nbeing trained itself, i.e., already at initialization.\\n\\nFigure 6.5: A neural network with\\nsparse connections.\\n\\nUnder certain simplifying assumptions, the existence of favorable subnetworks is quite easy to prove. We\\ncan use a technique that was previously indirectly used in Subsection 4.2—the Carath´eodory Lemma. This\\nresult states the following: Let n ∈ N, C ∈ (0, ∞), and let (H, (cid:107) · (cid:107)) be a Hilbert space. Let F ⊂ H with\\nsupf ∈F (cid:107)f (cid:107) ≤ C and let g ∈ H be in the convex hull of F . Then there exist fi ∈ F , i ∈ [n], and c ∈ [0, 1]n\\nwith (cid:107)c(cid:107)1 = 1 such that\\n\\n(cid:13)\\n(cid:13)\\n(cid:13)\\n(cid:13)\\n(cid:13)\\n\\ng −\\n\\nn\\n(cid:88)\\n\\ni=1\\n\\ncifi\\n\\n(cid:13)\\n(cid:13)\\n(cid:13)\\n(cid:13)\\n(cid:13)\\n\\n≤\\n\\nC\\n√\\nn\\n\\n,\\n\\nsee, e.g., [Ver18, Theorem 0.0.2].\\n\\nProposition 6.2 (Carath´eodory pruning). Let d, n ∈ N, with n ≥ 100 and let µ be a probability measure\\non the unit ball B1(0) ⊂ Rd. Let a = ((d, n, 1), (cid:37)R) be the architecture of a two-layer ReLU network and let\\nθ ∈ RP ((d,n,1)) be corresponding parameters such that\\n\\nΦa(·, θ) =\\n\\nn\\n(cid:88)\\n\\ni=1\\n\\ni (cid:37)R((cid:104)(w(1)\\nw(2)\\n\\ni\\n\\n, ·(cid:105) + b(1)\\n\\ni\\n\\n)),\\n\\n, b(1)\\ni\\n\\n) ∈ Rd × R, i ∈ [n], and w(2) ∈ Rn. Assume that for every i ∈ [n] it holds that (cid:107)w(1)\\ni (cid:107)2 ≤ 1/2\\ni ≤ 1/2. Then there exists a parameter ˜θ ∈ RP ((d,n,1)) with at least 99% of its entries being zero such\\n\\ni\\n\\nwhere (w(1)\\nand b(1)\\nthat\\n\\n(cid:107)Φa(·, θ) − Φa(·, ˜θ)(cid:107)L2(µ) ≤\\n\\n15(cid:107)w(2)(cid:107)1√\\nn\\n\\n.\\n\\nSpeciﬁcally, there exists an index set I ⊂ [n] with |I| ≤ n/100 such that ˜θ satisﬁes that\\n\\n(cid:101)w(2)\\ni = 0,\\n\\nif i /∈ I,\\n\\nand\\n\\n( (cid:101)w(1)\\n\\ni\\n\\n, ˜b(1)\\ni\\n\\n) =\\n\\n(cid:40)\\n\\n, b(1)\\ni\\n\\n(w(1)\\ni\\n(0, 0),\\n\\n),\\n\\nif i ∈ I,\\nif i /∈ I.\\n\\nThe result is clear if w(2) = 0. Otherwise, deﬁne\\n\\nfi := (cid:107)w(2)(cid:107)1(cid:37)R((cid:104)w(1)\\n\\ni\\n\\nand observe that Φa(·, θ) is in the convex hull of {fi}n\\ninequality, it holds that\\n\\n, ·(cid:105) + b(1)\\n\\n),\\n\\ni\\ni=1 ∪ {−fi}n\\n\\ni ∈ [n],\\n\\ni=1. Moreover, by the Cauchy–Schwarz\\n\\n(cid:107)fi(cid:107)L2(µ) ≤ (cid:107)w(2)(cid:107)1(cid:107)fi(cid:107)L∞(B1(0)) ≤ (cid:107)w(2)(cid:107)1.\\nWe conclude with the Carath´eodory Lemma that there exists I ⊂ [n] with |I| = (cid:98)n/100(cid:99) ≥ n/200 and\\nci ∈ [−1, 1], i ∈ I, such that\\n(cid:13)\\n(cid:13)\\n(cid:13)\\n(cid:13)\\n(cid:13)\\n\\n200(cid:107)w(2)(cid:107)1\\nn\\n\\n(cid:107)w(2)(cid:107)1\\n(cid:112)|I|\\n\\nΦa(·, θ) −\\n\\ncifi\\n\\n(cid:88)\\n\\n√\\n\\n√\\n\\n≤\\n\\n≤\\n\\n,\\n\\ni∈I\\n\\n(cid:13)\\n(cid:13)\\n(cid:13)\\n(cid:13)\\n(cid:13)L2(µ)\\n\\n51\\n\\n\\x0cwhich yields the result.\\n\\nProposition 6.2 shows that certain, very wide NNs can be approximated very well by sparse subnetworks\\nwhere only the output weight matrix needs to be changed. The argument of Proposition 6.2 is inspired\\nby [BK18], where a much more reﬁned result is shown for deep NNs.\\n\\n6.6 Recurrent neural networks\\n\\nRecurrent NNs are NNs where the underlying graph is allowed to\\nexhibit cycles as in Figure 6.6, see [Hop82, RHW86, Elm90, Jor90].\\nPreviously, we had excluded cyclic computational graphs. For a feed-\\nforward NN, the computation of internal states is naturally performed\\nstep by step through the layers. Since the output of a layer does\\nnot aﬀect previous layers, the order in which the computations of\\nthe NN are performed corresponds to the order of the layers. For\\nrecurrent NNs, the concept of layers does not exist, and the order\\nof operations is much more delicate. Therefore, one considers time\\nsteps. In each time step, all possible computations of the graph are\\napplied to the current state of the NN. This yields a new internal\\nstate. Given that time steps arise naturally from the deﬁnition of\\nrecurrent NNs, this NN type is typically used for sequential data.\\n\\nFigure 6.6: Sketch of a recurrent neu-\\nral network. Cycles in the computa-\\ntional graph incorporate the sequen-\\ntial structure of the input and output.\\n\\nIf the input to a recurrent NN is a sequence, then every input determines the internal state of the recurrent\\nNN for the following inputs. Therefore, one can claim that these NNs exhibit a memory. This fact is extremely\\ndesirable in natural language processing, which is why recurrent NNs are widely used in this application.\\n\\nRecurrent NNs can be trained similarly to regular feed-forward NNs by an algorithm called backpropagation\\nthrough time [MP69, Wer88, WZ95]. This procedure essentially unfolds the recurrent structure yielding a\\nclassical NN structure. However, the algorithm may lead to very deep structures. Due to the vanishing\\nand exploding gradient problem discussed earlier, very deep NNs are often hard to train. Because of this,\\nspecial recurrent structures were introduced that include gates which prohibit too many recurrent steps; these\\ninclude the widely used LSTMs [HS97].\\n\\nThe application area of recurrent NNs is typically quite diﬀerent from that of regular NNs since they\\nare specialized on sequential data. Therefore, it is hard to quantify the eﬀect of a recurrent connection on\\na fully connected NN. However, it is certainly true that with recurrent connections certain computations\\ncan be performed much more eﬃciently than with feed-forward NN structures. A particularly interesting\\nconstruction can be found in [BF19, Theorem 4.4], where it is shown that a ﬁxed size, recurrent NN with\\nReLU activation function, can approximate the function x (cid:55)→ x2 to any desired accuracy. The reason for\\nthis eﬃcient representation can be seen when considering the self-referential deﬁnition of the approximant to\\nx − x2 shown in Figure 3.2.\\n\\nOn the other hand, with feed-forward NNs, it transpires from Theorem 3.3 that the approximation error\\n\\nof ﬁxed-sized ReLU NNs for any non-aﬃne function is greater than a positive lower bound.\\n\\n7 Describing the features a deep neural network learns\\n\\nThis section presents two viewpoints which help in understanding the nature of features that can be described\\nby NNs. Section 7.1 summarizes aspects of the so-called scattering transform which constitutes a speciﬁc NN\\narchitecture that can be shown to satisfy desirable properties, such as translation and deformation invariance.\\nSection 7.2 relates NN features to the current paradigm of sparse coding.\\n\\n7.1 Invariances and the scattering transform\\n\\nOne of the ﬁrst theoretical contributions to the understanding of the mathematical properties of CNNs\\nis [Mal12]. The approach taken in that work is to consider speciﬁc CNN architectures with ﬁxed parameters\\n\\n52\\n\\n\\x0cthat result in a stand-alone feature descriptor whose output may be fed into a subsequent classiﬁer (for example,\\na kernel support vector machine or a trainable FC NN). From an abstract point of view, a feature descriptor\\nis a function Ψ mapping from a signal space, such as L2(Rd) or the space of piecewise smooth functions, to\\na feature space. In an ideal world, such a classiﬁer should “factor” out invariances that are irrelevant to a\\nsubsequent classiﬁcation problem while preserving all other information of the signal. A very simple example\\nof a classiﬁer which is invariant under translations is the Fourier modulus Ψ : L2(Rd) → L2(Rd), u (cid:55)→ |ˆu|.\\nThis follows from the fact that a translation of a signal u results in a modulation of its Fourier transform, i.e.,\\n(cid:92)u(· − τ )(ω) = e−2πi(cid:104)τ,ω(cid:105) ˆu(ω), τ, ω ∈ Rd. Furthermore, in most cases (for example, if u is a generic compactly\\nsupported function [GKR20]), u can be reconstructed up to a translation from its Fourier modulus [GKR20]\\nand an energy conservation property of the form (cid:107)Ψ(u)(cid:107)L2 = (cid:107)u(cid:107)L2 holds true. Translation invariance is,\\nfor example, typically exhibited by image classiﬁers, where the label of an image does not change if it is\\ntranslated.\\n\\nIn practical problems many more invariances arise. Providing an analogous representation that factors\\nout general invariances would lead to a signiﬁcant reduction in the problem dimensionality and constitutes an\\nextremely promising route towards dealing with the very high dimensionality that is commonly encountered\\nin practical problems [Mal16]. This program is carried out in [Mal12] for additional invariances with respect\\nto deformations u (cid:55)→ uτ := u(· − τ (·)), where τ : Rd → Rd is a smooth mapping. Such transformations may\\noccur in practice, for instance, as image warpings. In particular, a feature descriptor Ψ is designed that, with\\na suitable norm (cid:107) · (cid:107) on the image of Ψ,\\n\\n(a) is Lipschitz continuous with respect to deformations in the sense that (cid:107)Ψ(u) − Ψ(uτ )(cid:107) (cid:46) K(τ, ∇τ, ∇2τ )\\nholds for some K that only mildly depends on τ and essentially grows linearly in ∇τ and ∇2τ ,\\n\\n(b) is almost (i.e., up to a small and controllable error) invariant under translations of the input data, and\\n\\n(c) contains all relevant information on the input data in the sense that an energy conservation property\\n\\nholds true.\\n\\n(cid:107)Ψ(u)(cid:107) ≈ (cid:107)u(cid:107)L2\\n\\nObserve that, while the action of translations only represents a d-parameter group, the action of deforma-\\ntions/warpings represents an inﬁnite-dimensional group. Hence, a deformation invariant feature descriptor\\nrepresents a big potential for dimensionality reduction. Roughly speaking, the feature descriptor Ψ of [Mal12]\\n(also coined the scattering transform) is deﬁned by collecting features that are computed by iteratively\\napplying a wavelet transform followed by a pointwise modulus non-linearity and a subsequent low-pass\\nﬁltering step, i.e.,\\n\\n|||u ∗ ψj1| ∗ ψj2 ∗ . . . | ∗ ψj(cid:96)| ∗ ϕJ ,\\n\\nwhere ψj refers to a wavelet at scale j and ϕJ refers to a scaling function at scale J. The collection of all\\nthese so-called scattering coeﬃcients can then be shown to satisfy the properties in (a)–(c) above in a suitable\\n(asymptotic) sense. The proof of this result relies on a subtle interplay between a “deformation covariance”\\nproperty of the wavelet transform and a “regularizing” property of the operation of convolution with the\\nmodulus of a wavelet. We remark that similar results can be shown also for diﬀerent systems, such as Gabor\\nframes [WGB17, CL19].\\n\\n7.2 Hierarchical sparse representations\\n\\nThe previous approach modeled the learned features by a speciﬁc dictionary, namely wavelets. It is well known\\nthat one of the striking properties of wavelets is to provide sparse representations for functions belonging to\\ncertain function classes. More generally, we speak of sparse representations with respect to a representation\\nsystem. For a vector x ∈ Rd, a sparsifying representation system D ∈ Rd×p—also called dictionary—is such\\nthat x = Dφ with the coeﬃcients φ ∈ Rp being sparse in the sense that (cid:107)φ(cid:107)0 := | supp(φ)| = |{i ∈ [p] : φi (cid:54)= 0}|\\nis small compared to p. A similar deﬁnition can be made for signals in inﬁnite-dimensional spaces. Taking\\n\\n53\\n\\n\\x0csparse representations into account, the theory of sparse coding provides an approach to a theoretical\\nunderstanding of the features a deep NN learns.\\n\\nOne common method in image processing is the utilization of not the entire image but overlapping patches\\nof it, coined patch-based image processing. Thus of particular interest are local dictionaries which sparsify\\nthose patches, but presumably not the global image. This led to the introduction of the convolutional sparse\\ncoding model (CSC model), which links such local and global behaviors. Let us describe this model for\\none-dimensional convolutions on the group G := Z/(dZ) with kernels supported on the subgroup H := Z/(nZ),\\nwhere d, n ∈ N with n < d, see also Subsection 6.1. The corresponding CSC model is based on a decomposition\\nof a global signal x ∈ (RG)c with c ∈ N channels as\\n\\nxi =\\n\\nC\\n(cid:88)\\n\\nj=1\\n\\nκi,j ∗ φj,\\n\\ni ∈ [c],\\n\\n(7.1)\\n\\nwhere φ ∈ (RG)C is supposed to be a sparse representation with C ∈ N channels and κi,j ∈ RG, i ∈ [c],\\nj ∈ [C], are local kernels with supp(κi,j) ⊂ H. Let us consider a patch ((xi)g+h)i∈[c],h∈H of n adjacent entries,\\nstarting at position g ∈ G, in each channel of x. The condition on the support of the kernels κi,j and the\\nrepresentation in (7.1) imply that this patch is only aﬀected by a stripe of at most (2n − 1) entries in each\\nchannel of φ. The local, patch-based sparsity of the representation φ can thus be appropriately measured via\\n\\n(cid:107)φ(cid:107)(n)\\n\\n0,∞ := max\\ng∈G\\n\\n(cid:107)((φj)g+k)j∈[C],k∈[2n−1](cid:107)0,\\n\\nsee [PSE17]. Furthermore, note that we can naturally identify x and φ with vectors in Rdc and RdC and write\\nx = Dφ, where D ∈ Rdc×dC is a matrix consisting of circulant blocks, typically referred to as a convolutional\\ndictionary.\\n\\nThe relation between the CSC model and deep NNs is revealed by applying the CSC model in a layer-wise\\nfashion [PRE17, SPRE18, PRSE18]. To see this, let C0 ∈ N and for every (cid:96) ∈ [L] let C(cid:96), k(cid:96) ∈ N and let\\nD((cid:96)) ∈ RdC(cid:96)−1×dC(cid:96) be a convolutional dictionary with kernels supported on Z/(n(cid:96)Z). A signal x = φ(0) ∈ RdC0\\nis said to belong to the corresponding multi-layered CSC model (ML-CSC model) if there exist coeﬃcients\\nφ((cid:96)) ∈ RdC(cid:96) with\\n\\nφ((cid:96)−1) = D((cid:96))φ((cid:96))\\n\\nand (cid:107)φ((cid:96))(cid:107)(n(cid:96))\\n\\n0,∞ ≤ k(cid:96),\\n\\n(cid:96) ∈ [L].\\n\\n(7.2)\\n\\nWe now consider the problem of reconstructing the sparse coeﬃcients (φ((cid:96)))L\\n(cid:96)=1 from a noisy signal ˜x := x + ν,\\nwhere the noise ν ∈ RdC0 is assumed to have small (cid:96)2-norm and x is assumed to follow the ML-CSC model\\nin (7.2). In general, this problem is NP-hard. However, under suitable conditions on the ML-CSC model, it\\ncan be approximately solved, for instance, by a layered thresholding algorithm.\\n\\nMore precisely, for D ∈ Rdc×dC and b ∈ RdC, we deﬁne a soft-thresholding operator by\\n\\nTD,b(x) := (cid:37)R(DT x − b) − (cid:37)R(−DT x − b),\\n\\nx ∈ Rdc,\\n\\n(7.3)\\n\\nwhere (cid:37)R(x) = max{0, x} is applied componentwise. If x = Dφ as in (7.1), we obtain φ ≈ TD,b(x) roughly\\nunder the following conditions: The distance of φ and ψ := DT x = DT Dφ can be bounded using the local\\nsparsity of φ and the mutual coherence and locality of the kernels of the convolutional dictionary D. For a\\nsuitable threshold b, the mapping ψ (cid:55)→ (cid:37)R(ψ − b) − (cid:37)R(−ψ − b) further recovers the support of φ by nullifying\\nentries of ψ with ψi ≤ |bi|. Utilizing the soft-thresholding operator (7.3) iteratively for corresponding vectors\\nb((cid:96)) ∈ RdC(cid:96), (cid:96) ∈ [L], then suggests the following approximations:\\n\\nφ((cid:96)) ≈ (TD((cid:96)),b((cid:96)) ◦ · · · ◦ TD(1),b(1))(˜x),\\n\\n(cid:96) ∈ [L].\\n\\nThe resemblance with the realization of a CNN with ReLU activation function is evident. The transposed\\ndictionary (D((cid:96)))T can be regarded as modeling the learned convolutional kernels, the threshold b((cid:96)) models\\nthe bias vector, and the soft-thresholding operator TD((cid:96)),b((cid:96)) mimics the application of a convolutional block\\nwith a ReLU non-linearity in the (cid:96)-th layer.\\n\\n54\\n\\n\\x0cUsing this model, a theoretical understanding of CNNs from the perspective of sparse coding is now at\\nhand. This novel perspective gives a precise mathematical meaning of the kernels in a CNN as sparsifying\\ndictionaries of an ML-CSC model. Moreover, the forward pass of a CNN can be understood as a layered\\nthresholding algorithm for decomposing a noisy signal ˜x. The results derived are then of the following ﬂavor:\\nGiven a suitable reconstruction procedure such as thresholding or (cid:96)1-minimization, the sparse coeﬃcients\\n(φ((cid:96)))L\\n(cid:96)=1 of a signal x following a ML-CSC model can be stably recovered from the noisy signal ˜x under\\ncertain hypotheses on the ingredients of the ML-CSC model.\\n\\n8 Eﬀectiveness in natural sciences\\n\\nThe theoretical insights of the previous sections do not always accurately describe the performance of NNs in\\napplications. Indeed, there often exists a considerable gap between the predictions of approximation theory\\nand the practical performance of NNs [AD20].\\n\\nIn this section, we consider concrete applications which have been very successfully solved with deep-\\nlearning-based methods. In Section 8.1 we present an overview of deep-learning-based algorithms applied to\\ninverse problems. Section 8.2 then continues by describing how NNs can be used as a numerical ansatz for\\nsolving PDEs, highlighting their use in the solution of the multi-electron Schr¨odinger equation.\\n\\n8.1 Deep neural networks meet inverse problems\\n\\nThe area of inverse problems, predominantly in imaging, was presumably the ﬁrst class of mathematical\\nmethods embracing deep learning with overwhelming success. Let us consider a forward operator K : Y → X\\nwith X , Y being Hilbert spaces and the associated inverse problem of ﬁnding y ∈ Y such that Ky = x for\\ngiven features x ∈ X . The classical model-based approach to regularization aims to approximate K by\\ninvertible operators, and is hence strongly based on functional analytic principles. Today, such approaches\\ntake well-posedness of the approximation, convergence properties, as well as the structure of regularized\\nsolutions into account. The last item allows to incorporate prior information of the original solution such as\\nregularity, sharpness of edges, or—in the case of sparse regularization [JMS17]—a sparse coeﬃcient sequence\\nwith respect to a prescribed representation system. Such approaches are typically realized in a variational\\nsetting and hence aim to minimize functionals of the form\\n\\n(cid:107)Ky − x(cid:107)2 + αR(y),\\n\\nwhere α ∈ (0, ∞) is a regularization parameter, R : Y → [0, ∞) a regularization term, and (cid:107) · (cid:107) denotes the\\nnorm on Y. As said, the regularization term aims to model structural information about the desired solution.\\nHowever, one main hurdle in this approach is the problem that typically solution classes such as images from\\ncomputed tomography cannot be modeled accurately enough to, for instance, allow reconstruction under the\\nconstraint of a signiﬁcant amount of missing features.\\n\\nThis has opened the door to data-driven approaches, and recently, deep NNs. Solvers of inverse problems\\n\\nwhich are based on deep learning techniques can be roughly categorized into three classes:\\n\\n1. Supervised approaches: The most straightforward approach is to train a NN Φ(·, θ) : X → Y end-to-end,\\ni.e., to completely learn the map from data x to the solution y. More advanced approaches in this\\ndirection incorporate information about the operator K into the NN such as in [A ¨O17, GOW19, MLE21].\\nYet another type of approaches aims to combine deep NNs with classical model-based approaches.\\nThe ﬁrst suggestion in this realm was to start by applying a standard solver, followed by a deep NN\\nΦ(·, θ) : Y → Y which serves as a denoiser for speciﬁc reconstruction artifacts, e.g., [JMFU17]. This\\nwas followed by more sophisticated methods such as plug-and-play frameworks for coupling inversion\\nand denoising [REM17].\\n\\n2. Semi-supervised approaches: These type of approaches aim to encode the regularization by a deep NN\\nΦ(·, θ) : Y → [0, ∞). The underlying idea is often to require stronger regularization on solutions y(i)\\n\\n55\\n\\n\\x0cthat are more prone to artifacts or other eﬀects of the instability of the problem. On solutions where\\ntypically few artifacts are observed less regularization can be used. Therefore, the learning algorithm\\nonly requires a set of labels (y(i))m\\ni=1 as well as a method to assess how hard the inverse problem for this\\nlabel would be. In this sense, the algorithm can be considered semi-supervised. This idea was followed,\\nfor example, in [L ¨OS18, LSAH20]. Taking a Bayesian viewpoint, one can also learn prior distributions\\nas deep NNs, which was done in [BZAJ20].\\n\\n3. Unsupervised approaches: One highlight of what we might coin unsupervised approaches in our problem\\nsetting is the introduction of deep image priors in [DKMB20, UVL18]. The key idea is to parametrize\\nthe solutions y as the output of a NN Φ(ξ, ·) : P → Y with parameters in a suitable space P, applied to\\na ﬁxed input ξ. Then, for given features x, one tries to solve minθ∈P (cid:107)KΦ(ξ, θ) − x(cid:107)2 in order to obtain\\nparameters ˆθ ∈ P that yield a solution candidate y = Φ(ξ, ˆθ). Here often early stopping is applied in\\nthe training of the network parameters.\\n\\nAs can be seen, one key conceptual question is how to “take the best out of both worlds”, in the sense\\nof optimally combining classical (model-based) methods—in particular the forward operator K—with deep\\nlearning. This is certainly sensitively linked to all characteristics of the particular application at hand, such\\nas availability and accuracy of training data, properties of the forward operator, or requirements for the\\nsolution. And each of the three classes of hybrid solvers follows a diﬀerent strategy.\\n\\nLet us now discuss advantages and disadvantages of methods from the three categories with a particular\\nfocus on a mathematical foundation. Supervised approaches suﬀer on the one hand from the problem that\\noften ground-truth data is not available or only in a very distorted form, leading to the fact that synthetic\\ndata constitutes a signiﬁcant part of the training data. Thus the learned NN will mainly perform as well as\\nthe algorithm which generated the data, but not signiﬁcantly improve it—only from an eﬃciency viewpoint.\\nOn the other hand, the inversion is often highly ill-posed, i.e., the inversion map has a large Lipschitz constant,\\nwhich negatively aﬀects the generalization ability of the NN. Improved approaches incorporate knowledge\\nabout the forward operator K as discussed, which helps to circumvent this issue.\\n\\nOne signiﬁcant advantage of semi-supervised approaches is that the underlying mathematical model of\\nthe inverse problem is merely augmented by the neural network-based regularization. Assuming that the\\nlearned regularizer satisﬁes natural assumptions, convergence proofs or stability estimates for the resulting\\nregularized methods are still available.\\n\\nFinally, unsupervised approaches have the advantage that the regularization is then fully due to the\\nspeciﬁc architecture of the deep NN. This makes these methods slightly easier to understand theoretically,\\nalthough, for instance, the deep prior approach in its full generality is still lacking a profound mathematical\\nanalysis.\\n\\n8.2 PDE-based models\\n\\nBesides applications in image processing and artiﬁcial intelligence, deep learning methods have recently\\nstrongly impacted the ﬁeld of numerical analysis. In particular, regarding the numerical solution of high-\\ndimensional PDEs. These PDEs are widely used as a model for complex processes and their numerical\\nsolution presents one of the biggest challenges in scientiﬁc computing. We mention three exemplary problem\\nclasses:\\n\\n1. Black–Scholes model: The Nobel award-winning theory of Fischer Black, Robert Merton, and Myron\\nScholes proposes a linear PDE model for the determination of a fair price of a (complex) ﬁnancial\\nderivative. The dimensionality of the model corresponds to the number of ﬁnancial assets which is\\ntypically quite large. The classical linear model, which can be solved eﬃciently via Monte Carlo methods\\nis quite limited. In order to take into account more realistic phenomena such as default risk, the PDE\\nthat models a fair price becomes nonlinear, and much more challenging to solve. In particular (with the\\nnotable exception of Multilevel Picard algorithms [EHJK19]) no general algorithm exists that provably\\nscales well with the dimension.\\n\\n56\\n\\n\\x0c2. Schr¨odinger equation: The electronic Schr¨odinger equation describes the stationary nonrelativistic\\nbehavior of a quantum mechanical electron system in the electric ﬁeld generated by the nuclei of\\na molecule. Its numerical solution is required to obtain stable molecular conﬁgurations, compute\\nvibrational spectra, or obtain forces governing molecular dynamics. If the number of electrons is large,\\nthis is again a high-dimensional problem and to date there exist no satisfactory algorithms for its\\nsolution: It is well known that diﬀerent gold standard methods may produce completely diﬀerent energy\\npredictions, for example, when applied to large delocalized molecules, rendering these methods useless\\nfor those problems.\\n\\n3. Hamilton–Jacobi–Bellman equation: The Hamilton–Jacobi–Bellman (HJB) equation models the value\\nfunction of (deterministic or stochastic) optimal control problems. The underlying dimensionality of the\\nmodel corresponds to the dimension of the space of states to be controlled and tends to be rather high\\nin realistic applications. The high dimensionality, together with the fact that HJB equations typically\\ntend to be fully nonlinear with non-smooth solutions, renders the numerical solution of HJB equations\\nextremely challenging and no general algorithms exist for this problem.\\n\\nDue to the favorable approximation results of NNs for high-dimensional functions (see especially Subsec-\\ntion 4.3), it might not come as a surprise that a NN ansatz has proven to be quite successful in solving the\\naforementioned PDE models. A pioneering work in this direction is [HJE18] which uses the backwards SDE\\nreformulation of semilinear parabolic PDEs to reformulate the evaluation of such a PDE at a speciﬁc point as\\nan optimization problem that can be solved by the deep learning paradigm. The resulting algorithm proves\\nquite successful in the high-dimensional regime and, for instance, enables the eﬃcient modeling of complex\\nﬁnancial derivatives including nonlinear eﬀects such as default risk. Another approach speciﬁcally tailored to\\nthe numerical solution of HJB equations is [NZGK21]. In this work, one uses the Pontryagin principle to\\ngenerate samples of the PDE solution along solutions of the corresponding boundary value problem. Other\\nnumerical approaches include the Deep Ritz Method [EY18], where a Dirichlet energy is minimized over a\\nset of NNs, or so-called Physics Informed Neural Networks [RPK19], where typically the PDE residual is\\nminimized along with some natural constraints, for instance, to enforce boundary conditions.\\n\\nDeep-learning-based methods arguably work best if they are combined with domain knowledge to inspire\\nNN architecture choices. We would like to illustrate this interplay at the hand of a speciﬁc and extremely\\nrelevant example: the electronic Schr¨odinger equation (under the Born–Oppenheimer approximation) which\\namounts to ﬁnding the smallest nonzero eigenvalue of the eigenvalue problem\\n\\nfor ψ : R3×n → R, where the Hamiltonian\\n\\nHRψ = λψψ,\\n\\n(HRψ)(r) = −\\n\\nn\\n(cid:88)\\n\\ni=1\\n\\n1\\n2\\n\\n(∆riψ)(r) −\\n\\n\\uf8eb\\n\\n\\uf8ed\\n\\nn\\n(cid:88)\\n\\np\\n(cid:88)\\n\\ni=1\\n\\nj=1\\n\\nZj\\n(cid:107)ri − Rj(cid:107)2\\n\\n−\\n\\np−1\\n(cid:88)\\n\\np\\n(cid:88)\\n\\ni=1\\n\\nj=i+1\\n\\nZiZj\\n(cid:107)Ri − Rj(cid:107)2\\n\\n−\\n\\nn−1\\n(cid:88)\\n\\nn\\n(cid:88)\\n\\ni=1\\n\\nj=i+1\\n\\n1\\n(cid:107)ri − rj(cid:107)2\\n\\n(8.1)\\n\\n\\uf8f6\\n\\n\\uf8f8 ψ(r)\\n\\n(cid:3) ∈ R3×p refer to the positions of the nuclei, (Zi)p\\n\\ndescribes the kinetic energy (ﬁrst term) as well as Coulomb attraction force between electrons and nuclei\\n(second and third term) and the Coulomb repulsion force between diﬀerent electrons (third term). Here,\\nthe coordinates R = (cid:2)R1 . . . Rp\\ni=1 ∈ Np denote the atomic\\nnumbers of the nuclei, and the coordinates r = (cid:2)r1, . . . , rn\\n(cid:3) ∈ R3×n refer to the positions of the electrons.\\nThe associated eigenfunction ψ describes the so-called wavefunction which can be interpreted in the sense\\nthat |ψ(r)|2/(cid:107)ψ(cid:107)2\\nL2 describes the joint probability density of the n electrons to be located at r. The smallest\\nsolution λψ of (8.1) describes the ground state energy associated with the nuclear coordinates R. It is\\nof particular interest to know the ground state energy for all nuclear coordinates, the so-called potential\\nenergy surface whose gradient determines the forces governing the dynamic motions of the nuclei. The\\nnumerical solution of (8.1) is complicated by the Pauli principle which states that the wave function ψ must\\nbe antisymmetric in all coordinates representing electrons of equal spin. To state it, we need to clarify that\\nevery electron is not only deﬁned by its location but also by its spin which may be positive or negative.\\n\\n57\\n\\n\\x0cDepending on whether two electrons have the same spin or not, their interaction changes massively. This is\\nreﬂected by the Pauli principle that we already mentioned: Suppose that electrons i and j have equal spin,\\nthen the wave function must satisfy\\n\\nPi,jψ = −ψ,\\n\\n(8.2)\\n\\nIn\\nwhere Pi,j denotes the operator that swaps ri and rj, i.e., (Pi,jψ)(r) = ψ(r1, . . . , rj, . . . , ri, . . . , rn).\\nparticular, no two electrons with the same spin can occupy the same location. The challenges associated with\\nsolving the Schr¨odinger equation inspired the following famous quote by Paul Dirac [Dir29]:\\n\\n“The fundamental laws necessary for the mathematical treatment of a large part of physics and\\nthe whole of chemistry are thus completely known, and the diﬃculty lies only in the fact that\\napplication of these laws leads to equations that are too complex to be solved.”\\n\\nWe now describe how deep learning methods might help to mitigate this claim to a certain extent. Let X\\nL2. Using the Rayleigh–Ritz principle, ﬁnding the minimal\\n\\nbe a random variable with density |ψ(r)|2/(cid:107)ψ(cid:107)2\\nnonzero eigenvalue of (8.1) can be reformulated as minimizing the Rayleigh quotient\\n\\n(cid:82)\\n\\nR3×n ψ(r)(HRψ)(r) dr\\n(cid:107)ψ(cid:107)2\\nL2\\n\\n= E\\n\\n(cid:20) (HRψ)(X)\\nψ(X)\\n\\n(cid:21)\\n\\n(8.3)\\n\\nover all ψ’s satisfying the Pauli principle, see [SO12]. Since this represents a minimization problem it can in\\nprinciple be solved via a NN ansatz by generating training data distributed according to X using MCMC\\nsampling31. Since the wave function ψ will be parametrized as a NN, the minimization of (8.3) will require\\nthe computation of the gradient of (8.3) with respect to the NN parameters (the method in [PSMF20] even\\nrequires second order derivatives) which, at ﬁrst sight, might seem to require the computation of third order\\nderivatives. However, due to the Hermitian structure of the Hamiltonian one does not need to compute the\\nderivative of the Laplacian of ψ, see, for example, [HSN20, Equation (8)].\\n\\nCompared to the other PDE problems we have discussed, an additional complication arises from the\\nneed to incorporate structural properties and invariances such as the Pauli principle. Furthermore, empirical\\nevidence shows that it is also necessary to hard code the so-called cusp conditions which describe the\\nasymptotic behavior of nearby electrons and electrons close to a nucleus into the NN architecture. A ﬁrst\\nattempt in this direction has been made in [HZE19] and signiﬁcantly improved NN architectures have been\\ndeveloped in [HSN20, PSMF20, SRG+21] opening the possibility of accurate ab initio computations for\\npreviously intractable molecules. The mathematical properties of this exciting line of work remain largely\\nunexplored. We brieﬂy describe the main ideas behind the NN architecture of [HSN20, SRG+21]. Standard\\nnumerical approaches (notably the Multireference Hartree Fock Method, see [SO12]) use a low rank approach\\nto minimize (8.3). Such a low rank approach would approximate ψ by sums of products of one electron\\norbitals (cid:81)n\\ni=1 ϕi(ri) but clearly this does not satisfy the Pauli principle (8.2). In order to ensure the Pauli\\nprinciple, one constructs so-called Slater determinants from one electron orbitals with equal spin. More\\nprecisely, suppose that the ﬁrst n+ electrons with coordinates r1, . . . , rn+ have positive spin and the last\\nn − n+ electrons have negative spin. Then any function of the form\\n\\n(cid:16)\\n\\ndet\\n\\n(ϕi(rj))n+\\n\\ni,j=1\\n\\n(cid:17)\\n\\n(cid:16)\\n\\n· det\\n\\n(ϕi(rj))n\\n\\ni,j=n++1\\n\\n(cid:17)\\n\\n(8.4)\\n\\nsatisﬁes (8.2) and is typically called a Slater determinant. While the Pauli principle establishes an (non-\\nclassical) interaction between electrons of equal spin, the so-called exchange correlation, electrons with\\nopposite spins are uncorrelated in the representation (8.4). In particular, (8.4) ignores interactions between\\nelectrons that arise through Coulomb forces, implying that no nontrivial wavefunction can be accurately\\nrepresented by a single Slater determinant. To capture physical interactions between diﬀerent electrons,\\none needs to use sums of Slater determinants as an ansatz. However, it turns out that the number of such\\ndeterminants that are needed to guarantee a given accuracy scales very badly with the system size n (to the\\n\\n31Observe that for such sampling methods one can just use the unnormalized density |ψ(r)|2 and thus avoid the computation\\n\\nof the normalization (cid:107)ψ(cid:107)2\\n\\nL2 .\\n\\n58\\n\\n\\x0cbest of our knowledge the best currently known approximation results are contained in [Yse10], where an\\nn-independent error rate is shown, however the implicit constant in this rate depends at least exponentially\\non the system size n).\\n\\nWe would like to highlight the approach of [HSN20] whose main idea is to use NNs to incorporate\\ninteractions into Slater determinants of the form (8.4) using what is called the backﬂow trick [RMD+06].\\nThe basic building blocks would now consist of functions of the form\\n\\n(cid:16)\\n\\ndet\\n\\n(ϕi(rj)Ψj(r, θj))n+\\n\\ni,j=1\\n\\n(cid:17)\\n\\n(cid:16)\\n\\n· det\\n\\n(ϕi(rj)Ψj(r, θj))n\\n\\ni,j=n++1\\n\\n(cid:17)\\n\\n,\\n\\n(8.5)\\n\\nwhere Ψk(·, θk), k ∈ [n], are NNs. If these are arbitrary NNs, it is easy to see that the Pauli principle (8.2)\\nwill not be satisﬁed. However, if we require the NNs to be symmetric, for example, in the sense that for\\ni, j, s ∈ [n+] it holds that\\n\\nPi,jΨk(·, θk) =\\n\\n\\uf8f1\\n\\uf8f4\\uf8f2\\n\\n\\uf8f4\\uf8f3\\n\\nΨk(·, θk),\\nΨi(·, θi),\\nΨj(·, θj),\\n\\nif k /∈ {i, j},\\nif k = j,\\nif k = i,\\n\\n(8.6)\\n\\nand analogous conditions hold for i, j, k ∈ [n] \\\\ [n+], the expression (8.5) does actually satisfy (8.2). The\\nconstruction of such symmetric NNs can be achieved by using a modiﬁcation of the so-called SchNet\\nArchitecture [SKS+17] which can be considered as a speciﬁc residual NN.\\n\\nWe describe a simpliﬁed construction which is inspired by [HZE19] and used in a slightly more complex\\nform in [SRG+21]. We restrict ourselves to the case of positive spin (e.g., the ﬁrst n+ coordinates), the case\\nof negative spin being handled in the same way. Let Υ(·, θ+\\nemb) be a univariate NN (with possibly multivariate\\noutput) and denote\\n\\nEmbk(r, θ+\\n\\nemb) :=\\n\\nΥ((cid:107)rk − ri(cid:107)2, θ+\\n\\nemb),\\n\\nk ∈ [n+],\\n\\nn+\\n(cid:88)\\n\\nthe k-th embedding layer. For k ∈ [n+], we can now deﬁne\\n\\ni=1\\n\\nΨk (r, θk) = Ψk\\n\\n(cid:0)r, (θk,fc, θ+\\n\\nemb)(cid:1) = Γk\\n\\n(cid:0)(cid:0)Embk(r, θ+\\n\\nemb), (rn++1, . . . , rn)(cid:1) , θk,fc\\n\\n(cid:1) ,\\n\\nwhere Γk(·, θk,fc) denotes a standard FC NN with input dimension equal to the output dimension of Ψ+\\nplus the dimension of negative spin electrons. The networks Ψk, k ∈ [n] \\\\ [n+], are deﬁned analogously\\nusing diﬀerent parameters θ−\\nemb for the embeddings. It is straightforward to check that the NNs Ψk, k ∈ [n],\\nsatisfy (8.6) so that the backﬂow determinants (8.5) satisfy the Pauli principle (8.2).\\n\\nIn [HSN20] the backﬂow determinants (8.5) are further augmented by a multiplicative correction term,\\nthe so-called Jastrow factor which is also represented by a speciﬁc symmetric NN, as well as a correction\\nterm that ensures the validity of the cusp conditions. The results of [HSN20] show that this ansatz (namely\\nusing linear combinations of backﬂow determinants (8.5) instead of plain Slater determinants (8.4)) is vastly\\nmore eﬃcient in terms of number of determinants needed to obtain chemical accuracy. The full architecture\\nprovides a general purpose NN architecture to represent complicated wave functions. A distinct advantage\\nof this approach is that some parameters (for example, embedding layers) may be shared across diﬀerent\\nnuclear geometries R ∈ R3×p which allows for the eﬃcient computation of potential energy surfaces [SRG+21],\\nsee Figure 8.1. Finally, we would like to highlight the customized NN design that incorporates physical\\ninvariances, domain knowledge (for example, in the form of cusp conditions), and existing numerical methods,\\nall of which are required for the method to reach its full potential.\\n\\nAcknowledgment\\n\\nThe research of JB was supported by the Austrian Science Fund (FWF) under grant I3403-N32. GK\\nacknowledges support from DFG-SPP 1798 Grants KU 1446/21-2 and KU 1446/27-2, DFG-SFB/TR 109\\nGrant C09, BMBF Grant MaGriDo, and NSF-Simons Foundation Grant SIMONS 81420. The authors would\\n\\n59\\n\\n\\x0cFigure 8.1: By sharing layers across diﬀerent nuclear geometries one can eﬃciently compute diﬀerent\\ngeometries in one single training step [SRG+21]. Left: Potential energy surface of H10 chain computed by the\\ndeep-learning-based algorithm from [SRG+21]. The lowest energy is achieved when pairs of H atoms enter\\ninto a covalent bond to form ﬁve H2 molecules. Right: The method of [SRG+21] is capable of accurately\\ncomputing forces between nuclei which allows for molecular dynamics simulations from ﬁrst principles.\\n\\nlike to thank H´ector Andrade Loarca, Dennis Elbr¨achter, Adalbert Fono, Pavol Harar, Lukas Liehr, Duc Anh\\nNguyen, Mariia Seleznova, and Frieder Simon for their helpful feedback on an early version of this article. In\\nparticular, Dennis Elbr¨achter was providing help for several theoretical results.\\n\\nReferences\\n\\n[AA ˇC13]\\n\\n[AB99]\\n\\n[ACGH19]\\n\\nAntonio Auﬃnger, G´erard Ben Arous, and Jiˇr´ı ˇCern`y, Random matrices and complexity of spin\\nglasses, Communications on Pure and Applied Mathematics 66 (2013), no. 2, 165–201.\\n\\nMartin Anthony and Peter L Bartlett, Neural network learning: Theoretical foundations,\\nCambridge University Press, 1999.\\n\\nSanjeev Arora, Nadav Cohen, Noah Golowich, and Wei Hu, A convergence analysis of gradient\\ndescent for deep linear neural networks, International Conference on Learning Representations,\\n2019.\\n\\n[ACH18]\\n\\nSanjeev Arora, Nadav Cohen, and Elad Hazan, On the optimization of deep networks: Implicit\\nacceleration by overparameterization, International Conference on Machine Learning, 2018,\\npp. 372–389.\\n\\n[AD20]\\n\\nBen Adcock and Nick Dexter, The gap between theory and practice in function approximation\\nwith deep neural networks, 2020, arXiv preprint arXiv:2001.07523.\\n\\n[ADH+19]\\n\\n[AGNZ18]\\n\\nSanjeev Arora, Simon S Du, Wei Hu, Zhiyuan Li, Ruslan Salakhutdinov, and Ruosong Wang, On\\nexact computation with an inﬁnitely wide neural net, Advances in Neural Information Processing\\nSystems, 2019, pp. 8139–8148.\\n\\nSanjeev Arora, Rong Ge, Behnam Neyshabur, and Yi Zhang, Stronger generalization bounds\\nfor deep nets via a compression approach, International Conference on Machine Learning, 2018,\\npp. 254–263.\\n\\n[AHNB+20] Yasmine S Al-Hamdani, P´eter R Nagy, Dennis Barton, Mih´aly K´allay, Jan Gerit Brandenburg,\\nand Alexandre Tkatchenko, Interactions between large molecules: Puzzle for reference quantum-\\nmechanical methods, 2020, arXiv preprint arXiv:2009.08927.\\n\\n60\\n\\n\\x0c[AHS85]\\n\\nDavid H Ackley, Geoﬀrey E Hinton, and Terrence J Sejnowski, A learning algorithm for\\nBoltzmann machines, Cognitive Science 9 (1985), no. 1, 147–169.\\n\\n[AHW96]\\n\\nPeter Auer, Mark Herbster, and Manfred K Warmuth, Exponentially many local minima for\\nsingle neurons, Advances in Neural Information Processing Systems, 1996, p. 316–322.\\n\\n[AM ¨OS19]\\n\\nSimon Arridge, Peter Maass, Ozan ¨Oktem, and Carola-Bibiane Sch¨onlieb, Solving inverse\\nproblems using data-driven models, Acta Numerica 28 (2019), 1–174.\\n\\n[A ¨O17]\\n\\nJonas Adler and Ozan ¨Oktem, Solving ill-posed inverse problems using iterative deep neural\\nnetworks, Inverse Problems 33 (2017), no. 12, 124007.\\n\\n[AZLS19]\\n\\nZeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song, A convergence theory for deep learning via\\nover-parameterization, International Conference on Machine Learning, 2019, pp. 242–252.\\n\\n[Bar92]\\n\\nAndrew R Barron, Neural net approximation, Yale Workshop on Adaptive and Learning Systems,\\nvol. 1, 1992, pp. 69–72.\\n\\n[Bar93]\\n\\n, Universal approximation bounds for superpositions of a sigmoidal function, IEEE\\n\\nTransactions on Information Theory 39 (1993), no. 3, 930–945.\\n\\n[Bar98]\\n\\nPeter L Bartlett, The sample complexity of pattern classiﬁcation with neural networks: the size\\nof the weights is more important than the size of the network, IEEE Transactions on Information\\nTheory 44 (1998), no. 2, 525–536.\\n\\n[BBC17]\\n\\nAlfred Bourely, John Patrick Boueri, and Krzysztof Choromonski, Sparse neural networks\\ntopologies, 2017, arXiv preprint arXiv:1706.05683.\\n\\n[BBC+19] Christopher Berner, Greg Brockman, Brooke Chan, Vicki Cheung, Przemyslaw Debiak, Christy\\nDennison, David Farhi, Quirin Fischer, Shariq Hashme, and Chris Hesse, Dota 2 with large scale\\ndeep reinforcement learning, 2019, arXiv preprint arXiv:1912.06680.\\n\\n[BBG+21] Christian Beck, Sebastian Becker, Philipp Grohs, Nor Jaafari, and Arnulf Jentzen, Solving the\\nkolmogorov pde by means of deep learning, Journal of Scientiﬁc Computing 88 (2021), no. 3,\\n1–28.\\n\\n[BBL03]\\n\\nOlivier Bousquet, St´ephane Boucheron, and G´abor Lugosi, Introduction to statistical learning\\ntheory, Summer School on Machine Learning, 2003, pp. 169–207.\\n\\n[BBL+17] Michael M Bronstein, Joan Bruna, Yann LeCun, Arthur Szlam, and Pierre Vandergheynst,\\nGeometric deep learning: going beyond euclidean data, IEEE Signal Processing Magazine 34\\n(2017), no. 4, 18–42.\\n\\n[BBM05]\\n\\nPeter L Bartlett, Olivier Bousquet, and Shahar Mendelson, Local Rademacher complexities, The\\nAnnals of Statistics 33 (2005), no. 4, 1497–1537.\\n\\n[BCB15]\\n\\n[BDG20]\\n\\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio, Neural machine translation by jointly\\nlearning to align and translate, International Conference on Learning Representations, 2015.\\n\\nJulius Berner, Markus Dablander, and Philipp Grohs, Numerically solving parametric families\\nof high-dimensional Kolmogorov partial diﬀerential equations via deep learning, Advances in\\nNeural Information Processing Systems, 2020, pp. 16615–16627.\\n\\n[BE02]\\n\\nOlivier Bousquet and Andr´e Elisseeﬀ, Stability and generalization, Journal of Machine Learning\\nResearch 2 (2002), no. Mar, 499–526.\\n\\n61\\n\\n\\x0c[BEG19]\\n\\n[Bel52]\\n\\n[BF19]\\n\\n[BFT17]\\n\\n[BGJ20]\\n\\nJulius Berner, Dennis Elbr¨achter, and Philipp Grohs, How degenerate is the parametrization of\\nneural networks with the ReLU activation function?, Advances in Neural Information Processing\\nSystems, 2019, pp. 7790–7801.\\n\\nRichard Bellman, On the theory of dynamic programming, Proceedings of the National Academy\\nof Sciences 38 (1952), no. 8, 716.\\n\\nJan Bohn and Michael Feischl, Recurrent neural networks as optimal mesh reﬁnement strategies,\\n2019, arXiv preprint arXiv:1909.04275.\\n\\nPeter L Bartlett, Dylan J Foster, and Matus Telgarsky, Spectrally-normalized margin bounds for\\nneural networks, Advances in Neural Information Processing Systems, 2017, pp. 6240–6249.\\n\\nJulius Berner, Philipp Grohs, and Arnulf Jentzen, Analysis of the generalization error: Empirical\\nrisk minimization over deep artiﬁcial neural networks overcomes the curse of dimensionality in\\nthe numerical approximation of black–scholes partial diﬀerential equations, SIAM Journal on\\nMathematics of Data Science 2 (2020), no. 3, 631–657.\\n\\n[BH89]\\n\\nEric B Baum and David Haussler, What size net gives valid generalization?, Neural Computation\\n1 (1989), no. 1, 151–160.\\n\\n[BHLM19] Peter L Bartlett, Nick Harvey, Christopher Liaw, and Abbas Mehrabian, Nearly-tight VC-\\ndimension and pseudodimension bounds for piecewise linear neural networks, Journal of Machine\\nLearning Research 20 (2019), 63–1.\\n\\n[BHMM19] Mikhail Belkin, Daniel Hsu, Siyuan Ma, and Soumik Mandal, Reconciling modern machine-\\nlearning practice and the classical bias–variance trade-oﬀ, Proceedings of the National Academy\\nof Sciences 116 (2019), no. 32, 15849–15854.\\n\\n[BHX20]\\n\\nMikhail Belkin, Daniel Hsu, and Ji Xu, Two models of double descent for weak features, SIAM\\nJournal on Mathematics of Data Science 2 (2020), no. 4, 1167–1180.\\n\\n[BK18]\\n\\nAndrew R Barron and Jason M Klusowski, Approximation and estimation for high-dimensional\\ndeep learning networks, 2018, arXiv preprint arXiv:1809.03090.\\n\\n[BLLT20]\\n\\nPeter L Bartlett, Philip M Long, G´abor Lugosi, and Alexander Tsigler, Benign overﬁtting\\nin linear regression, Proceedings of the National Academy of Sciences 117 (2020), no. 48,\\n30063–30070.\\n\\n[BMM98]\\n\\nPeter L Bartlett, Vitaly Maiorov, and Ron Meir, Almost linear VC-dimension bounds for\\npiecewise polynomial networks, Neural Computation 10 (1998), no. 8, 2159–2173.\\n\\n[BMM18] Mikhail Belkin, Siyuan Ma, and Soumik Mandal, To understand deep learning we need to\\nunderstand kernel learning, International Conference on Machine Learning, 2018, pp. 541–549.\\n\\n[BMR+20] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,\\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel\\nHerbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler,\\nJeﬀrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray,\\nBenjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever,\\nand Dario Amodei, Language models are few-shot learners, Advances in Neural Information\\nProcessing Systems, 2020, pp. 1877–1901.\\n\\n[BR89]\\n\\nAvrim Blum and Ronald L Rivest, Training a 3-node neural network is NP-complete, Advances\\nin Neural Information Processing Systems, 1989, pp. 494–501.\\n\\n62\\n\\n\\x0c[BRT19]\\n\\nMikhail Belkin, Alexander Rakhlin, and Alexandre B Tsybakov, Does data interpolation con-\\ntradict statistical optimality?, International Conference on Artiﬁcial Intelligence and Statistics,\\n2019, pp. 1611–1619.\\n\\n[BSW14]\\n\\nPierre Baldi, Peter Sadowski, and Daniel Whiteson, Searching for exotic particles in high-energy\\nphysics with deep learning, Nature Communications 5 (2014), no. 1, 1–9.\\n\\n[BZAJ20]\\n\\nRiccardo Barbano, Chen Zhang, Simon Arridge, and Bangti Jin, Quantifying model uncertainty\\nin inverse problems via bayesian deep gradient descent, 2020, arXiv preprint arXiv:2007.09971.\\n\\n[Can98]\\n\\n[CB20]\\n\\nEmmanuel J Cand`es, Ridgelets: Theory and applications, Ph.D. thesis, Stanford University,\\n1998.\\n\\nLenaic Chizat and Francis Bach, Implicit bias of gradient descent for wide two-layer neural\\nnetworks trained with the logistic loss, Conference on Learning Theory, 2020, pp. 1305–1338.\\n\\n[CHM+15] Anna Choromanska, Mikael Henaﬀ, Michael Mathieu, G´erard Ben Arous, and Yann LeCun,\\nThe loss surfaces of multilayer networks, International Conference on Artiﬁcial Intelligence and\\nStatistics, 2015, pp. 192–204.\\n\\n[CJLZ19] Minshuo Chen, Haoming Jiang, Wenjing Liao, and Tuo Zhao, Eﬃcient approximation of deep\\nReLU networks for functions on low dimensional manifolds, Advances in Neural Information\\nProcessing Systems, 2019, pp. 8174–8184.\\n\\n[CK20]\\n\\n[CKP12]\\n\\n[CL19]\\n\\n[CLA15]\\n\\n[CLM94]\\n\\n[CM18]\\n\\nAlexander Cloninger and Timo Klock, ReLU nets adapt to intrinsic dimensionality beyond the\\ntarget domain, 2020, arXiv preprint arXiv:2008.02545.\\n\\nPeter G Casazza, Gitta Kutyniok, and Friedrich Philipp, Introduction to ﬁnite frame theory,\\nFinite Frames: Theory and Applications, Birkh¨auser Boston, 2012, pp. 1–53.\\n\\nWojciech Czaja and Weilin Li, Analysis of time-frequency scattering transforms, Applied and\\nComputational Harmonic Analysis 47 (2019), no. 1, 149–171.\\n\\nAnna Choromanska, Yann LeCun, and G´erard Ben Arous, Open problem: The landscape of the\\nloss surfaces of multilayer networks, Conference on Learning Theory, 2015, pp. 1756–1760.\\n\\nCharles K Chui, Xin Li, and Hrushikesh N Mhaskar, Neural networks for localized approximation,\\nMathematics of Computation 63 (1994), no. 208, 607–623.\\n\\nCharles K Chui and Hrushikesh N Mhaskar, Deep nets for local manifold learning, Frontiers in\\nApplied Mathematics and Statistics 4 (2018), 12.\\n\\n[CMBK20] Lin Chen, Yifei Min, Mikhail Belkin, and Amin Karbasi, Multiple descent: Design your own\\n\\ngeneralization curve, 2020, arXiv preprint arXiv:2008.01036.\\n\\n[COB19]\\n\\n[CPV20]\\n\\nLenaic Chizat, Edouard Oyallon, and Francis Bach, On lazy training in diﬀerentiable program-\\nming, Advances in Neural Information Processing Systems, 2019, pp. 2937–2947.\\n\\nAndrei Caragea, Philipp Petersen, and Felix Voigtlaender, Neural network approximation and\\nestimation of classiﬁers with classiﬁcation boundary in a Barron class, 2020, arXiv preprint\\narXiv:2011.09363.\\n\\n[CS02]\\n\\nFelipe Cucker and Steve Smale, On the mathematical foundations of learning, Bulletin of the\\nAmerican Mathematical Society 39 (2002), no. 1, 1–49.\\n\\n[CvMG+14] Kyunghyun Cho, Bart van Merri¨enboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares,\\nHolger Schwenk, and Yoshua Bengio, Learning phrase representations using rnn encoder–decoder\\nfor statistical machine translation, Proceedings of the 2014 Conference on Empirical Methods in\\nNatural Language Processing, 2014, pp. 1724–1734.\\n\\n63\\n\\n\\x0c[Cyb89]\\n\\n[CZ07]\\n\\n[DDS+09]\\n\\nGeorge Cybenko, Approximation by superpositions of a sigmoidal function, Mathematics of\\nControl, Signals and Systems 2 (1989), no. 4, 303–314.\\n\\nFelipe Cucker and Ding-Xuan Zhou, Learning theory: an approximation theory viewpoint, vol. 24,\\nCambridge University Press, 2007.\\n\\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei, Imagenet: A large-scale\\nhierarchical image database, Proceedings of the IEEE Conference on Computer Vision and\\nPattern Recognition, 2009, pp. 248–255.\\n\\n[DeV98]\\n\\nRonald A DeVore, Nonlinear approximation, Acta Numerica 7 (1998), 51–150.\\n\\n[DGL96]\\n\\n[DHL18]\\n\\n[DHP20]\\n\\n[Dir29]\\n\\nLuc Devroye, L´aszl´o Gy¨orﬁ, and G´abor Lugosi, A probabilistic theory of pattern recognition,\\nSpringer, 1996.\\n\\nSimon S Du, Wei Hu, and Jason D Lee, Algorithmic regularization in learning deep homogeneous\\nmodels: Layers are automatically balanced, Advances in Neural Information Processing Systems,\\n2018, pp. 384–395.\\n\\nRonald DeVore, Boris Hanin, and Guergana Petrova, Neural network approximation, 2020, arXiv\\npreprint arXiv:2012.14501.\\n\\nPaul Adrien Maurice Dirac, Quantum mechanics of many-electron systems, Proceedings of the\\nRoyal Society of London. Series A, Containing Papers of a Mathematical and Physical Character\\n123 (1929), no. 792, 714–733.\\n\\n[DKMB20] S¨oren Dittmer, Tobias Kluth, Peter Maass, and Daniel Otero Baguer, Regularization by ar-\\nchitecture: A deep prior approach for inverse problems, Journal of Mathematical Imaging and\\nVision 62 (2020), no. 3, 456–470.\\n\\n[DLL+19]\\n\\nSimon S Du, Jason D Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai, Gradient descent ﬁnds\\nglobal minima of deep neural networks, International Conference on Machine Learning, 2019,\\npp. 1675–1685.\\n\\n[Don69]\\n\\nWilliam F Donoghue, Distributions and fourier transforms, Pure and Applied Mathematics,\\nAcademic Press, 1969.\\n\\n[DPG+14] Yann N Dauphin, Razvan Pascanu, Caglar Gulcehre, Kyunghyun Cho, Surya Ganguli, and\\nYoshua Bengio, Identifying and attacking the saddle point problem in high-dimensional non-\\nconvex optimization, Advances in Neural Information Processing Systems, 2014, pp. 2933–2941.\\n\\n[DR17]\\n\\n[Dre62]\\n\\n[Dud67]\\n\\n[Dud14]\\n\\n[DZPS18]\\n\\nGintare Karolina Dziugaite and Daniel M Roy, Computing nonvacuous generalization bounds for\\ndeep (stochastic) neural networks with many more parameters than training data, Conference on\\nUncertainty in Artiﬁcial Intelligence, 2017.\\n\\nStuart Dreyfus, The numerical solution of variational problems, Journal of Mathematical\\nAnalysis and Applications 5 (1962), no. 1, 30–45.\\n\\nRichard M Dudley, The sizes of compact subsets of hilbert space and continuity of Gaussian\\nprocesses, Journal of Functional Analysis 1 (1967), no. 3, 290–330.\\n\\n, Uniform central limit theorems, vol. 142, Cambridge University Press, 2014.\\n\\nSimon S Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh, Gradient descent provably optimizes\\nover-parameterized neural networks, International Conference on Learning Representations, 2018.\\n\\n[E17]\\n\\nWeinan E, A proposal on machine learning via dynamical systems, Communications in Mathe-\\nmatics and Statistics 5 (2017), no. 1, 1–11.\\n\\n64\\n\\n\\x0c[EGJS18]\\n\\nDennis Elbr¨achter, Philipp Grohs, Arnulf Jentzen, and Christoph Schwab, DNN expression\\nrate analysis of high-dimensional PDEs: Application to option pricing, 2018, arXiv preprint\\narXiv:1809.07669.\\n\\n[EHJK19] Weinan E, Martin Hutzenthaler, Arnulf Jentzen, and Thomas Kruse, On multilevel picard\\nnumerical approximations for high-dimensional nonlinear parabolic partial diﬀerential equations\\nand high-dimensional nonlinear backward stochastic diﬀerential equations, Journal of Scientiﬁc\\nComputing 79 (2019), no. 3, 1534–1571.\\n\\n[EHL19]\\n\\nWeinan E, Jiequn Han, and Qianxiao Li, A mean-ﬁeld optimal control formulation of deep\\nlearning, Research in the Mathematical Sciences 6 (2019), no. 1, 1–41.\\n\\n[Elm90]\\n\\nJeﬀrey L Elman, Finding structure in time, Cognitive Science 14 (1990), no. 2, 179–211.\\n\\n[EMW19a] Weinan E, Chao Ma, and Lei Wu, Barron spaces and the compositional function spaces for\\n\\nneural network models, 2019, arXiv preprint arXiv:1906.08039.\\n\\n[EMW19b]\\n\\n, A priori estimates of the population risk for two-layer neural networks, Communications\\n\\nin Mathematical Sciences 17 (2019), no. 5, 1407–1425.\\n\\n[EMWW20] Weinan E, Chao Ma, Stephan Wojtowytsch, and Lei Wu, Towards a mathematical understanding\\nof neural network-based machine learning: what we know and what we don’t, 2020, arXiv preprint\\narXiv:2009.10713.\\n\\n[EPGB19] Dennis Elbr¨achter, Dmytro Perekrestenko, Philipp Grohs, and Helmut B¨olcskei, Deep neural\\n\\nnetwork approximation theory, 2019, arXiv preprint arXiv:1901.02220.\\n\\n[ES16]\\n\\nRonen Eldan and Ohad Shamir, The power of depth for feedforward neural networks, Conference\\non Learning Theory, vol. 49, 2016, pp. 907–940.\\n\\n[EW20a] Weinan E and Stephan Wojtowytsch, On the Banach spaces associated with multi-layer ReLU\\nnetworks: Function representation, approximation theory and gradient descent dynamics, 2020,\\narXiv preprint arXiv:2007.15623.\\n\\n[EW20b]\\n\\n, A priori estimates for classiﬁcation problems using neural networks, 2020, arXiv preprint\\n\\narXiv:2009.13500.\\n\\n[EW20c]\\n\\n, Representation formulas and pointwise properties for Barron functions, 2020, arXiv\\n\\npreprint arXiv:2006.05982.\\n\\n[EY18]\\n\\n[FB17]\\n\\n[FC18]\\n\\n[FHH+17]\\n\\nWeinan E and Bing Yu, The deep ritz method: a deep learning-based numerical algorithm for\\nsolving variational problems, Communications in Mathematics and Statistics 6 (2018), no. 1,\\n1–12.\\n\\nDaniel C Freeman and Joan Bruna, Topology and geometry of half-rectiﬁed network optimization,\\nInternational Conference on Learning Representations, 2017.\\n\\nJonathan Frankle and Michael Carbin, The lottery ticket hypothesis: Finding sparse, trainable\\nneural networks, International Conference on Learning Representations, 2018.\\n\\nFelix A Faber, Luke Hutchison, Bing Huang, Justin Gilmer, Samuel S Schoenholz, George E\\nDahl, Oriol Vinyals, Steven Kearnes, Patrick F Riley, and O Anatole Von Lilienfeld, Prediction\\nerrors of molecular machine learning models lower than hybrid DFT error, Journal of Chemical\\nTheory and Computation 13 (2017), no. 11, 5255–5264.\\n\\n[Fun89]\\n\\nKen-Ichi Funahashi, On the approximate realization of continuous mappings by neural networks,\\nNeural Networks 2 (1989), no. 3, 183–192.\\n\\n65\\n\\n\\x0c[GBC16]\\n\\nIan Goodfellow, Yoshua Bengio, and Aaron Courville, Deep learning, MIT Press, 2016.\\n\\n[G´er17]\\n\\n[GH21]\\n\\nAurelien G´eron, Hands-on machine learning with scikit-learn and tensorﬂow: Concepts, tools,\\nand techniques to build intelligent systems, O’Reilly Media, 2017.\\n\\nPhilipp Grohs and Lukas Herrmann, Deep neural network approximation for high-dimensional\\nparabolic Hamilton-Jacobi-Bellman equations, 2021, arXiv preprint arXiv:2103.05744.\\n\\n[GH22]\\n\\n, Deep neural network approximation for high-dimensional elliptic PDEs with boundary\\n\\nconditions, IMA Journal of Numerical Analysis 42 (2022), no. 3, 2055–2082.\\n\\n[GHJVW20] Philipp Grohs, Fabian Hornung, Arnulf Jentzen, and Philippe Von Wurstemberger, A proof that\\nartiﬁcial neural networks overcome the curse of dimensionality in the numerical approximation\\nof Black-Scholes partial diﬀerential equations, Memoirs of the American Mathematical Society\\n(2020).\\n\\n[GHJY15] Rong Ge, Furong Huang, Chi Jin, and Yang Yuan, Escaping from saddle points—online stochastic\\n\\ngradient for tensor decomposition, Conference on Learning Theory, 2015, pp. 797–842.\\n\\n[GJS+20] Mario Geiger, Arthur Jacot, Stefano Spigler, Franck Gabriel, Levent Sagun, St´ephane d’Ascoli,\\nGiulio Biroli, Cl´ement Hongler, and Matthieu Wyart, Scaling description of generalization with\\nnumber of parameters in deep learning, Journal of Statistical Mechanics: Theory and Experiment\\n(2020), no. 2, 023401.\\n\\n[GKP20]\\n\\n[GKR20]\\n\\n[GL13]\\n\\nIngo G¨uhring, Gitta Kutyniok, and Philipp Petersen, Error bounds for approximations with deep\\nReLU neural networks in W s,p norms, Analysis and Applications 18 (2020), no. 05, 803–859.\\n\\nPhilipp Grohs, Sarah Koppensteiner, and Martin Rathmair, Phase retrieval: Uniqueness and\\nstability, SIAM Review 62 (2020), no. 2, 301–350.\\n\\nSaeed Ghadimi and Guanghui Lan, Stochastic ﬁrst-and zeroth-order methods for nonconvex\\nstochastic programming, SIAM Journal on Optimization 23 (2013), no. 4, 2341–2368.\\n\\n[GLSS18a]\\n\\nSuriya Gunasekar, Jason D Lee, Daniel Soudry, and Nathan Srebro, Characterizing implicit\\nbias in terms of optimization geometry, International Conference on Machine Learning, 2018,\\npp. 1832–1841.\\n\\n[GLSS18b]\\n\\n, Implicit bias of gradient descent on linear convolutional networks, Advances in Neural\\n\\nInformation Processing Systems, 2018, pp. 9461–9471.\\n\\n[GMMM21] Behrooz Ghorbani, Song Mei, Theodor Misiakiewicz, and Andrea Montanari, Linearized two-\\nlayers neural networks in high dimension, The Annals of Statistics 49 (2021), no. 2, 1029–1054.\\n\\n[GOW19]\\n\\nDavis Gilton, Greg Ongie, and Rebecca Willett, Neumann networks for linear inverse problems\\nin imaging, IEEE Transactions on Computational Imaging 6 (2019), 328–343.\\n\\n[GPAM+14] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil\\nOzair, Aaron Courville, and Yoshua Bengio, Generative adversarial nets, Advances in Neural\\nInformation Processing Systems, 2014, pp. 2672–2680.\\n\\n[GRK20]\\n\\n[GRS18]\\n\\n[GS20]\\n\\nIngo G¨uhring, Mones Raslan, and Gitta Kutyniok, Expressivity of deep neural networks, 2020,\\narXiv preprint arXiv:2007.04759.\\n\\nNoah Golowich, Alexander Rakhlin, and Ohad Shamir, Size-independent sample complexity of\\nneural networks, Conference On Learning Theory, 2018, pp. 297–299.\\n\\nLukas Gonon and Christoph Schwab, Deep ReLU network expression rates for option prices in\\nhigh-dimensional, exponential L´evy models, 2020, ETH Zurich SAM Research Report.\\n\\n66\\n\\n\\x0c[GV21]\\n\\n[GW08]\\n\\n[GZ84]\\n\\n[Han19]\\n\\n[Hau95]\\n\\n[HH19]\\n\\n[HHJ15]\\n\\n[HJE18]\\n\\nPhilipp Grohs and Felix Voigtlaender, Proof of the theory-to-practice gap in deep learning via\\nsampling complexity bounds for neural network approximation spaces, 2021, arXiv preprint\\narXiv:2104.02746.\\n\\nAndreas Griewank and Andrea Walther, Evaluating derivatives: principles and techniques of\\nalgorithmic diﬀerentiation, SIAM, 2008.\\n\\nEvarist Gin´e and Joel Zinn, Some limit theorems for empirical processes, The Annals of\\nProbability (1984), 929–989.\\n\\nBoris Hanin, Universal function approximation by deep neural nets with bounded width and\\nReLU activations, Mathematics 7 (2019), no. 10, 992.\\n\\nDavid Haussler, Sphere packing numbers for subsets of the boolean n-cube with bounded vapnik-\\nchervonenkis dimension, Journal of Combinatorial Theory, Series A 2 (1995), no. 69, 217–232.\\n\\nCatherine F Higham and Desmond J Higham, Deep learning: An introduction for applied\\nmathematicians, SIAM Review 61 (2019), no. 4, 860–891.\\n\\nMartin Hairer, Martin Hutzenthaler, and Arnulf Jentzen, Loss of regularity for Kolmogorov\\nequations, The Annals of Probability 43 (2015), no. 2, 468–527.\\n\\nJiequn Han, Arnulf Jentzen, and Weinan E, Solving high-dimensional partial diﬀerential equations\\nusing deep learning, Proceedings of the National Academy of Sciences 115 (2018), no. 34, 8505–\\n8510.\\n\\n[HJKN20] Martin Hutzenthaler, Arnulf Jentzen, Thomas Kruse, and Tuan Anh Nguyen, A proof that recti-\\nﬁed deep neural networks overcome the curse of dimensionality in the numerical approximation\\nof semilinear heat equations, SN Partial Diﬀerential Equations and Applications 1 (2020), no. 2,\\n1–34.\\n\\n[HLXZ20]\\n\\nJuncai He, Lin Li, Jinchao Xu, and Chunyue Zheng, ReLU deep neural networks and linear\\nﬁnite elements, Journal of Computational Mathematics 38 (2020), no. 3, 502–527.\\n\\n[HMD16]\\n\\nSong Han, Huizi Mao, and William J Dally, Deep compression: Compressing deep neural network\\nwith pruning, trained quantization and Huﬀman coding, International Conference on Learning\\nRepresentations, 2016.\\n\\n[HMRT19] Trevor Hastie, Andrea Montanari, Saharon Rosset, and Ryan J Tibshirani, Surprises in high-\\n\\ndimensional ridgeless least squares interpolation, 2019, arXiv preprint arXiv:1903.08560.\\n\\n[Hoe63]\\n\\n[Hop82]\\n\\n[HR19]\\n\\nWassily Hoeﬀding, Probability inequalities for sums of bounded random variables, Journal of the\\nAmerican Statistical Association 58 (1963), no. 301, 13–30.\\n\\nJohn J Hopﬁeld, Neural networks and physical systems with emergent collective computational\\nabilities, Proceedings of the National Academy of Sciences 79 (1982), no. 8, 2554–2558.\\n\\nBoris Hanin and David Rolnick, Deep ReLU networks have surprisingly few activation patterns,\\nAdvances in Neural Information Processing Systems, 2019, pp. 359–368.\\n\\n[HRS16]\\n\\nMoritz Hardt, Ben Recht, and Yoram Singer, Train faster, generalize better: Stability of stochastic\\ngradient descent, International Conference on Machine Learning, 2016, pp. 1225–1234.\\n\\n[HS97]\\n\\n[HS17]\\n\\nSepp Hochreiter and J¨urgen Schmidhuber, Long short-term memory, Neural Computation 9\\n(1997), no. 8, 1735–1780.\\n\\nBoris Hanin and Mark Sellke, Approximating continuous functions by ReLU nets of minimal\\nwidth, 2017, arXiv preprint arXiv:1710.11278.\\n\\n67\\n\\n\\x0c[HSL+16]\\n\\n[HSN20]\\n\\n[HSW89]\\n\\n[HTF01]\\n\\n[HV17]\\n\\n[HvdG19]\\n\\n[HZ94]\\n\\n[HZE19]\\n\\nGao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and Kilian Q Weinberger, Deep networks with\\nstochastic depth, European Conference on Computer Vision, 2016, pp. 646–661.\\n\\nJan Hermann, Zeno Sch¨atzle, and Frank No´e, Deep-neural-network solution of the electronic\\nSchr¨odinger equation, Nature Chemistry 12 (2020), no. 10, 891–897.\\n\\nKurt Hornik, Maxwell Stinchcombe, and Halbert White, Multilayer feedforward networks are\\nuniversal approximators, Neural Networks 2 (1989), no. 5, 359–366.\\n\\nTrevor Hastie, Robert Tibshirani, and Jerome Friedman, The elements of statistical learning:\\nData mining, inference, and prediction, Springer Series in Statistics, Springer, 2001.\\n\\nBenjamin D Haeﬀele and Ren´e Vidal, Global optimality in neural network training, Proceedings\\nof the IEEE Conference on Computer Vision and Pattern Recognition, 2017, pp. 7331–7339.\\n\\nPeter Hinz and Sara van de Geer, A framework for the construction of upper bounds on the\\nnumber of aﬃne linear regions of ReLU feed-forward neural networks, IEEE Transactions on\\nInformation Theory 65 (2019), 7304–7324.\\n\\nGeoﬀrey E Hinton and Richard S Zemel, Autoencoders, minimum description length, and\\nhelmholtz free energy, Advances in Neural Information Processing Systems 6 (1994), 3–10.\\n\\nJiequn Han, Linfeng Zhang, and Weinan E, Solving many-electron Schr¨odinger equation using\\ndeep neural networks, Journal of Computational Physics 399 (2019), 108929.\\n\\n[HZRS15] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun, Delving deep into rectiﬁers: Sur-\\npassing human-level performance on imagenet classiﬁcation, Proceedings of IEEE International\\nConference on Computer Vision, 2015, pp. 1026–1034.\\n\\n[HZRS16]\\n\\n, Deep residual learning for image recognition, Proceedings of the IEEE Conference on\\n\\nComputer Vision and Pattern Recognition, 2016, pp. 770–778.\\n\\n[IS15]\\n\\n[JGH18]\\n\\nSergey Ioﬀe and Christian Szegedy, Batch normalization: Accelerating deep network training by\\nreducing internal covariate shift, International Conference on Machine Learning, 2015, pp. 448–\\n456.\\n\\nArthur Jacot, Franck Gabriel, and Cl´ement Hongler, Neural tangent kernel: Convergence and\\ngeneralization in neural networks, Advances in Neural Information Processing Systems, 2018,\\npp. 8571–8580.\\n\\n[JKMB19] Yiding Jiang, Dilip Krishnan, Hossein Mobahi, and Samy Bengio, Predicting the generaliza-\\ntion gap in deep networks with margin distributions, International Conference on Learning\\nRepresentations, 2019.\\n\\n[JKNvW20] Arnulf Jentzen, Benno Kuckuck, Ariel Neufeld, and Philippe von Wurstemberger, Strong error\\nanalysis for stochastic gradient descent optimization algorithms, IMA Journal of Numerical\\nAnalysis 41 (2020), no. 1, 455–492.\\n\\n[JMFU17] Kyong Hwan Jin, Michael T McCann, Emmanuel Froustey, and Michael Unser, Deep convolu-\\ntional neural network for inverse problems in imaging, IEEE Transactions on Image Processing\\n26 (2017), no. 9, 4509–4522.\\n\\n[JMS17]\\n\\nBangti Jin, Peter Maaß, and Otmar Scherzer, Sparsity regularization in inverse problems, Inverse\\nProblems 33 (2017), no. 6, 060301.\\n\\n[JNM+20] Yiding Jiang, Behnam Neyshabur, Hossein Mobahi, Dilip Krishnan, and Samy Bengio, Fan-\\ntastic generalization measures and where to ﬁnd them, International Conference on Learning\\nRepresentations, 2020.\\n\\n68\\n\\n\\x0c[Jor90]\\n\\n[JT19a]\\n\\n[JT19b]\\n\\n[JT20]\\n\\nMichael I Jordan, Attractor dynamics and parallelism in a connectionist sequential machine,\\nArtiﬁcial neural networks: concept learning, IEEE Press, 1990, pp. 112–127.\\n\\nZiwei Ji and Matus Telgarsky, Gradient descent aligns the layers of deep linear networks,\\nInternational Conference on Learning Representations, 2019.\\n\\n, A reﬁned primal-dual analysis of the implicit bias, 2019, arXiv preprint arXiv:1906.04540.\\n\\n, Directional convergence and alignment in deep learning, Advances in Neural Information\\n\\nProcessing Systems, 2020, pp. 17176–17186.\\n\\n[Jud90]\\n\\nStephen J Judd, Neural network design and the complexity of learning, MIT Press, 1990.\\n\\n[Kel60]\\n\\nHenry J Kelley, Gradient theory of optimal ﬂight paths, Ars Journal 30 (1960), no. 10, 947–954.\\n\\n[KH09]\\n\\n[KL18]\\n\\n[KL20]\\n\\n[KM97]\\n\\nAlex Krizhevsky and Geoﬀrey Hinton, Learning multiple layers of features from tiny images,\\nTech. report, University of Toronto, 2009.\\n\\nSham M Kakade and Jason D Lee, Provably correct automatic subdiﬀerentiation for qualiﬁed\\nprograms, Advances in Neural Information Processing Systems, 2018, pp. 7125–7135.\\n\\nPatrick Kidger and Terry Lyons, Universal approximation with deep narrow networks, Conference\\non Learning Theory, 2020, pp. 2306–2327.\\n\\nMarek Karpinski and Angus Macintyre, Polynomial bounds for VC dimension of sigmoidal and\\ngeneral Pfaﬃan neural networks, Journal of Computer and System Sciences 54 (1997), no. 1,\\n169–176.\\n\\n[KMN+17] Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping\\nTak Peter Tang, On large-batch training for deep learning: Generalization gap and sharp minima,\\nInternational Conference on Learning Representations, 2017.\\n\\n[KPRS19] Gitta Kutyniok, Philipp Petersen, Mones Raslan, and Reinhold Schneider, A theoretical analysis\\n\\nof deep neural networks and parametric PDEs, 2019, arXiv preprint arXiv:1904.00377.\\n\\n[KSH12]\\n\\nAlex Krizhevsky, Ilya Sutskever, and Geoﬀrey E Hinton, Imagenet classiﬁcation with deep\\nconvolutional neural networks, Advances in Neural Information Processing Systems, 2012,\\npp. 1097–1105.\\n\\n[KW52]\\n\\nJack Kiefer and Jacob Wolfowitz, Stochastic estimation of the maximum of a regression function,\\nThe Annals of Mathematical Statistics 23 (1952), no. 3, 462–466.\\n\\n[LBBH98] Yann LeCun, L´eon Bottou, Yoshua Bengio, and Patrick Haﬀner, Gradient-based learning applied\\nto document recognition, Proceedings of the IEEE 86 (1998), no. 11, 2278–2324.\\n\\n[LBD+89] Yann LeCun, Bernhard Boser, John S Denker, Donnie Henderson, Richard E Howard, Wayne\\nHubbard, and Lawrence D Jackel, Backpropagation applied to handwritten zip code recognition,\\nNeural Computation 1 (1989), no. 4, 541–551.\\n\\n[LBH15]\\n\\nYann LeCun, Yoshua Bengio, and Geoﬀrey Hinton, Deep learning, Nature 521 (2015), no. 7553,\\n436–444.\\n\\n[LBN+18]\\n\\nJaehoon Lee, Yasaman Bahri, Roman Novak, Samuel S Schoenholz, Jeﬀrey Pennington, and\\nJascha Sohl-Dickstein, Deep neural networks as Gaussian processes, International Conference on\\nLearning Representations, 2018.\\n\\n[LC19]\\n\\nGuillaume Lample and Fran¸cois Charton, Deep learning for symbolic mathematics, International\\nConference on Learning Representations, 2019.\\n\\n69\\n\\n\\x0c[LD21]\\n\\n[LDS89]\\n\\n[Lew43]\\n\\n[Li21]\\n\\n[Lin70]\\n\\n[LL18]\\n\\nLicong Lin and Edgar Dobriban, What causes the test error? Going beyond bias-variance via\\nANOVA, Journal of Machine Learning Research 22 (2021), no. 155, 1–82.\\n\\nYann LeCun, John S Denker, and Sara A Solla, Optimal brain damage, Advances in Neural\\nInformation Processing Systems, 1989, pp. 598–605.\\n\\nKurt Lewin, Psychology and the process of group living, The Journal of Social Psychology 17\\n(1943), no. 1, 113–131.\\n\\nWeilin Li, Generalization error of minimum weighted norm and kernel interpolation, SIAM\\nJournal on Mathematics of Data Science 3 (2021), no. 1, 414–438.\\n\\nSeppo Linnainmaa, Alogritmin kumulatiivinen py¨oristysvirhe yksitt¨aisten py¨oristysvirheiden\\nTaylor-kehitelm¨an¨a, Master’s thesis, University of Helsinki, 1970.\\n\\nYuanzhi Li and Yingyu Liang, Learning overparameterized neural networks via stochastic\\ngradient descent on structured data, Advances in Neural Information Processing Systems, 2018,\\npp. 8157–8166.\\n\\n[LL19]\\n\\nKaifeng Lyu and Jian Li, Gradient descent maximizes the margin of homogeneous neural\\nnetworks, International Conference on Learning Representations, 2019.\\n\\n[LLPS93] Moshe Leshno, Vladimir Ya Lin, Allan Pinkus, and Shimon Schocken, Multilayer feedforward\\nnetworks with a nonpolynomial activation function can approximate any function, Neural\\nNetworks 6 (1993), no. 6, 861–867.\\n\\n[LLS19]\\n\\nQianxiao Li, Ting Lin, and Zuowei Shen, Deep learning via dynamical systems: An approximation\\nperspective, 2019, arXiv preprint arXiv:1912.10382.\\n\\n[LML+20] Yiping Lu, Chao Ma, Yulong Lu, Jianfeng Lu, and Lexing Ying, A mean ﬁeld analysis of\\ndeep ResNet and beyond: Towards provably optimization via overparameterization from depth,\\nInternational Conference on Machine Learning, 2020, pp. 6426–6436.\\n\\n[L ¨OS18]\\n\\n[LP21]\\n\\nSebastian Lunz, Ozan ¨Oktem, and Carola-Bibiane Sch¨onlieb, Adversarial regularizers in inverse\\nproblems, Advances in Neural Information Processing Systems, 2018, pp. 8507–8516.\\n\\nFabian Laakmann and Philipp Petersen, Eﬃcient approximation of solutions of parametric\\nlinear transport equations by ReLU DNNs, Advances in Computational Mathematics 47 (2021),\\nno. 1, 1–32.\\n\\n[LPRS19]\\n\\nTengyuan Liang, Tomaso Poggio, Alexander Rakhlin, and James Stokes, Fisher–Rao metric,\\ngeometry, and complexity of neural networks, International Conference on Artiﬁcial Intelligence\\nand Statistics, 2019, pp. 888–896.\\n\\n[LR20]\\n\\n[LRZ20]\\n\\nTengyuan Liang and Alexander Rakhlin, Just interpolate: Kernel “ridgeless” regression can\\ngeneralize, The Annals of Statistics 48 (2020), no. 3, 1329–1347.\\n\\nTengyuan Liang, Alexander Rakhlin, and Xiyu Zhai, On the multiple descent of minimum-norm\\ninterpolants and restricted lower isometry of kernels, Conference on Learning Theory, 2020,\\npp. 2683–2711.\\n\\n[LS17]\\n\\nShiyu Liang and R Srikant, Why deep neural networks for function approximation?, International\\nConference on Learning Representations, 2017.\\n\\n[LSAH20]\\n\\nHousen Li, Johannes Schwab, Stephan Antholzer, and Markus Haltmeier, NETT: Solving inverse\\nproblems with deep neural networks, Inverse Problems 36 (2020), no. 6, 065005.\\n\\n70\\n\\n\\x0c[LSJR16]\\n\\nJason D Lee, Max Simchowitz, Michael I Jordan, and Benjamin Recht, Gradient descent only\\nconverges to minimizers, Conference on Learning Theory, 2016, pp. 1246–1257.\\n\\n[LT91]\\n\\n[LTY19]\\n\\n[LXS+20]\\n\\n[Mal12]\\n\\n[Mal16]\\n\\nMichel Ledoux and Michel Talagrand, Probability in Banach spaces: Isoperimetry and processes,\\nvol. 23, Springer Science & Business Media, 1991.\\n\\nBo Li, Shanshan Tang, and Haijun Yu, Better approximations of high dimensional smooth\\nfunctions by deep neural networks with rectiﬁed power units, Communications in Computational\\nPhysics 27 (2019), no. 2, 379–411.\\n\\nJaehoon Lee, Lechao Xiao, Samuel S Schoenholz, Yasaman Bahri, Roman Novak, Jascha Sohl-\\nDickstein, and Jeﬀrey Pennington, Wide neural networks of any depth evolve as linear models\\nunder gradient descent, Journal of Statistical Mechanics: Theory and Experiment 2020 (2020),\\nno. 12, 124002.\\n\\nSt´ephane Mallat, Group invariant scattering, Communications on Pure and Applied Mathematics\\n65 (2012), no. 10, 1331–1398.\\n\\n, Understanding deep convolutional networks, Philosophical Transactions of the Royal\\nSociety A: Mathematical, Physical and Engineering Sciences 374 (2016), no. 2065, 20150203.\\n\\n[MAV18]\\n\\nPoorya Mianjy, Raman Arora, and Rene Vidal, On the implicit bias of dropout, International\\nConference on Machine Learning, 2018, pp. 3540–3548.\\n\\n[McA99]\\n\\n[McD89]\\n\\n[Men14]\\n\\n[Mha96]\\n\\nDavid A McAllester, Pac-bayesian model averaging, Conference on Learning Theory, 1999,\\npp. 164–170.\\n\\nColin McDiarmid, On the method of bounded diﬀerences, Surveys in Combinatorics 141 (1989),\\nno. 1, 148–188.\\n\\nShahar Mendelson, Learning without concentration, Conference on Learning Theory, 2014,\\npp. 25–39.\\n\\nHrushikesh N Mhaskar, Neural networks for optimal approximation of smooth and analytic\\nfunctions, Neural Computation 8 (1996), no. 1, 164–177.\\n\\n[MHR+18] Alexander G de G Matthews, Jiri Hron, Mark Rowland, Richard E Turner, and Zoubin\\nGhahramani, Gaussian process behaviour in wide deep neural networks, International Conference\\non Learning Representations, 2018.\\n\\n[MKS+13] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan\\nWierstra, and Martin Riedmiller, Playing atari with deep reinforcement learning, 2013, arXiv\\npreprint arXiv:1312.5602.\\n\\n[MLE21]\\n\\nVishal Monga, Yuelong Li, and Yonina C Eldar, Algorithm unrolling: Interpretable, eﬃcient\\ndeep learning for signal and image processing, IEEE Signal Processing Magazine 38 (2021),\\nno. 2, 18–44.\\n\\n[MM19]\\n\\nSong Mei and Andrea Montanari, The generalization error of random features regression: Precise\\nasymptotics and double descent curve, 2019, arXiv preprint arXiv:1908.05355.\\n\\n[MOPS20] Carlo Marcati, Joost Opschoor, Philipp Petersen, and Christoph Schwab, Exponential ReLU\\nneural network approximation rates for point and edge singularities, 2020, ETH Zurich SAM\\nResearch Report.\\n\\n[MP43]\\n\\nWarren S McCulloch and Walter Pitts, A logical calculus of the ideas immanent in nervous\\nactivity, The Bulletin of Mathematical Biophysics 5 (1943), no. 4, 115–133.\\n\\n71\\n\\n\\x0c[MP69]\\n\\nMarvin Minsky and Seymour A Papert, Perceptrons, MIT Press, 1969.\\n\\n[MP99]\\n\\nVitaly Maiorov and Allan Pinkus, Lower bounds for approximation by MLP neural networks,\\nNeurocomputing 25 (1999), no. 1-3, 81–91.\\n\\n[MPCB14] Guido Mont´ufar, Razvan Pascanu, Kyunghyun Cho, and Yoshua Bengio, On the number of\\nlinear regions of deep neural networks, Advances in Neural Information Processing Systems,\\n2014, pp. 2924–2932.\\n\\n[MSL+15]\\n\\nJunshui Ma, Robert P Sheridan, Andy Liaw, George E Dahl, and Vladimir Svetnik, Deep\\nneural nets as a method for quantitative structure–activity relationships, Journal of chemical\\ninformation and modeling 55 (2015), no. 2, 263–274.\\n\\n[MV03]\\n\\nShahar Mendelson and Roman Vershynin, Entropy and the combinatorial dimension, Inventiones\\nmathematicae 152 (2003), no. 1, 37–55.\\n\\n[MVSS20] Vidya Muthukumar, Kailas Vodrahalli, Vignesh Subramanian, and Anant Sahai, Harmless\\ninterpolation of noisy data in regression, IEEE Journal on Selected Areas in Information Theory\\n1 (2020), no. 1, 67–83.\\n\\n[MZ20]\\n\\nAndrea Montanari and Yiqiao Zhong, The interpolation phase transition in neural networks:\\nMemorization and generalization under lazy training, 2020, arXiv preprint arXiv:2007.12826.\\n\\n[NBMS17] Behnam Neyshabur, Srinadh Bhojanapalli, David McAllester, and Nati Srebro, Exploring\\ngeneralization in deep learning, Advances in Neural Information Processing Systems, 2017,\\npp. 5947–5956.\\n\\n[NBS18]\\n\\n[NH17]\\n\\n[NI20]\\n\\n[NJLS09]\\n\\n[NK19]\\n\\nBehnam Neyshabur, Srinadh Bhojanapalli, and Nathan Srebro, A PAC-Bayesian approach to\\nspectrally-normalized margin bounds for neural networks, International Conference on Learning\\nRepresentations, 2018.\\n\\nQuynh Nguyen and Matthias Hein, The loss surface of deep and wide neural networks, Interna-\\ntional Conference on Machine Learning, 2017, pp. 2603–2612.\\n\\nRyumei Nakada and Masaaki Imaizumi, Adaptive approximation and generalization of deep\\nneural network with intrinsic dimensionality, Journal of Machine Learning Research 21 (2020),\\nno. 174, 1–38.\\n\\nArkadi Nemirovski, Anatoli Juditsky, Guanghui Lan, and Alexander Shapiro, Robust stochastic\\napproximation approach to stochastic programming, SIAM Journal on Optimization 19 (2009),\\nno. 4, 1574–1609.\\n\\nVaishnavh Nagarajan and J Zico Kolter, Uniform convergence may be unable to explain general-\\nization in deep learning, Advances in Neural Information Processing Systems, 2019, pp. 11615–\\n11626.\\n\\n[NKB+20] Preetum Nakkiran, Gal Kaplun, Yamini Bansal, Tristan Yang, Boaz Barak, and Ilya Sutskever,\\nDeep double descent: Where bigger models and more data hurt, International Conference on\\nLearning Representations, 2020.\\n\\n[NLG+19] Mor Shpigel Nacson, Jason D Lee, Suriya Gunasekar, Pedro Henrique Pamplona Savarese, Nathan\\nSrebro, and Daniel Soudry, Convergence of gradient descent on separable data, International\\nConference on Artiﬁcial Intelligence and Statistics, 2019, pp. 3420–3428.\\n\\n[NTS14]\\n\\nBehnam Neyshabur, Ryota Tomioka, and Nathan Srebro, In search of the real inductive bias:\\nOn the role of implicit regularization in deep learning, 2014, arXiv preprint arXiv:1412.6614.\\n\\n72\\n\\n\\x0c[NTS15]\\n\\n, Norm-based capacity control in neural networks, Conference on Learning Theory, 2015,\\n\\npp. 1376–1401.\\n\\n[NW09]\\n\\n[NY83]\\n\\nErich Novak and Henryk Wo´zniakowski, Approximation of inﬁnitely diﬀerentiable multivariate\\nfunctions is intractable, Journal of Complexity 25 (2009), no. 4, 398–404.\\n\\nArkadi Semenovich Nemirovsky and David Borisovich Yudin, Problem complexity and method\\neﬃciency in optimization, Wiley-Interscience Series in Discrete Mathematics, Wiley, 1983.\\n\\n[NZGK21] Tenavi Nakamura-Zimmerer, Qi Gong, and Wei Kang, Adaptive deep learning for high-\\ndimensional Hamilton–Jacobi–Bellman Equations, SIAM Journal on Scientiﬁc Computing 43\\n(2021), no. 2, A1221–A1247.\\n\\n[OF96]\\n\\nBruno A Olshausen and David J Field, Sparse coding of natural images produces localized,\\noriented, bandpass receptive ﬁelds, Nature 381 (1996), no. 60, 609.\\n\\n[OM98]\\n\\nGenevieve B Orr and Klaus-Robert M¨uller, Neural networks: tricks of the trade, Springer, 1998.\\n\\n[OPS20]\\n\\n[OS19]\\n\\nJoost Opschoor, Philipp Petersen, and Christoph Schwab, Deep ReLU networks and high-order\\nﬁnite element methods, Analysis and Applications (2020), no. 0, 1–56.\\n\\nKenta Oono and Taiji Suzuki, Approximation and non-parametric estimation of ResNet-type\\nconvolutional neural networks, International Conference on Machine Learning, 2019, pp. 4922–\\n4931.\\n\\n[PGZ+18] Hieu Pham, Melody Guan, Barret Zoph, Quoc Le, and Jeﬀ Dean, Eﬃcient neural architecture\\nsearch via parameters sharing, International Conference on Machine Learning, 2018, pp. 4095–\\n4104.\\n\\n[PKL+17] Tomaso Poggio, Kenji Kawaguchi, Qianli Liao, Brando Miranda, Lorenzo Rosasco, Xavier\\nBoix, Jack Hidary, and Hrushikesh N Mhaskar, Theory of deep learning III: explaining the\\nnon-overﬁtting puzzle, 2017, arXiv preprint arXiv:1801.00173.\\n\\n[PLR+16]\\n\\nBen Poole, Subhaneil Lahiri, Maithra Raghu, Jascha Sohl-Dickstein, and Surya Ganguli,\\nExponential expressivity in deep neural networks through transient chaos, Advances in Neural\\nInformation Processing Systems, 2016, pp. 3368–3376.\\n\\n[PMR+17] Tomaso Poggio, Hrushikesh N Mhaskar, Lorenzo Rosasco, Brando Miranda, and Qianli Liao,\\nWhy and when can deep-but not shallow-networks avoid the curse of dimensionality: a review,\\nInternational Journal of Automation and Computing 14 (2017), no. 5, 503–519.\\n\\n[PP92]\\n\\nEtienne Pardoux and Shige Peng, Backward stochastic diﬀerential equations and quasilin-\\near parabolic partial diﬀerential equations, Stochastic partial diﬀerential equations and their\\napplications, Springer, 1992, pp. 200–217.\\n\\n[PRE17]\\n\\nVardan Papyan, Yaniv Romano, and Michael Elad, Convolutional neural networks analyzed via\\nconvolutional sparse coding, Journal of Machine Learning Research 18 (2017), no. 1, 2887–2938.\\n\\n[PRMN04] Tomaso Poggio, Ryan Rifkin, Sayan Mukherjee, and Partha Niyogi, General conditions for\\n\\npredictivity in learning theory, Nature 428 (2004), no. 6981, 419–422.\\n\\n[PRSE18]\\n\\n[PRV20]\\n\\nVardan Papyan, Yaniv Romano, Jeremias Sulam, and Michael Elad, Theoretical foundations\\nof deep learning via sparse representations: A multilayer sparse model and its connection to\\nconvolutional neural networks, IEEE Signal Processing Magazine 35 (2018), no. 4, 72–89.\\n\\nPhilipp Petersen, Mones Raslan, and Felix Voigtlaender, Topological properties of the set of\\nfunctions generated by neural networks of ﬁxed size, Foundations of Computational Mathematics\\n(2020), 1–70.\\n\\n73\\n\\n\\x0c[PSE17]\\n\\nVardan Papyan, Jeremias Sulam, and Michael Elad, Working locally thinking globally: Theoretical\\nguarantees for convolutional sparse coding, IEEE Transactions on Signal Processing 65 (2017),\\nno. 21, 5687–5701.\\n\\n[PSMF20] David Pfau, James S Spencer, Alexander GDG Matthews, and W Matthew C Foulkes, Ab initio\\nsolution of the many-electron schr¨odinger equation with deep neural networks, Physical Review\\nResearch 2 (2020), no. 3, 033429.\\n\\n[PV18]\\n\\nPhilipp Petersen and Felix Voigtlaender, Optimal approximation of piecewise smooth functions\\nusing deep ReLU neural networks, Neural Networks 108 (2018), 296–330.\\n\\n[PV20]\\n\\n, Equivalence of approximation by convolutional neural networks and fully-connected\\n\\nnetworks, Proceedings of the American Mathematical Society 148 (2020), no. 4, 1567–1581.\\n\\n[REM17]\\n\\nYaniv Romano, Michael Elad, and Peyman Milanfar, The little engine that could: Regularization\\nby denoising (red), SIAM Journal on Imaging Sciences 10 (2017), no. 4, 1804–1844.\\n\\n[RFB15]\\n\\nOlaf Ronneberger, Philipp Fischer, and Thomas Brox, U-net: Convolutional networks for\\nbiomedical image segmentation, International Conference on Medical image computing and\\ncomputer-assisted intervention, 2015, pp. 234–241.\\n\\n[RH19]\\n\\nLars Ruthotto and Eldad Haber, Deep neural networks motivated by partial diﬀerential equations,\\nJournal of Mathematical Imaging and Vision (2019), 1–13.\\n\\n[RHW86]\\n\\nDavid E Rumelhart, Geoﬀrey E Hinton, and Ronald J Williams, Learning representations by\\nback-propagating errors, Nature 323 (1986), no. 6088, 533–536.\\n\\n[RM51]\\n\\nHerbert Robbins and Sutton Monro, A stochastic approximation method, The Annals of Mathe-\\nmatical Statistics (1951), 400–407.\\n\\n[RMD+06] P L´opez R´ıos, Ao Ma, Neil D Drummond, Michael D Towler, and Richard J Needs, Inhomoge-\\nneous backﬂow transformations in quantum Monte Carlo calculations, Physical Review E 74\\n(2006), no. 6, 066701.\\n\\n[Ros58]\\n\\nFrank Rosenblatt, The perceptron: a probabilistic model for information storage and organization\\nin the brain, Psychological review 65 (1958), no. 6, 386.\\n\\n[RPK+17] Maithra Raghu, Ben Poole, Jon Kleinberg, Surya Ganguli, and Jascha Sohl-Dickstein, On the\\nexpressive power of deep neural networks, International Conference on Machine Learning, 2017,\\npp. 2847–2854.\\n\\n[RPK19]\\n\\n[RR+07]\\n\\n[Rud06]\\n\\nMaziar Raissi, Paris Perdikaris, and George E Karniadakis, Physics-informed neural networks:\\nA deep learning framework for solving forward and inverse problems involving nonlinear partial\\ndiﬀerential equations, Journal of Computational Physics 378 (2019), 686–707.\\n\\nAli Rahimi, Benjamin Recht, et al., Random features for large-scale kernel machines, Advances\\nin Neural Information Processing Systems, 2007, pp. 1177–1184.\\n\\nWalter Rudin, Real and complex analysis, McGraw-Hill Series in Higher Mathematics, Tata\\nMcGraw-Hill, 2006.\\n\\n[RWK+20] Vivek Ramanujan, Mitchell Wortsman, Aniruddha Kembhavi, Ali Farhadi, and Mohammad\\nRastegari, What’s hidden in a randomly weighted neural network?, Proceedings of the IEEE\\nConference on Computer Vision and Pattern Recognition, 2020, pp. 11893–11902.\\n\\n[Sak99]\\n\\nAkito Sakurai, Tight bounds for the VC-dimension of piecewise polynomial networks, Advances\\nin Neural Information Processing Systems, 1999, pp. 323–329.\\n\\n74\\n\\n\\x0c[SCC18]\\n\\n[Sch15]\\n\\n[SDR14]\\n\\n[SEJ+20]\\n\\nUri Shaham, Alexander Cloninger, and Ronald R Coifman, Provable approximation properties\\nfor deep neural networks, Applied and Computational Harmonic Analysis 44 (2018), no. 3,\\n537–557.\\n\\nJ¨urgen Schmidhuber, Deep learning in neural networks: An overview, Neural Networks 61\\n(2015), 85–117.\\n\\nAlexander Shapiro, Darinka Dentcheva, and Andrzej Ruszczy´nski, Lectures on stochastic pro-\\ngramming: modeling and theory, SIAM, 2014.\\n\\nAndrew W Senior, Richard Evans, John Jumper, James Kirkpatrick, Laurent Sifre, Tim Green,\\nChongli Qin, Augustin ˇZ´ıdek, Alexander WR Nelson, and Alex Bridgland, Improved protein\\nstructure prediction using potentials from deep learning, Nature 577 (2020), no. 7792, 706–710.\\n\\n[SGHK18] David Saxton, Edward Grefenstette, Felix Hill, and Pushmeet Kohli, Analysing mathematical\\nreasoning abilities of neural models, International Conference on Learning Representations, 2018.\\n\\n[SGS15]\\n\\n[SH19]\\n\\n[She20]\\n\\nRupesh Kumar Srivastava, Klaus Greﬀ, and J¨urgen Schmidhuber, Training very deep networks,\\nAdvances in Neural Information Processing Systems, 2015, pp. 2377–2385.\\n\\nJohannes Schmidt-Hieber, Deep ReLU network approximation of functions on a manifold, 2019,\\narXiv preprint arXiv:1908.00695.\\n\\nZuowei Shen, Deep network approximation characterized by number of neurons, Communications\\nin Computational Physics 28 (2020), no. 5, 1768–1811.\\n\\n[SHK+14] Nitish Srivastava, Geoﬀrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov,\\nDropout: a simple way to prevent neural networks from overﬁtting, Journal of Machine Learning\\nResearch 15 (2014), no. 1, 1929–1958.\\n\\n[SHM+16] David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driess-\\nche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, and Marc Lanctot, Mas-\\ntering the game of go with deep neural networks and tree search, Nature 529 (2016), no. 7587,\\n484–489.\\n\\n[SHN+18] Daniel Soudry, Elad Hoﬀer, Mor Shpigel Nacson, Suriya Gunasekar, and Nathan Srebro, The\\n\\nimplicit bias of gradient descent on separable data, 2018.\\n\\n[ˇS´ım02]\\n\\n[SKS+17]\\n\\n[SLJ+15]\\n\\nJiˇr´ı ˇS´ıma, Training a single sigmoidal neuron is hard, Neural Computation 14 (2002), no. 11,\\n2709–2728.\\n\\nKristof T Sch¨utt, Pieter-Jan Kindermans, Huziel E Sauceda, Stefan Chmiela, Alexandre\\nTkatchenko, and Klaus-Robert M¨uller, Schnet: A continuous-ﬁlter convolutional neural network\\nfor modeling quantum interactions, Advances in Neural Information Processing Systems, 2017,\\npp. 992–1002.\\n\\nChristian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov,\\nDumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich, Going deeper with convolutions,\\nProceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2015, pp. 1–9.\\n\\n[SO12]\\n\\nAttila Szabo and Neil S Ostlund, Modern quantum chemistry: introduction to advanced electronic\\nstructure theory, Courier Corporation, 2012.\\n\\n[SPRE18]\\n\\nJeremias Sulam, Vardan Papyan, Yaniv Romano, and Michael Elad, Multilayer convolutional\\nsparse modeling: Pursuit and dictionary learning, IEEE Transactions on Signal Processing 66\\n(2018), no. 15, 4090–4104.\\n\\n75\\n\\n\\x0c[SRG+21] Michael Scherbela, Rafael Reisenhofer, Leon Gerard, Philipp Marquetand, and Philipp Grohs,\\nSolving the electronic Schr¨odinger equation for multiple nuclear geometries with weight-sharing\\ndeep neural networks, 2021, arXiv preprint arXiv:2105.08351.\\n\\n[SS16]\\n\\n[SS17]\\n\\n[SS18]\\n\\nItay Safran and Ohad Shamir, On the quality of the initial basin in overspeciﬁed neural networks,\\nInternational Conference on Machine Learning, 2016, pp. 774–782.\\n\\n, Depth-width tradeoﬀs in approximating natural functions with neural networks, Interna-\\n\\ntional Conference on Machine Learning, 2017, pp. 2979–2987.\\n\\n, Spurious local minima are common in two-layer ReLU neural networks, International\\n\\nConference on Machine Learning, 2018, pp. 4433–4441.\\n\\n[SSBD14]\\n\\nShai Shalev-Shwartz and Shai Ben-David, Understanding machine learning: From theory to\\nalgorithms, Cambridge University Press, 2014.\\n\\n[SSS+17]\\n\\nDavid Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur\\nGuez, Thomas Hubert, Lucas Baker, Matthew Lai, and Adrian Bolton, Mastering the game of\\ngo without human knowledge, Nature 550 (2017), no. 7676, 354–359.\\n\\n[SSSSS09]\\n\\nShai Shalev-Shwartz, Ohad Shamir, Nathan Srebro, and Karthik Sridharan, Stochastic convex\\noptimization, Conference on Learning Theory, 2009.\\n\\n[STIM18]\\n\\nShibani Santurkar, Dimitris Tsipras, Andrew Ilyas, and Aleksander Madry, How does batch\\nnormalization help optimization?, Advances in Neural Information Processing Systems, 2018,\\npp. 2488–2498.\\n\\n[SZ19]\\n\\n[Tal94]\\n\\n[Tel15]\\n\\n[TvG18]\\n\\n[UVL18]\\n\\n[Vap99]\\n\\n[Vap13]\\n\\n[VBB19]\\n\\nChristoph Schwab and Jakob Zech, Deep learning in high dimension: Neural network expression\\nrates for generalized polynomial chaos expansions in uq, Analysis and Applications 17 (2019),\\nno. 01, 19–55.\\n\\nMichel Talagrand, Sharper bounds for Gaussian and empirical processes, The Annals of Proba-\\nbility (1994), 28–76.\\n\\nMatus Telgarsky, Representation beneﬁts of deep feedforward networks, 2015, arXiv preprint\\narXiv:1509.08101.\\n\\nMatthew Thorpe and Yves van Gennip, Deep limits of residual neural networks, 2018, arXiv\\npreprint arXiv:1810.11741.\\n\\nDmitry Ulyanov, Andrea Vedaldi, and Victor Lempitsky, Deep image prior, Proceedings of the\\nIEEE Conference on Computer Vision and Pattern Recognition, 2018, pp. 9446–9454.\\n\\nVladimir Vapnik, An overview of statistical learning theory, IEEE Transactions on Neural\\nNetworks 10 (1999), no. 5, 988–999.\\n\\n, The nature of statistical learning theory, Springer science & business media, 2013.\\n\\nLuca Venturi, Afonso S Bandeira, and Joan Bruna, Spurious valleys in one-hidden-layer neural\\nnetwork optimization landscapes, Journal of Machine Learning Research 20 (2019), no. 133,\\n1–34.\\n\\n[VBC+19] Oriol Vinyals, Igor Babuschkin, Wojciech M Czarnecki, Micha¨el Mathieu, Andrew Dudzik,\\nJunyoung Chung, David H Choi, Richard Powell, Timo Ewalds, and Petko Georgiev, Grandmaster\\nlevel in StarCraft II using multi-agent reinforcement learning, Nature 575 (2019), no. 7782,\\n350–354.\\n\\n76\\n\\n\\x0c[VC71]\\n\\nVladimir Vapnik and Alexey Chervonenkis, On the uniform convergence of relative frequencies of\\nevents to their probabilities, Theory of Probability & Its Applications 16 (1971), no. 2, 264–280.\\n\\n[vdVW97] Aad W van der Vaart and Jon A Wellner, Weak convergence and empirical processes with\\napplications to statistics, Journal of the Royal Statistical Society-Series A Statistics in Society\\n160 (1997), no. 3, 596–608.\\n\\n[Ver18]\\n\\nRoman Vershynin, High-dimensional probability: An introduction with applications in data\\nscience, vol. 47, Cambridge University Press, 2018.\\n\\n[VSP+17]\\n\\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\\n(cid:32)Lukasz Kaiser, and Illia Polosukhin, Attention is all you need, Advances in Neural Information\\nProcessing Systems, 2017, pp. 5998–6008.\\n\\n[Wer88]\\n\\nPaul J Werbos, Generalization of backpropagation with application to a recurrent gas market\\nmodel, Neural Networks 1 (1988), no. 4, 339–356.\\n\\n[WGB17]\\n\\nThomas Wiatowski, Philipp Grohs, and Helmut B¨olcskei, Energy propagation in deep convolu-\\ntional neural networks, IEEE Transactions on Information Theory 64 (2017), no. 7, 4819–4842.\\n\\n[Whi34]\\n\\nHassler Whitney, Analytic extensions of diﬀerentiable functions deﬁned in closed sets, Transac-\\ntions of the American Mathematical Society 36 (1934), no. 1, 63–89.\\n\\n[WPC+21] Zonghan Wu, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi Zhang, and S Yu Philip, A\\ncomprehensive survey on graph neural networks, IEEE Transactions on Neural Networks and\\nLearning Systems 32 (2021), no. 1, 4–24.\\n\\n[WZ95]\\n\\nRonald J Williams and David Zipser, Gradient-based learning algorithms for recurrent, Back-\\npropagation: Theory, Architectures, and Applications 433 (1995), 17.\\n\\n[WZZ+13]\\n\\nLi Wan, Matthew Zeiler, Sixin Zhang, Yann Le Cun, and Rob Fergus, Regularization of neural\\nnetworks using dropconnect, International Conference on Machine Learning, 2013, pp. 1058–1066.\\n\\n[XM12]\\n\\n[Yan19]\\n\\nHuan Xu and Shie Mannor, Robustness and generalization, Machine learning 86 (2012), no. 3,\\n391–423.\\n\\nGreg Yang, Scaling limits of wide neural networks with weight sharing: Gaussian process\\nbehavior, gradient independence, and neural tangent kernel derivation, 2019, arXiv preprint\\narXiv:1902.04760.\\n\\n[Yar17]\\n\\nDmitry Yarotsky, Error bounds for approximations with deep ReLU networks, Neural Networks\\n94 (2017), 103–114.\\n\\n[Yar18a]\\n\\n, Optimal approximation of continuous functions by very deep ReLU networks, Conference\\n\\non Learning Theory, 2018, pp. 639–649.\\n\\n[Yar18b]\\n\\n, Universal approximations of invariant maps by neural networks, 2018, arXiv preprint\\n\\narXiv:1804.10306.\\n\\n[Yar21]\\n\\n, Elementary superexpressive activations, 2021, arXiv preprint arXiv:2102.10911.\\n\\n[YGLD17] Rujie Yin, Tingran Gao, Yue M Lu, and Ingrid Daubechies, A tale of two bases: Local-nonlocal\\nregularization on image patches with convolution framelets, SIAM Journal on Imaging Sciences\\n10 (2017), no. 2, 711–750.\\n\\n[YHC18]\\n\\nJong Chul Ye, Yoseob Han, and Eunju Cha, Deep convolutional framelets: A general deep\\nlearning framework for inverse problems, SIAM Journal on Imaging Sciences 11 (2018), no. 2,\\n991–1048.\\n\\n77\\n\\n\\x0c[YHPC18] Tom Young, Devamanyu Hazarika, Soujanya Poria, and Erik Cambria, Recent trends in deep\\nlearning based natural language processing, IEEE Computational Intelligence Magazine 13 (2018),\\nno. 3, 55–75.\\n\\n[Yse10]\\n\\nHarry Yserentant, Regularity and approximability of electronic wave functions, Springer, 2010.\\n\\n[YZ20]\\n\\n[ZAP16]\\n\\n[Zas75]\\n\\n[ZBH+17]\\n\\n[ZBH+20]\\n\\nDmitry Yarotsky and Anton Zhevnerchuk, The phase diagram of approximation rates for deep\\nneural networks, Advances in Neural Information Processing Systems, vol. 33, 2020.\\n\\nHao Zhou, Jose M Alvarez, and Fatih Porikli, Less is more: Towards compact CNNs, European\\nConference on Computer Vision, 2016, pp. 662–677.\\n\\nThomas Zaslavsky, Facing up to arrangements: Face-count formulas for partitions of space\\nby hyperplanes: Face-count formulas for partitions of space by hyperplanes, Memoirs of the\\nAmerican Mathematical Society, American Mathematical Society, 1975.\\n\\nChiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals, Under-\\nstanding deep learning requires rethinking generalization, International Conference on Learning\\nRepresentations, 2017.\\n\\nChiyuan Zhang, Samy Bengio, Moritz Hardt, Michael C Mozer, and Yoram Singer, Identity\\ncrisis: Memorization and generalization under extreme overparameterization, International\\nConference on Learning Representations, 2020.\\n\\n[ZBS19]\\n\\nChiyuan Zhang, Samy Bengio, and Yoram Singer, Are all layers created equal?, 2019, arXiv\\npreprint arXiv:1902.01996.\\n\\n[ZCZG20] Difan Zou, Yuan Cao, Dongruo Zhou, and Quanquan Gu, Gradient descent optimizes over-\\nparameterized deep ReLU networks, Machine Learning 109 (2020), no. 3, 467–492.\\n\\n[Zho20a]\\n\\nDing-Xuan Zhou, Theory of deep convolutional neural networks: Downsampling, Neural Networks\\n124 (2020), 319–327.\\n\\n[Zho20b]\\n\\n, Universality of deep convolutional neural networks, Applied and Computational Har-\\n\\nmonic Analysis 48 (2020), no. 2, 787–794.\\n\\n[ZKS+18]\\n\\nJure Zbontar, Florian Knoll, Anuroop Sriram, Tullie Murrell, Zhengnan Huang, Matthew J\\nMuckley, Aaron Defazio, Ruben Stern, Patricia Johnson, Mary Bruno, Marc Parente, Krzysztof J\\nGeras, Joe Katsnelson, Hersh Chandarana, Zizhao Zhang, Michal Drozdzal, Adriana Romero,\\nMichael Rabbat, Pascal Vincent, Naﬁssa Yakubova, James Pinkerton, Duo Wang, Erich Owens,\\nC Lawrence Zitnick, Michael P Recht, Daniel K Sodickson, and Yvonne W Lui, fastMRI: An\\nopen dataset and benchmarks for accelerated MRI, 2018, arXiv preprint arXiv:1811.08839.\\n\\n[ZL17]\\n\\nBarret Zoph and Quoc V Le, Neural architecture search with reinforcement learning, International\\nConference on Learning Representations, 2017.\\n\\n78\\n\\n'},\n",
       " {'title': 'Learn to Accumulate Evidence from All Training Samples: Theory and Practice',\n",
       "  'authors': ['Deep Pandey', 'Qi Yu'],\n",
       "  'published': '2023-06-19',\n",
       "  'abstract': 'Evidential deep learning, built upon belief theory and subjective logic, offers a principled and computationally efficient way to turn a deterministic neural network uncertainty-aware. The resultant evidential models can quantify fine-grained uncertainty using the learned evidence. To ensure theoretically sound evidential models, the evidence needs to be non-negative, which requires special activation functions for model training and inference. This constraint often leads to inferior predictive performance compared to standard softmax models, making it challenging to extend them to many large-scale datasets. To unveil the real cause of this undesired behavior, we theoretically investigate evidential models and identify a fundamental limitation that explains the inferior performance: existing evidential activation functions create zero evidence regions, which prevent the model to learn from training samples falling into such regions. A deeper analysis of evidential activation functions based on our theoretical underpinning inspires the design of a novel regularizer that effectively alleviates this fundamental limitation. Extensive experiments over many challenging real-world datasets and settings confirm our theoretical findings and demonstrate the effectiveness of our proposed approach.',\n",
       "  'full_text': '3\\n2\\n0\\n2\\n\\nn\\nu\\nJ\\n\\n4\\n2\\n\\n]\\n\\nG\\nL\\n.\\ns\\nc\\n[\\n\\n2\\nv\\n3\\n1\\n1\\n1\\n1\\n.\\n6\\n0\\n3\\n2\\n:\\nv\\ni\\nX\\nr\\na\\n\\nLearn to Accumulate Evidence from All Training Samples: Theory and Practice\\n\\nDeep Pandey 1 Qi Yu 1\\n\\nAbstract\\nEvidential deep learning, built upon belief the-\\nory and subjective logic, offers a principled and\\ncomputationally efficient way to turn a determin-\\nistic neural network uncertainty-aware. The resul-\\ntant evidential models can quantify fine-grained\\nuncertainty using the learned evidence. To en-\\nsure theoretically sound evidential models, the ev-\\nidence needs to be non-negative, which requires\\nspecial activation functions for model training\\nand inference. This constraint often leads to infe-\\nrior predictive performance compared to standard\\nsoftmax models, making it challenging to extend\\nthem to many large-scale datasets. To unveil the\\nreal cause of this undesired behavior, we theoreti-\\ncally investigate evidential models and identify a\\nfundamental limitation that explains the inferior\\nperformance: existing evidential activation func-\\ntions create zero evidence regions, which prevent\\nthe model to learn from training samples falling\\ninto such regions. A deeper analysis of eviden-\\ntial activation functions based on our theoretical\\nunderpinning inspires the design of a novel regu-\\nlarizer that effectively alleviates this fundamental\\nlimitation. Extensive experiments over many chal-\\nlenging real-world datasets and settings confirm\\nour theoretical findings and demonstrate the effec-\\ntiveness of our proposed approach.\\n\\n1. Introduction\\nDeep Learning (DL) models have found great success in\\nmany real-world applications such as speech recognition\\n(Kamath et al., 2019), machine translation (Singh et al.,\\n2017), and computer vision (Voulodimos et al., 2018). How-\\never, these highly expressive models may easily fit the noise\\nin the training data, which leads to overconfident predictions\\n(Nguyen et al., 2015). The challenge is further compounded\\nwhen learning from limited labeled data, which is common\\n\\n1Rochester Institute of Technology. Correspondence to: Qi Yu\\n\\n<qi.yu@rit.edu>.\\n\\nProceedings of the 40 th International Conference on Machine\\nLearning, Honolulu, Hawaii, USA. PMLR 202, 2023. Copyright\\n2023 by the author(s).\\n\\n1\\n\\nfor applications from specialized domain (e.g., medicine,\\npublic safety, and military operations) where data collec-\\ntion and annotation is highly costly. Accurate uncertainty\\nquantification is essential for successful application of DL\\nmodels in these domains. To this end, DL models have been\\naugmented to become uncertainty-aware (Gal & Ghahra-\\nmani, 2016; Blundell et al., 2015; Pearce et al., 2020). How-\\never, commonly used extensions require expensive sampling\\noperations (Gal & Ghahramani, 2016; Blundell et al., 2015),\\nwhich significantly increase the computational costs (Lak-\\nshminarayanan et al., 2017).\\n\\nThe recently developed evidential models bring together\\nevidential theory (Shafer, 1976; Jøsang, 2016) and deep\\nneural architectures that turn a deterministic neural network\\nuncertainty-aware. By leveraging the learned evidence, evi-\\ndential models are capable of quantifying fine-grained un-\\ncertainty that helps to identify the sources of ‘unknowns’.\\nFurthermore, since only lightweight modifications are intro-\\nduced to existing DL architectures, additional computational\\ncosts remain minimum. Such evidential models have been\\nsuccessfully extended to classification (Sensoy et al., 2018),\\nregression (Amini et al., 2020), meta-learning (Pandey & Yu,\\n2022a), and open-set recognition (Bao et al., 2021) settings.\\n\\nFigure 1. Cifar100 Result\\n\\nDespite the attractive\\nuncertainty quantifica-\\ntion capacity, eviden-\\ntial models are only\\nable to achieve a pre-\\ndictive performance on\\npar with standard deep\\narchitectures in rela-\\ntively simple learning problems. They suffer from a sig-\\nnificant performance drop when facing large datasets with\\nmore complex features even in the common classification\\nsetting. As shown in Figure 1, an evidential model using\\nReLU activation and an evidential MSE loss (Sensoy et al.,\\n2018) only achieves 36% test accuracy on Cifar100, which\\nis almost 40% lower than a standard model trained using\\nsoftmax. Additionally, most evidential models can easily\\nbreak down with minor architecture changes and/or have\\na much stronger dependency on hyperparameter tuning to\\nachieve reasonable predictive performance. The experiment\\nsection provides more details on these failure cases.\\n\\n\\x0cLearn to Accumulate Evidence from All Training Samples: Theory and Practice\\n\\ndevelopment of trustworthy Deep Learning (DL) models.\\nDeep ensemble techniques (Pearce et al., 2020; Lakshmi-\\nnarayanan et al., 2017) have been developed for uncer-\\ntainty quantification. An ensemble of neural networks is\\nconstructed and the agreement/disagreement across the en-\\nsemble components is used to quantify different uncertain-\\nties. Ensemble-based methods significantly increase the\\nnumber of model parameters, which are computationally\\nexpensive at both training and test times. Alternatively,\\nBayesian neural networks (Gal & Ghahramani, 2016)(Blun-\\ndell et al., 2015)(Mobiny et al., 2021) have been devel-\\noped that consider a Bayesian formalism to quantify dif-\\nferent uncertainties. For instance, (Blundell et al., 2015)\\nuse Bayes-by-backdrop to learn a distribution over neural\\nnetwork parameters, whereas (Gal & Ghahramani, 2016)\\nenable dropout during inference phase to obtain predictive\\nuncertainty. Bayesian methods resort to some form of ap-\\nproximation to address the intractability issue in marginal-\\nization of latent variables. Moreover, these methods are\\nalso computationally expensive as they require sampling for\\nuncertainty quantification.\\n\\nEvidential Deep Learning. Evidential models introduce a\\nconjugate higher-order evidential prior for the likelihood dis-\\ntribution that enables the model to capture the fine-grained\\nuncertainties. For instance, Dirichlet prior is introduced\\nover the multinomial likelihood for evidential classification\\n(Bao et al., 2021; Zhao et al., 2020), and NIG prior is in-\\ntroduced over the Gaussian likelihood (Amini et al., 2020;\\nPandey & Yu, 2022b) for the evidential regression models.\\nAdversarial robustness (Kopetzki et al., 2021) and calibra-\\ntion (Tomani & Buettner, 2021) of evidential models have\\nalso been well studied. Usually, these models are trained\\nwith evidential losses in conjunction with heuristic evidence\\nregularization to guide the uncertainty behavior (Pandey &\\nYu, 2022a; Shi et al., 2020) in addition to reasonable gen-\\neralization performance. Some evidential models assume\\naccess to out-of-distribution data during training (Malinin &\\nGales, 2019; 2018) and use the OOD data to guide the un-\\ncertainty behavior. A recent survey (Ulmer, 2021) provides\\na thorough review of the evidential deep learning field.\\n\\nIn this work, we focus on evidential classification models\\nand consider settings where no OOD data is used during\\nmodel training to make the proposed approach more broadly\\napplicable to practical real-world situations.\\n\\n3. Learning Deficiency of Evidential Models\\n\\n3.1. Preliminaries and problem setup\\nStandard classification models use a softmax transformation\\non the output from the neural network FΘ for input x to ob-\\ntain the class probabilities in K-class classification problem.\\nSuch models are trained with the cross-entropy based loss.\\n\\nFigure 2. Visualization of zero-evidence region for evidential mod-\\nels with ReLU activation in a binary classification setting. Existing\\nmodels fail to learn from samples that are mapped to such zero-\\nevidence region (shared area at the bottom left quadrant).\\n\\nTo train uncertainty-aware evidential models that can also\\npredict well, we perform a novel theoretical analysis with\\na focus on the standard classification setting to unveil the\\nunderlying cause of the performance gap. Our theoreti-\\ncal results show that existing evidential models learn sub-\\noptimally compared to corresponding softmax counterparts.\\nSuch sub-optimal training is mainly attributed to the inher-\\nent learning deficiency of evidential models that prevents\\nthem from learning across all training samples. More specif-\\nically, they are incapable to acquire new knowledge from\\ntraining samples mapped to “zero-evidence regions” in the\\nevidence space, where the predicted evidence reduces to\\nzero. The sub-optimal learning phenomenon is illustrated\\nin Figure 2 (detailed discussion is presented in Section 4.2).\\nWe analyze different variants of evidential models present\\nin the existing literature and observe this limitation across\\nall the models and settings. Our theoretical results inspire\\nthe design of a novel Regularized Evidential model (RED)\\nthat includes positive evidence regularization in its train-\\ning objective to battle the learning deficiency. Our major\\ncontributions can be summarized as follows:\\n\\n• We identify a fundamental limitation of evidential models,\\ni.e., lack the capability to learn from any data samples that\\nlie in the “zero-evidence” region in the evidence space.\\n• We theoretically show the superiority of evidential models\\n\\nwith exp activation over other activation functions.\\n\\n• We conduct novel evidence regularization that enables\\nevidential models to avoid the “zero-evidence” region so\\nthat they can effectively learn from all training samples.\\n• We carry out experiments over multiple challenging real-\\nworld datasets to empirically validate the presented theory,\\nand show the effectiveness of our proposed ideas.\\n\\n2. Related Works\\nUncertainty Quantification in Deep Learning. Accu-\\nrate quantification of predictive uncertainty is essential for\\n\\n2\\n\\n\\x0cLearn to Accumulate Evidence from All Training Samples: Theory and Practice\\n\\nFor a given training sample (x, y), the loss is given by\\n\\nin the prediction) computed as\\n\\nLcross = −\\n\\nK\\n(cid:88)\\n\\nk=1\\n\\nyk log(smk)\\n\\n(1)\\n\\nwhere smk is the softmax output. These models have\\nachieved state-of-the-art performance on many benchmark\\nproblems. A detailed gradient analysis shows that they can\\neffectively learn from all training data samples (see Ap-\\npendix A). Nevertheless, these models lack a systematic\\nmechanism to quantify different sources of uncertainty, a\\nhighly desired property in many real-world problems.\\n\\nFigure 3. Graphical model for Evidential Deep Learning\\n\\nEvidential classification models formulate training as an\\nevidence acquisition process and consider a higher-order\\nDirichlet prior Dir(p|α) over the predictive Multino-\\nmial distribution Mult(y|p). Different from a standard\\nBayesian formulation which optimizes Type II Maximum\\nLikelihood to learn the Dirichlet hyperparameter (Bishop\\n& Nasrabadi, 2006), evidential models directly predict α\\nusing data features x and then generate the prediction y by\\nmarginalizing the Multinomial parameter p. Figure 3 de-\\nscribes this generative process. Such higher-order prior en-\\nables the model to systematically quantify different sources\\nof uncertainty. In evidential models, the softmax layer of\\nthe standard neural networks is replaced by a non-negative\\nactivation function A, where A(x) ≥ 0 ∀x ∈ [−∞, ∞],\\nsuch that for input x, the neural network model FΘ with\\nparameters Θ can output evidence e for different classes.\\nDirichlet prior α is evaluated as α = e + 1 to ensure α ≥ 1.\\nThe trained evidential model outputs Dirichlet parameters\\nα for input x that can quantify fine-grained uncertainties in\\naddition to the prediction y. Mathematically, for K−class\\nclassification problem,\\n\\nEvidence(e) = A(FΘ(x)) = A(o)\\nDirichlet Parameter(α) = e + 1\\n\\n(2)\\n\\n(3)\\n\\nDirichlet Strength(S) = K +\\n\\nK\\n(cid:88)\\n\\nk=1\\n\\nek\\n\\n(4)\\n\\nThe activation function A(·) assumes three common forms\\nto transform the neural network output into evidence: (1)\\nReLU(·) = max(0, ·), (2) SoftPlus(·) = log(1 +\\nexp(·)), and (3) exp(·).\\n\\nEvidential models assign input sample to that class for which\\nthe output evidence is greatest. Moreover, they quantify the\\nconfidence in the prediction for K class classification prob-\\nlem through vacuity ν (i.e., measure of lack of confidence\\n\\n3\\n\\nVacuity(ν) =\\n\\nK\\nS\\n\\n(5)\\n\\nFor any training sample (x, y), the evidential models aim to\\nmaximize the evidence for the correct class, minimize the\\nevidence for the incorrect classes, and output accurate confi-\\ndence. To this end, three variants of evidential loss functions\\nhave been proposed (Sensoy et al., 2018): 1) Bayes risk with\\nsum of squares loss, 2) Bayes risk with cross-entropy loss,\\nand 3) Type II Maximum Likelihood loss. Please refer to\\nequations (21), (22), and (23) in the Appendix for the spe-\\ncific forms of these losses. Additionally, incorrect evidence\\nregularization terms are introduced to guide the model to\\noutput low evidence for classes other than the ground truth\\nclass (See Appendix C for discussion on the regularization).\\nWith evidential training, accurate evidential deep learning\\nmodels are expected to output high evidence for the correct\\nclass, low evidence for all other classes, and output very\\nhigh vacuity for unseen/out-of-distribution samples.\\n\\n3.2. Theoretical Analysis of Learning Deficiency in\\n\\nEvidential Learning\\n\\nTo identify the underlying reason that causes the perfor-\\nmance gap of evidential models as described earlier, we\\nconsider a K class classification problem and a represen-\\ntative evidential model trained using Bayes risk with sum\\nof squares loss given in (21). We first provide an important\\ndefinition that is critical for our theoretical analysis.\\nDefinition 1 (Zero-Evidence Region). A Zero-evidence\\nsample is a data sample for which the model outputs zero\\nevidence for all classes. A region in the evidence space that\\ncontains zero-evidence samples is a zero-evidence region.\\n\\nFor a reasonable evidential model, novel data samples not\\nyet seen during training, difficult data samples, and out-of-\\ndistribution samples should become zero-evidence samples.\\n\\nTheorem 1. Given a training sample (x, y), if an evidential\\nneural network outputs zero evidence e, then the gradients\\nof the evidential loss evaluated on this training sample over\\nthe network parameters reduce to zero.\\n\\nProof. Consider an input x with one-hot ground truth label\\ny. Let the ground truth class index be gt, i.e., ygt = 1,\\nwith corresponding Dirichlet parameter αgt, and y̸=gt =\\n0. Moreover, let o, e, and α represent the neural network\\noutput vector before applying the activation A, the evidence\\nvector, and the Dirichlet parameters respectively.\\n\\nIn this evidential model, the loss is given by\\n\\nLMSE(x, y) =\\n\\nK\\n(cid:88)\\n\\nj=1\\n\\n(yj −\\n\\nαj\\nS\\n\\n)2 +\\n\\nαj(S − αj)\\nS2(S + 1)\\n\\n(6)\\n\\n\\x0cLearn to Accumulate Evidence from All Training Samples: Theory and Practice\\n\\nNow, the gradient of the loss with respect to the neural\\nnetwork output can be computed using the chain rule:\\n\\n∂LMSE(x, y)\\n∂ok\\n(cid:20) 2αgt\\n\\n=\\n\\nS2 − 2\\n\\n=\\n\\nyk\\nS\\n\\n∂ek\\n∂ok\\n\\n∂LMSE(x, y)\\n∂αk\\n2(S − αk)\\nS(S + 1)\\n(cid:80)\\n\\n−\\n\\n2(2S + 1) (cid:80)\\ni\\n\\nj αiαj\\n\\n+\\n\\n(S2 + S)2\\n\\n+\\n\\n(7)\\n\\n(cid:21)\\n\\n×\\n\\n∂ek\\n∂ok\\n\\nBased on the actual form of A, we have three cases:\\n\\nCase I: ReLU(·) to transform logits to evidence\\n\\nek = ReLU(ok) =⇒\\n\\n∂ek\\n∂ok\\n\\n=\\n\\n(cid:40)\\n1\\n0\\n\\nok > 0\\n\\nif\\notherwise\\n\\n(8)\\n\\nFor a zero-evidence sample, the logits ok satisfy the rela-\\ntionship ok ≤ 0 ∀ k =⇒ ∂ek\\n∂ok\\nCase II: SoftPlus(·) to transform logits to evidence\\n\\n= 0 =⇒ ∂LMSE(x,y)\\n\\n= 0\\n\\n∂ok\\n\\nek = log(exp(ok) + 1) =⇒\\n\\n∂ek\\n∂ok\\n\\n= Sigmoid(ok) (9)\\n\\nFor a zero-evidence sample, the logits ok → −∞ =⇒\\nSigmoid(ok) → 0 & ∂ek\\n→ 0.\\n∂ok\\n\\nCase III: exp(·) to transform logits to evidence\\n\\nek = exp(ok) =⇒\\n\\n∂ek\\n∂ok\\n\\n= exp(ok) = αk − 1\\n\\n(10)\\n\\nFor a zero-evidence sample, αk → 1 =⇒ ∂ek\\n→ 0.\\n∂ok\\nMoreover, there is no term in the first part of the loss gradient\\nin (7) to counterbalance these zero-approaching gradients.\\nSo, for zero-evidence training samples, for any node k,\\n\\n∂LMSE(x, y)\\n∂ok\\n\\n= 0\\n\\n(11)\\n\\nSince the gradient of the loss with respect to all the nodes\\nis zero, there is no update to the model from such samples.\\nThis implies that the evidential models fail to learn from a\\nzero-evidence data sample.\\n\\nFor completeness, we present the analysis of standard clas-\\nsification models in Appendix A, detailed proof of the evi-\\ndential models trained using Bayes risk with sum of squares\\nerror along with other evidential lossses in Appendix B, and\\nimpact of incorrect evidence regularization in Appendix C.\\n\\nRemark: Evidential models can not learn from a train-\\ning sample that the model has never seen and for which\\nthe model accurately outputs “I don’t know”, i.e., ek =\\n0 ∀k ∈ [1, K]. Such samples are expected and likely to be\\npresent during model training. However, the supervised in-\\nformation in such training data points is completely missed\\n\\n4\\n\\nby evidential models so they fail to acquire any new knowl-\\nedge from all such training data samples (i.e., data samples\\nin zero-evidence region of the evidence space).\\n\\nCorollary 1. Incorrect evidence regularization can not help\\nevidential models learn from zero-evidence samples.\\n\\nIntuitively, the incorrect evidence regularization encourages\\nthe model to output zero evidence for all classes other than\\nthe ground truth class and the regularization does not have\\nany impact on the evidence for the ground truth class. So,\\nthe regularization updates the model parameters such that\\nthe model is likely to map input samples closer to zero-\\nevidence region in the evidence space. Thus, the regular-\\nization does not address the failure of evidential models to\\nlearn from zero evidence samples.\\n\\nTheorem 2. For a data sample x, if an evidential model\\noutputs logits ok ≤ 0 ∀k ∈ [0, K], the exponential acti-\\nvation function leads to a larger gradident update on the\\nmodel parameters than softplus and ReLu.\\n\\nLimited by space, we present the proof of Theorem 2 along\\nwith additional analysis in the Appendix D. The proof fol-\\nlows the gradient analysis of the exponential, Softplus,\\nand ReLU based models. It implies that the the training\\nof evidential models is most effective with the exponential\\nactivation function. Intuitively, the ReLU based activation\\ncompletely destroys all the information in the negative logits,\\nand has largest region in evidence space in which training\\ndata have zero evidence. Softplus activation improves\\nover the ReLU, and compared to ReLU, has smaller region\\nin evidence space where training data have zero evidence.\\nHowever, Softplus based evidential models fail to cor-\\nrect the acquired knowledge when the model has strong\\nwrong evidence. Moreover, these models are likely to suf-\\nfer from vanishing gradients problem when the number of\\nclasses increases (i.e., classification problem becomes more\\nchallenging). Finally, exponential activation has the smallest\\nzero-evidence region in the evidence space without suffering\\nfrom the issues of SoftPlus based evidential models.\\n\\n4. Avoiding Zero-Evidence Regions Through\\n\\nCorrect Evidence Regularization\\n\\nWe now consider an evidential model with exponential func-\\ntion to transform the logits into evidence. We propose a\\nnovel vacuity-guided correct evidence regularization term\\n\\nLcor(x, y) = −λcor log(αgt − 1)\\n\\n(12)\\n\\nwhere λcor = ν = K\\nS represents the regularization term\\nwhose value is given by the magnitude of the vacuity output\\nby the evidential model and αgt − 1 represents the predicted\\nevidence for the ground truth class. The regularization\\nterm λcor determines the relative importance of the correct\\n\\n\\x0cLearn to Accumulate Evidence from All Training Samples: Theory and Practice\\n\\nevidence regularization term compared to the evidential\\nloss and incorrect evidence regularization and is treated as\\nconstant during model parameter update.\\n\\nTheorem 3. Correct evidence regularization Lcor(x, y)\\ncan address the issue of learning from zero-evidence train-\\ning samples.\\n\\n4.2. Evidence Space Visualization\\n\\nProof. The proposed regularization term Lcor(x, y) does\\nnot contain any evidence terms other than the evidence for\\nthe ground truth node. So, the gradient of the regularization\\nfor nodes other than the ground truth node will be 0 i.e.\\n∂Lcor(x,y)\\n= 0 and there will be no update on these\\n∂ok\\n\\n(cid:12)\\n(cid:12)\\n(cid:12)k̸=gt\\n\\nnodes. For the ground truth node gt, ygt = 1, the gradient\\nis given by\\n\\n∂Lcor(x, y)\\n∂ogt\\n\\n=\\n\\n∂(cid:0) − λcor log(αgt − 1)(cid:1)\\n∂ogt\\n\\n= −λcor\\n\\n∂ log(αgt − 1)\\n∂αgt\\n\\n×\\n\\n∂αgt\\n∂ogt\\n\\n(13)\\n\\n(14)\\n\\n= −\\n\\nλcor\\n(αgt − 1)\\n\\n(αgt − 1) = −λcor\\n\\n(15)\\n\\nThe gradient value equals the magnitude of the vacuity. The\\nvacuity is bounded in the range [0, 1], and zero-evidence\\nsample, the vacuity is maximum, leading to the greatest\\ngradient value of ∂Lcor(x,y)\\n= −1. In other words, the reg-\\nularization encourages the model to update the parameters\\nsuch that the correct evidence αgt − 1 increases. As the\\nmodel evidence increases, the vacuity decreases, and the\\ncontribution of the regularization Lcor(x, y) is minimized.\\nThus, the proposed regularization enables the evidential\\nmodel to learn from zero-evidence samples.\\n\\n∂ogt\\n\\n4.1. Evidential Model Training\\nWe formulate an overall objective used to train the pro-\\nposed Regularized evidential model (RED). Essentially,\\nthe evidential model is trained to maximize the correct evi-\\ndence, minimize the incorrect evidence, and avoid the zero-\\nevidence region during training. The overall loss is\\n\\nL(x, y) = Levid(x, y) + η1Linc(x, y) + Lcor(x, y)\\n\\n(16)\\n\\nwhere Levid(x, y) is the loss based on the evidential\\nframework given by (21), (23), or (22) (See Appendix B),\\nLinc(x, y) represents the incorrect evidence regularization\\n(See Appendix Section C), Lcor(x, y) represents the pro-\\nposed novel correct evidence regularization term in (12), and\\nη1 = λ1 × min(1.0, epoch index/10) controls the impact\\nof incorrect evidence regularization to the overall model\\ntraining. In this work, we consider the forward-KL based\\nincorrect evidence regularization given in (42) based on\\n(Sensoy et al., 2018).\\n\\n5\\n\\nFigure 4. Evidence space visualization to demonstrate the effec-\\ntiveness of the proposed method.\\n\\nFigure 2 visualizes the evidence space in ReLU-based ev-\\nidential models by considering the pre-ReLU output in a\\nbinary classification setting. Ideally, all samples that belong\\nto Class 1 should be mapped to the blue region (region of\\nhigh evidence for Class 1, low evidence for all other classes),\\nall samples that belong to Class 2 should be mapped to the\\nred region, and all out-of distribution samples should be\\nmapped to the zero-evidence region (no evidence for all\\nclasses). To realize this goal, the models are trained using\\nthe evidential loss Levid with incorrect evidence regular-\\nization Linc. However, there is no update to the evidential\\nmodel from such samples of zero-evidence region. Model’s\\nprior belief of “I don’t know” for such samples does not\\nget updated even after being exposed to the true label. For\\nthe samples with high incorrect evidence and low correct\\nevidence, evidential model aims to correct itself. However,\\nmany such samples are likely to get mapped to the zero-\\nevidence region (as shown by blue and orange arrows in\\nFigure 2) after which there is no update to the model. Such\\nfundamental limitation holds true for all evidential models.\\n\\nThe evidence space visualization for RED is shown in Figure\\n4 to illustrate how it addresses the above limitation. Cor-\\nrect evidence regularization (indicated by green arrows) is\\nweighted by the magnitude of the vacuity and is maximum\\nin the zero-evidence region. In this problematic region, the\\nproposed regularization fully dominates the model update\\nas there is no update to the model from the two loss com-\\nponents (Levid and Linc) in (16). As the sample gets far\\naway from the zero evidence region, the vacuity decreases\\nproportionally, the impact of the proposed regularization\\nto model update becomes insignificant, and the evidential\\nlosses (Levid & Linc) guide the model training. In this\\nway, RED can effectively learn from all training samples\\nirrespective of the model’s existing evidence.\\n\\n\\x0cLearn to Accumulate Evidence from All Training Samples: Theory and Practice\\n\\n5. Experiments\\nDatasets and setup. We consider the standard supervised\\nclassification problem with MNIST (LeCun, 1998), Ci-\\nfar10, and Cifar100 datasets (Krizhevsky et al., 2009), and\\nfew-shot classification with mini-ImageNet dataset (Vinyals\\net al., 2016). We employ the LeNet model for MNIST,\\nResNet18 model (He et al., 2016) for Cifar10/Cifar100,\\nand ResNet12 model (He et al., 2016) for mini-ImageNet.\\nWe first conduct experiments to demonstrate the learning\\ndeficiency of existing evidential models to confirm our the-\\noretical findings. We then evaluate the proposed correct\\nevidence regularization to show its effectiveness. We finally\\nconduct ablation studies to investigate the impact of evi-\\ndential losses on model generalization and the uncertainty\\nquantification of the proposed evidential model. Limited by\\nspace, additional clarifications, experiment results includ-\\ning few-shot classification experiments, experiments over\\nchallenging tiny-Imagenet datasett with Swin Transformer,\\nhyperparameter details, and discussions are presented in the\\nAppendix.\\n\\n5.1. Learning Deficiency of Evidential Models\\n\\nSensitivity to the change of the architecture. We first\\nconsider a toy illustrative experiment with two frameworks:\\n1) standard softmax, 2) evidential learning, and experiment\\nwith the LeNet (LeCun et al., 1999) model considered in\\nEDL (Sensoy et al., 2018) with a minor modification to the\\narchitecture: no dropout in the model. To construct the toy\\ndataset, we randomly select 4 labeled data points from the\\nMNIST training dataset as shown in the Figure 5. For the\\nevidential model, we use ReLU to transform the network\\noutputs to evidence, and train the model with MSE-based\\nevidential loss (Sensoy et al., 2018) given in (21) without\\nincorrect evidence regularization. We train both models\\nusing only these 4 training data points.\\n\\nFigure 6 compares the training accuracy and training loss\\ntrends of the evidential model with the standard softmax\\nmodel (trained with the cross-entropy loss). Before any\\ntraining, both models have 0% accuracy and the loss is high\\nas expected. For the evidential model, in the first few iter-\\nations, the model learns from the training dataset, and the\\nmodel’s accuracy increases to 50%. Afterward, the eviden-\\ntial model fails to learn as the evidential model maps two of\\nthe training data samples to the zero-evidence region. Even\\nin such a trivial setting, the evidential model fails to fit the 4\\ntraining data points showing their learning deficiency that\\nempirically verifies the conclusion in Theorem 1. It is also\\nworth noting that the range of the evidential model’s loss is\\nsignificantly smaller than the standard model. This is mainly\\ndue to the bounded nature of the evidential MSE loss(i.e., it\\nis bounded in the range [0, 2]) (a detailed theoretical analy-\\nsis of the evidential losses is provided in the Appendix). In\\ncontrast, the standard model trained with cross-entropy loss\\n\\nFigure 5. Toy dataset with 4 data points.\\n\\n(a) Training accuracy trend\\n\\n(b) Training loss trend\\n\\nFigure 6. Training of standard and evidential models\\n\\neasily fits the trivial dataset, obtains near 0 loss, and perfect\\naccuracy of 100% after a few iterations of training.\\n\\nFigure 7. Zero-evidence trend during model training\\n\\nAdditionally, we visualize the zero-evidence data samples\\nfor the toy dataset setting. We plot the total evidence for\\neach training sample as training progresses for the first 100\\niterations. The total evidence trend as training progresses\\nfor the first 100 iterations is shown in Figure 7. The ev-\\nidential model’s predictions are correct for data samples\\nwith ground truth labels of 3 and 6, and incorrect for the\\nremaining two data samples. After few iterations of training,\\nthe remaining two samples have zero total evidence (i.e.\\nsamples are mapped to zero evidence region), the model\\nnever learns from them, and the model only achieves overall\\n50% training accuracy even after 100 iterations. Clearly,\\nthe evidential model continues to output zero evidence for\\ntwo of the training examples and fails to learn from them.\\nSuch learning deficiency of evidential models limits their\\nextension to challenging settings. In contrast, the standard\\nmodel easily overfits the 4 training examples and achieves\\n100% accuracy.\\n\\nSensitivity to hyperparameter tuning.\\nIn this experi-\\nment, evidential models are trained using evidential losses\\ngiven in (21), (22), or (23) with incorrect evidence regular-\\nization to guide the model for accurate uncertainty quan-\\n\\n6\\n\\nGT: 3GT: 5GT: 2GT: 602468Iterations (× 10)0.000.250.500.751.00AccuracyStandard modelEvidential model02468Iterations (× 10)012LossStandard ModelEvidential Model0255075100Iteration024Evidence3526\\x0cLearn to Accumulate Evidence from All Training Samples: Theory and Practice\\n\\nFigure 8. Impact of different incorrect evidence regularization\\nstrengths to the test set accuracy on Cifar100 dataset\\n\\ntification. We study the impact of the incorrect evidence\\nregularization λ1 to the evidential model’s performance\\nusing Cifar100. The result shows that the generalization\\nperformance of evidential models is highly sensitive to λ1\\nvalues. To illustrate, we consider the Type II Maximum\\nLikelihood loss in (23) with different λ1 to control KL reg-\\nularization (results on other loss functions are presented in\\nthe Appendix). As shown in Figure 8, when some regular-\\nization is introduced, evidential model’s test performance\\nimproves slightly. However, when strong regularization is\\nused, the model focuses strongly on minimizing the incor-\\nrect evidence. Such regularization causes the model to push\\nmany training samples into or close to the zero-evidence\\nregions, which hurts the model’s learning capabilities. In\\ncontrast, the proposed model can continue to learn from\\nsamples in zero-evidence regions, which shows its robust-\\nness to incorrect evidence regularization. Moreover, our\\nmodel has stable performance across all hyperparameter\\nsettings as it can effectively learn from all training samples.\\n\\nChallenging datasets and settings. We next consider\\nstandard classification models for the Cifar100 dataset and\\n1-shot classification with the mini-ImageNet dataset. We\\ndevelop evidential extensions of the classification models\\nusing Type II Maximum Likelihood loss given in (23) with-\\nout any incorrect evidence regularization and use ReLU to\\ntransform logits to evidence. As shown in Figure 10, com-\\npared to the standard classification model, the evidential\\nmodel’s predictive performance is sub-optimal (almost 20%\\nlower for both classification problems). This is mainly due\\nto the fact that evidential model maps many of the training\\ndata points to zero-evidence region, which is equivalent to\\nthe model saying “I don’t know to which class this sample\\nbelongs” and stopping to learn from them. Consequently,\\nthe model fails to acquire new knowledge (i.e., update itself),\\neven after being exposed to correct supervision (the label\\ninformation). In these cases, instead of learning, the eviden-\\ntial model chooses to ignore the training data on which it\\ndoes not have any evidence and remains to be ignorant.\\n\\nVisualization of zero-evidence samples. We next show\\nthe 2-dimensional visualization of the latent representation\\nfor the randomly selected 500 training examples based on\\n\\nFigure 9. Zero-Evidence Sample Visualization\\n\\n(a) Cifar100 Results\\n\\n(b) 1-Shot Results\\n\\nFigure 10. Learning trends in complex classification problems\\n\\nthe tSNE plot for ReLU based evidential model trained on\\nthe Cifar100 dataset with λ1 = 0.1. Figure 9 plot visualizes\\nthe latent embedding of zero evidence (Zero E) training sam-\\nples with non-zero evidence (Non-Zero E) training samples.\\nAs can be seen, both zero and non-zero evidence samples ap-\\npear to be dispersed, overlap at different regions, and cover\\na large area in the embedding space. This further confirms\\nthe challenge of effectively learning from these samples\\n\\n5.2. Effectiveness of the RED\\n\\nEvidential activation function. We first experiment with\\ndifferent activation functions for the evidential models to\\nshow the superior predictive performance and generalization\\ncapability of exp activation validating our Theorem 2. We\\nconsider evidential models trained with evidential log loss\\ngiven by (23) in Table 1 (Additional results along with hy-\\nperparameter details are presented in Appendix Section F).\\nAs can be seen, exp activation to transform network outputs\\ninto evidence leads to superior performance compared to\\nReLU and Softplus based transformations. Furthermore,\\nour proposed model with correct evidence regularization\\nfurther improves over the exp-based evidential models as\\nit enables the evidential model to continue learning from\\nzero-evidence samples.\\n\\nTable 1. Classification performance comparison\\n\\nModel\\nReLU\\nSoftPlus\\nexp\\nRED(Ours)\\n\\nMNIST\\n98.19±0.08\\n98.21±0.05\\n98.79±0.02\\n99.10±0.02\\n\\nCifar10\\n41.43±19.60\\n95.18±0.11\\n95.11±0.10\\n95.24±0.06\\n\\nCifar100\\n61.27±3.79\\n74.48±0.17\\n76.12±0.04\\n76.43±0.21\\n\\nWe next present the test set performance change as training\\n\\n7\\n\\n\\x0cLearn to Accumulate Evidence from All Training Samples: Theory and Practice\\n\\nprogresses with MNIST dataset and two different evidential\\nlosses in Figure 11 where we observe similar results. The\\nexp activation shows superior performance, as it has small-\\nest zero-evidence region, and does not suffer from many\\nlearning issues present in other activation functions.\\n\\n(a) Trend for λ1 = 1.0\\n\\n(b) Trend for λ1 = 10.0\\n\\n(a) Evidential MSE loss\\n\\n(b) Evidential Log loss\\n\\nFigure 11. Impact of evidential activation functions to the Test\\nAccuracy\\n\\nCorrect evidence regularization. We now study the im-\\npact of the proposed correct evidence regularization using\\nthe MNIST and Cifar100 classification problems. We con-\\nsider the evidential baseline model that uses exp activation\\nto acquire evidence, and is trained with Type II Maximum\\nLikelihood based loss with different incorrect evidence reg-\\nularization strengths. We introduce the proposed novel cor-\\nrect evidence regularization to the model. As can be seen in\\nFigure 12, the model with correct-evidence regularization\\nhas superior generalization performance compared to the\\nbaseline evidential model. This is mainly due to the fact\\nthat with proposed correct evidence regularization, the evi-\\ndential model can also learn from the zero-evidence training\\nsamples to acquire new knowledge instead of ignoring them.\\nOur proposed model considers knowledge from all the train-\\ning data and aims to acquire new knowledge to improve its\\ngeneralization instead of ignoring the samples on which it\\nhas no knowledge. Finally, even though strong incorrect\\nevidence regularization hurts the model’s generalization, the\\nproposed model is robust and generalizes better, empirically\\nvalidating our Theorem 3. Limited by space, we present\\nadditional results in Appendix F.3.2.\\n\\nZero-evidence Sample Anaysis. Similar to the toy\\nMNIST zero-evidence analysis, we consider the Cifar100\\ndataset, and carry out\\nthe analysis for this complex\\ndataset/setting. Instead of focusing on a few training ex-\\namples, we present the average statistics of the evidence\\n(E) for the 50,000 training samples in the 100 class classi-\\nfication problem for a model trained for 200 epochs using\\na log-based evidential loss in (23) with λ1 = 1.0. For ref-\\nerence, the samples with less than 0.01 average evidence\\n(i.e., E ≤ 0.01) are samples on which the model is not\\nconfident (i.e., having a high vacuity of ν ≥ 0.99), and are\\nclose to the ideal zero-evidence region. Our proposed RED\\nmodel effectively avoids such zero evidence regions, and\\nhas the lowest number of samples (i.e. only 0.06% of total\\ntraining dataset compared to 58.96% of SoftPlus based,\\n\\n(c) Trend for λ1 = 0.1\\n\\n(d) Trend for λ1 = 1.0\\n\\nFigure 12. Impact of correct evidence regularization to test accu-\\nracy: (a), (b) - MNIST Results; (c), (d) - Cifar100 Results\\n\\nand 100% of ReLU based evidential models) in very low\\nevidence regions.\\n\\nTable 2. Zero-Evidence Analysis for Complex Dataset-Setting\\nE ≤ 0.1\\n50000\\n32006\\n49881\\n16322\\n\\nE ≤ .01\\n50000\\n29483\\n48318\\n30\\n\\nE ≤ 1.0\\n50000\\n49938\\n49949\\n25154\\n\\nE > 1.0\\n0\\n62\\n51\\n24846\\n\\nModel\\nReLU\\nSoftPlus\\nExp\\nRED\\n\\n5.3. Ablation Study\\n\\nImpact of loss function. We next study the impact of\\nthe evidential loss function on the model’s performance\\nusing MNIST and CIFAR100 classification problems. We\\nconsider all three activations: ReLU, SoftPlus, and exp\\nto transform neural network outputs to evidence and carry\\nout experiments over CIFAR100 with identical model and\\nsettings. As seen in Table 3, the generalization performance\\nof evidential model is consistently sub-optimal when trained\\nwith evidential MSE loss given by (21) compared to the two\\nother evidential losses (22) & (23). This is consistent across\\nall three evidence activation functions. This is mainly due\\nto the bounded nature of the evidential MSE loss (21): for\\nall training samples, evidential MSE loss is bounded in the\\nrange of [0, 2]. Type II Maximum Likelihood loss given in\\n(23) and cross-entropy based evidential loss given in (22)\\nshow comparable empirical results.\\n\\nNext, we consider exp activation and conduct experiments\\nover the MNIST dataset for incorrect evidence regulariza-\\ntion strengths of λ1 = 0&1. We again observe similar\\nresults where the training with the Evidential MSE loss in\\n(21) leads to sub-optimal test performance. Additional re-\\nsults, along with theoretical analysis are presented in the\\nAppendix. In the subsequent experiments, we consider the\\nType II Maximum Likelihood loss (23) for evidential model\\ntraining due to its simplicity and some theoretical advan-\\n\\n8\\n\\n\\x0cLearn to Accumulate Evidence from All Training Samples: Theory and Practice\\n\\ntages (see Appendix E). We leave a thorough investigation\\nof these two evidential losses ((22) & (23)) as future work.\\n\\nto superior results demonstrating its accurate uncertainty\\nquantification capability.\\n\\nReLU\\n\\nTable 3. Impact of evidential losses on classification performance\\nexp\\nRED(Ours)\\n42.95±0.7 75.73±0.3\\n76.23±0.1 76.35±0.1\\n76.12±0.1 76.43±0.2\\n\\nLoss\\nMSE(21) 31.49±0.3\\n68.62±2.4\\nCE (22)\\n61.27±3.8\\nLog(23)\\n\\nSoftPlus\\n15.74±0.5\\n74.44±0.1\\n74.48±0.1\\n\\nTable 4. Accuracy on Top-K% confident samples (%)\\n\\n10% 20% 30% 50% 80% 100%\\nModel\\nReLU\\n61.27\\n98.50\\nSoftPlus 99.10\\n74.48\\nexp\\n76.12\\n99.40\\nRED\\n76.43\\n99.60\\n\\n71.54\\n85.56\\n86.46\\n86.38\\n\\n90.60\\n95.86\\n96.52\\n96.24\\n\\n98.30\\n98.75\\n98.95\\n99.35\\n\\n97.27\\n98.30\\n98.50\\n98.83\\n\\nWe next consider out-of-distribution (OOD) detection ex-\\nperiments for the Cifar100-trained evidential model using\\nSVHN dataset (as OOD) (Netzer et al., 2011). As seen\\nin Table 5, the evidential models, on average, output very\\nhigh vacuity for the OOD samples, showing the potential\\nfor OOD detection.\\n\\nTable 5. Out-of-Distribution sample detection\\n\\nModel\\nexp\\nRED (Ours)\\n\\nInD Vacuity\\n0.3227\\n0.2729\\n\\nOOD Vacuity (SVHN)\\n0.7681\\n0.7552\\n\\nWe present the AUROC score for Cifar100 trained models\\nwith SVHN dataset test set as the OOD samples in Table\\n6. In AUROC calculation, we use the maximum softmax\\nscore for the standard model, and predicted vacuity score\\nfor all the evidential models. As can be seen, the exp-based\\nmodel outperforms all other activation functions, and the\\nproposed model RED can learn from all the training samples\\nthat leads to the best performance.\\n\\nTable 6. AUROC for Cifar100-SVHN experiment\\n\\nReLU\\nModel\\nAUROC 0.7430\\n\\nSoftPlus\\n0.8058\\n\\nStandard\\n0.8669\\n\\nexp\\n0.8804\\n\\nRED\\n0.8833\\n\\n6. Conclusion\\n\\nIn this paper, we theoretically investigate the evidential mod-\\nels to identify their learning deficiency, which makes them\\nfail to learn from zero-evidence regions. We then show\\nthe superiority of the evidential model with exp evidential\\nactivation over the ReLU and SoftPlus based models.\\nWe further analyze the evidential losses, and introduce a\\nnovel correct evidence regularization over the exp-based ev-\\nidential model. The proposed model effectively pushes the\\ntraining samples out of the zero-evidence regions, leading to\\nsuperior learning capabilities. We conduct extensive experi-\\nments that empirically validate all theoretical claims while\\ndemonstrating the effectiveness of the proposed approach.\\n\\nAcknowledgements\\n\\nThis research was supported in part by an NSF IIS award\\nIIS-1814450 and an ONR award N00014-18-1-2875. The\\nviews and conclusions contained in this paper are those of\\nthe authors and should not be interpreted as representing\\nany funding agency.\\n\\n(a) Trend for λ1 = 0.0\\n\\n(b) Trend for λ1 = 1.0\\n\\nFigure 13. Impact of evidential losses on test set accuracy\\n\\nFigure 14. Accuracy-Vacuity curve\\n\\nStudy of uncertainty information. We now investigate\\nthe uncertainty behavior of the proposed evidential model\\nwith Cifar100 experiments. We present the Accuracy-\\nVacuity curve for different incorrect evidence regulariza-\\ntion strengths (λ1) in Figure 14. Vacuity reflects the lack\\nof confidence in the predictions, and the accuracy of effec-\\ntive evidential model should increase with lower vacuity\\nthreshold. Without any incorrect evidence regularization\\n(i.e., λ1 = 0), the evidential model is highly confident on\\nits predictions and all test samples are concentrated on the\\nlow vacuity region. As the incorrect evidence regularization\\nstrength is increased, the model outputs more accurate confi-\\ndence in the predictions. Strong incorrect evidence regular-\\nization hurts the generalization over the test set as indicated\\nby low accuracy when all test samples are considered (i.e.,\\nvacuity threshold of 1.0). In all cases, the evidential model\\nshows reasonable uncertainty behavior: the model’s test set\\naccuracy increases as the vacuity threshold is decreased.\\n\\nNext, we look at the accuracy of the evidential models on\\ntheir top-K % most confident predictions over the test set.\\nTable 4 shows the accuracy trend of Top-K (%) confident\\nsamples. Consider the most confident 20% samples (cor-\\nresponding to 2000 test samples of Cifar100 dataset). The\\nproposed model leads to highest accuracy (of 99.35%) com-\\npared to all the models. Similar trend is seen for different\\nK values where the proposed model shows comparable\\n\\n9\\n\\n\\x0cLearn to Accumulate Evidence from All Training Samples: Theory and Practice\\n\\nReferences\\n\\nAmini, A., Schwarting, W., Soleimany, A., and Rus, D.\\nDeep evidential regression. Advances in Neural Informa-\\ntion Processing Systems, 33:14927–14937, 2020.\\n\\nBao, W., Yu, Q., and Kong, Y. Evidential deep learning\\nfor open set action recognition. In Proceedings of the\\nIEEE/CVF International Conference on Computer Vision,\\npp. 13349–13358, 2021.\\n\\nBishop, C. M. and Nasrabadi, N. M. Pattern recognition\\n\\nand machine learning, volume 4. Springer, 2006.\\n\\nBlundell, C., Cornebise, J., Kavukcuoglu, K., and Wierstra,\\nD. Weight uncertainty in neural network. In International\\nconference on machine learning, pp. 1613–1622. PMLR,\\n2015.\\n\\nCharpentier, B., Z¨ugner, D., and G¨unnemann, S. Posterior\\nnetwork: Uncertainty estimation without ood samples\\nvia density-based pseudo-counts. Advances in Neural\\nInformation Processing Systems, 33:1356–1367, 2020.\\n\\nChen, Y., Liu, Z., Xu, H., Darrell, T., and Wang, X. Meta-\\nbaseline: Exploring simple meta-learning for few-shot\\nlearning. In Proceedings of the IEEE/CVF International\\nConference on Computer Vision, pp. 9062–9071, 2021.\\n\\nFinn, C., Abbeel, P., and Levine, S. Model-agnostic meta-\\nlearning for fast adaptation of deep networks. In Proceed-\\nings of the 34th International Conference on Machine\\nLearning-Volume 70, pp. 1126–1135. JMLR. org, 2017.\\n\\nGal, Y. and Ghahramani, Z. Dropout as a bayesian approx-\\nimation: Representing model uncertainty in deep learn-\\ning. In international conference on machine learning, pp.\\n1050–1059. PMLR, 2016.\\n\\nHe, K., Zhang, X., Ren, S., and Sun, J. Deep residual learn-\\ning for image recognition. In Proceedings of the IEEE\\nconference on computer vision and pattern recognition,\\npp. 770–778, 2016.\\n\\nHuynh, E. Vision transformers in 2022: An update on tiny\\n\\nimagenet. arXiv preprint arXiv:2205.10660, 2022.\\n\\nJøsang, A. Subjective logic, volume 3. Springer, 2016.\\n\\nKamath, U., Liu, J., and Whitaker, J. Deep learning for\\nNLP and speech recognition, volume 84. Springer, 2019.\\n\\nKingma, D. P. and Ba, J. Adam: A method for stochastic\\noptimization. arXiv preprint arXiv:1412.6980, 2014.\\n\\nKnopp, K. Weierstrass’s factor-theorem.\\nFunctions: Part II, pp. 1–7. Dover, 1996.\\n\\nIn Theory of\\n\\nKopetzki, A.-K., Charpentier, B., Z¨ugner, D., Giri, S., and\\nG¨unnemann, S. Evaluating robustness of predictive un-\\ncertainty estimation: Are dirichlet-based models reliable?\\nIn International Conference on Machine Learning, pp.\\n5707–5718. PMLR, 2021.\\n\\nKrizhevsky, A., Hinton, G., et al. Learning multiple layers\\n\\nof features from tiny images. -, 2009.\\n\\nLakshminarayanan, B., Pritzel, A., and Blundell, C. Simple\\nand scalable predictive uncertainty estimation using deep\\nensembles. Advances in neural information processing\\nsystems, 30, 2017.\\n\\nLeCun, Y. The mnist database of handwritten digits.\\n\\nhttp://yann. lecun. com/exdb/mnist/, 1998.\\n\\nLeCun, Y., Haffner, P., Bottou, L., and Bengio, Y. Ob-\\nject recognition with gradient-based learning. In Shape,\\ncontour and grouping in computer vision, pp. 319–345.\\nSpringer, 1999.\\n\\nMalinin, A. and Gales, M. Predictive uncertainty estima-\\ntion via prior networks. Advances in neural information\\nprocessing systems, 31, 2018.\\n\\nMalinin, A. and Gales, M. Reverse kl-divergence training\\nof prior networks: Improved uncertainty and adversarial\\nrobustness. Advances in Neural Information Processing\\nSystems, 32, 2019.\\n\\nMobiny, A., Yuan, P., Moulik, S. K., Garg, N., Wu, C. C.,\\nand Van Nguyen, H. Dropconnect is effective in modeling\\nuncertainty of bayesian deep networks. Scientific reports,\\n11(1):1–14, 2021.\\n\\nNetzer, Y., Wang, T., Coates, A., Bissacco, A., Wu, B.,\\nand Ng, A. Y. Reading digits in natural images with\\nunsupervised feature learning, 2011.\\n\\nNguyen, A., Yosinski, J., and Clune, J. Deep neural net-\\nworks are easily fooled: High confidence predictions for\\nunrecognizable images. In Proceedings of the IEEE con-\\nference on computer vision and pattern recognition, pp.\\n427–436, 2015.\\n\\nPandey, D. S. and Yu, Q. Multidimensional belief quantifi-\\ncation for label-efficient meta-learning. In Proceedings of\\nthe IEEE/CVF Conference on Computer Vision and Pat-\\ntern Recognition (CVPR), pp. 14391–14400, June 2022a.\\n\\nPandey, D. S. and Yu, Q. Evidential conditional neural\\nprocesses. arXiv preprint arXiv:2212.00131, 2022b.\\n\\nPearce, T., Leibfried, F., and Brintrup, A. Uncertainty in\\nneural networks: Approximately bayesian ensembling.\\nIn International conference on artificial intelligence and\\nstatistics, pp. 234–244. PMLR, 2020.\\n\\n10\\n\\n\\x0cLearn to Accumulate Evidence from All Training Samples: Theory and Practice\\n\\nSensoy, M., Kaplan, L., and Kandemir, M. Evidential deep\\nlearning to quantify classification uncertainty. Advances\\nin neural information processing systems, 31, 2018.\\n\\nShafer, G. A mathematical theory of evidence, volume 42.\\n\\nPrinceton university press, 1976.\\n\\nShi, W., Zhao, X., Chen, F., and Yu, Q. Multifaceted\\nuncertainty estimation for label-efficient deep learning.\\nAdvances in neural information processing systems, 33,\\n2020.\\n\\nSingh, S. P., Kumar, A., Darbari, H., Singh, L., Rastogi, A.,\\nand Jain, S. Machine translation using deep learning: An\\noverview. In 2017 international conference on computer,\\ncommunications and electronics (comptelix), pp. 162–\\n167. IEEE, 2017.\\n\\nTomani, C. and Buettner, F. Towards trustworthy predictions\\nfrom deep neural networks with fast adversarial calibra-\\ntion. In Proceedings of the AAAI Conference on Artificial\\nIntelligence, volume 35, pp. 9886–9896, 2021.\\n\\nUlmer, D. A survey on evidential deep learning for\\narXiv preprint\\n\\nsingle-pass uncertainty estimation.\\narXiv:2110.03051, 2021.\\n\\nVinyals, O., Blundell, C., Lillicrap, T., Wierstra, D., et al.\\nMatching networks for one shot learning. Advances in\\nneural information processing systems, 29, 2016.\\n\\nVoulodimos, A., Doulamis, N., Doulamis, A., and Protopa-\\npadakis, E. Deep learning for computer vision: A brief\\nreview. Computational intelligence and neuroscience,\\n2018, 2018.\\n\\nZhao, X., Chen, F., Hu, S., and Cho, J.-H. Uncertainty\\naware semi-supervised learning on graph data. Advances\\nin Neural Information Processing Systems, 33:12827–\\n12836, 2020.\\n\\n11\\n\\n\\x0cLearn to Accumulate Evidence from All Training Samples: Theory and Practice\\n\\nAppendix\\n\\nOrganization of the Appendix\\n\\n• In Section A, we present an analysis of standard classification models trained with cross-entropy loss to show their\\n\\nlearning capabilities.\\n\\n• In Section B, we present a complete proof of Theorem 1 for different evidential losses that demonstrates the inability of\\n\\nevidential models to learn from zero-evidence samples.\\n\\n• In Section C, we describe different incorrect evidence regularizations used in the existing literature and carry out a\\n\\ngradient analysis to study their impact on evidential model learning.\\n\\n• In Section D, we present the proof for Theorem 2 that shows the superiority of exp activation over the SoftPlus and\\n\\nReLU functions to transform logits to evidence.\\n\\n• In Section E, we analyze the evidential losses that reveals the theoretical limitation of evidential models trained using\\n\\nBayes risk with sum of squares loss.\\n\\n• In Section F, we present additional experiment results, clarifications, hyperparameter details, and discuss some\\n\\nlimitations along with possible future works.\\n\\nThe source code for the experiments carried out in this work is attached in the supplementary materials and is available at\\nthe link: https://github.com/pandeydeep9/EvidentialResearch2023\\n\\nA. Standard Classification Model\\n\\nConsider a standard cross-entropy based model for K−class classification. Let the overall network be represented by fΘ(.),\\nand let o = fΘ(x) be the output from this network before the softmax layer for input x and one-hot ground truth label of y.\\nThe output after the softmax layer is given by\\n\\nsmi =\\n\\nexp(oi)\\nk=1 exp(ok)\\n\\n(cid:80)K\\n\\n=\\n\\nexp(oi)\\nSce\\n\\n(17)\\n\\nWhere Sce = (cid:80)K\\n\\ni=1 exp(oi). The model is trained with cross-entropy loss. For a given sample (x, y), the loss is given by\\n\\nLcross-entropy = −\\n\\nK\\n(cid:88)\\n\\nk=1\\n\\nyk log(smk) = −\\n\\nK\\n(cid:88)\\n\\nk=1\\n\\n(cid:104)\\nykok − yk log\\n\\n(cid:16) K\\n(cid:88)\\n\\ni=1\\n\\n(cid:17)(cid:105)\\n\\nexp(oi)\\n\\n= log Sce −\\n\\nK\\n(cid:88)\\n\\nk=1\\n\\nykok\\n\\nNow, looking at the gradient of this loss with respect to the pre-softmax values o\\n\\ngradk =\\n\\n∂Lcross-entropy\\n∂ok\\n\\n=\\n\\n(cid:16) 1\\nSce\\n\\n∂Sce\\n∂ok\\n\\n(cid:17)\\n\\n=\\n\\n− yk\\n\\n(cid:16) exp(ok)\\nSce\\n\\n(cid:17)\\n\\n− yk\\n\\n= smk − yk\\n\\n(18)\\n\\n(19)\\n\\n(20)\\n\\nAnalysis of the gradients For Standard Classification Model.\\n\\nThe gradient measures the error signal, and for standard classification models, it is bounded in the range [-1, 1] as\\n0 ≤ smk ≤ 1 and yk ∈ {0, 1}. The model is updated using gradient descent based optimization objectives. For input x, the\\nneural network outputs K values o1 to oK, and the corresponding ground truth is y, ygt = 1, y̸=gt = 0.\\n\\nWhen yi = 0, the gradient signal is gradi = smi and the model optimizes the parameters to minimize this value. Only when\\nsmi = 0, the gradient is zero, and the model is not updated. In all other cases when smi ̸= 0, there is a non-zero gradient\\ndependent on smi, and the model is updated to minimize the smi as expected.\\n\\n12\\n\\n\\x0cLearn to Accumulate Evidence from All Training Samples: Theory and Practice\\n\\nWhen yi = 1, the gradient signal is gradi = smi − 1 and the model optimizes the parameters to minimize this value. As\\nsmi ∈ [0, 1], only when the model outputs a large logit on i (corresponding to the ground truth class) and small logit for\\nall other nodes, smi = 1, the gradient is zero, and the model is not updated. In all other cases when smi < 1, there is a\\nnon-zero gradient dependent on smi and the model is updated to maximize the smi and minimize all other sm̸=i as expected.\\nThe gradient signal in standard classification models trained with standard cross-entropy loss is reasonable and enables\\nlearning from all the training data samples.\\n\\nB. Evidential Classification Models\\n\\nTheorem 1: Given a training sample (x, y), if an evidential neural network outputs zero evidence e, then the gradients of\\nthe evidential loss evaluated on this training sample over the network parameters reduce to zero.\\n\\nProof. In the main paper, we considered a K−class classification problem and a representative evidential model trained\\nusing Bayes risk with sum of squares loss (Eqn. 21) in the proof. Following 3 variants of evidential losses ((Sensoy et al.,\\n2018)) have been commonly used in evidential classification works:\\n\\n1. Bayes risk with sum of squares loss (i.e., Evidential MSE loss) (Zhao et al., 2020)\\n\\nLMSE(x, y) =\\n\\nK\\n(cid:88)\\n\\n(yj −\\n\\nj=1\\n\\nαj\\nS\\n\\n)2 +\\n\\nαj(S − αj)\\nS2(S + 1)\\n\\n2. Bayes risk with cross-entropy loss (i.e., Evidential CE loss)(Charpentier et al., 2020)\\n\\nLCE(x, y) =\\n\\n(cid:16)\\n\\nyk\\n\\nK\\n(cid:88)\\n\\nj=1\\n\\nΨ(S) − Ψ(αk)\\n\\n(cid:17)\\n\\n3. Type II Maximum Likelihood loss (i.e., Evidential log loss)(Pandey & Yu, 2022a)\\n\\nLLog(x, y) =\\n\\n(cid:16)\\n\\nyk\\n\\nK\\n(cid:88)\\n\\nk=1\\n\\nlog(S) − log(αk)\\n\\n(cid:17)\\n\\n(21)\\n\\n(22)\\n\\n(23)\\n\\nFor completeness, we consider all three loss functions used in evidential classification models and carry out their analysis.\\n\\nB.1. Gradient of Evidential Activation Functions A(.)\\n\\nThree non-linear functions are proposed and commonly used in the existing literature to transform the neural network output\\nto evidence: 1) ReLU function, 2) SoftPlus function, and 3) Exponential function. In this section, we compute the\\ngradients of the evidence output ei from these non-linear activation functions with respect to the logit input oi\\n\\n1. A(.) = ReLU(.) = max(0, .)\\n\\nek = ReLU(ok) = max(0, ok) =⇒\\n\\n∂ek\\n∂ok\\n\\n=\\n\\n(cid:40)\\n0\\n1\\n\\nok ≤ 0\\n\\nif\\notherwise\\n\\n2. A(.) = SoftPlus(.) = log(1 + exp(.))\\n\\nek = log(exp(ok) + 1) =⇒\\n\\n∂ek\\n∂ok\\n\\n=\\n\\n1\\n1 + exp(−ok)\\n\\n= Sigmoid(ok)\\n\\n3. A(.) = exp(.)\\n\\nek = exp(ok) =⇒\\n\\n∂ek\\n∂ok\\n\\n13\\n\\n= exp(ok) = ek = αk − 1\\n\\n(24)\\n\\n(25)\\n\\n(26)\\n\\n\\x0cLearn to Accumulate Evidence from All Training Samples: Theory and Practice\\n\\nB.2. Evidential Model Trained using Bayes risk with sum of squares loss (i.e., Eqn. 21)\\n\\nProof. Consider an input x with one-hot ground truth label of y. Let the ground truth class be g i.e. ygt = 1, with\\ncorresponding Dirichlet parameter αgt, and y̸=gt = 0. Moreover, let o, e, and α represent the neural network output vector\\nbefore applying the activation A, the evidence vector, and the Dirichlet parameters respectively.\\n\\nIn this evidential framework, the loss is given by\\n\\nLMSE(x, y) =\\n\\nK\\n(cid:88)\\n\\nj=1\\n\\n(yj −\\n\\n= 2 −\\n\\n2αgt\\nS\\n\\nαj\\nS\\n\\n−\\n\\n)2 +\\n\\nαj(S − αj)\\nS2(S + 1)\\n\\n= 1 −\\n\\n2αgt\\nS\\n\\n+\\n\\n(cid:80)\\n\\nk α2\\nk\\nS2 +\\n\\n2 (cid:80)\\ni\\n\\n(cid:80)\\n\\nj αiαj\\n\\nS2(S + 1)\\n\\n2 (cid:80)\\ni\\n\\n(cid:80)\\n\\nj αiαj\\n\\nS(S + 1)\\n\\n(27)\\n\\n(28)\\n\\nNow, consider different components of the loss and compute the gradients of the components with respect to Dirichlet\\nparameters α,\\n\\nαgt\\nS2 &\\nThe gradient of the variance term is the same for all the K Dirichlet parameters and is given by\\n\\nαgt\\nS2 =⇒\\n\\nαgt\\nS2\\n\\nyk\\nS\\n\\n= −\\n\\n1\\nS\\n\\n=\\n\\n−\\n\\n=\\n\\n−\\n\\n∂ αgt\\nS\\n∂α̸=gt\\n\\n∂ αgt\\nS\\n∂αgt\\n\\n∂ αgt\\nS\\n∂αk\\n\\n∂\\n\\nj αiαj\\n\\n(cid:80)\\n\\n(cid:80)\\ni\\nS(S+1)\\n∂αk\\n\\n=\\n\\n(S − αk)\\nS(S + 1)\\n\\n−\\n\\n(2S + 1) (cid:80)\\n\\ni\\n\\n(cid:80)\\n\\nj αiαj\\n\\n(S2 + S)2\\n\\nNow, the gradient of the loss with respect to the neural network output can be computed using the chain rule as\\n\\n=\\n\\n∂LMSE(x, y)\\n∂ok\\n(cid:20) 2αgt\\n\\n=\\n\\nS2 − 2\\n\\nyk\\nS\\n\\n∂ek\\n∂ok\\n\\n∂LMSE(x, y)\\n∂αk\\n2(S − αk)\\nS(S + 1)\\n\\n−\\n\\n+\\n\\n(cid:20)\\n2\\n\\n= −\\n\\n∂ αk\\nS\\n∂αk\\n\\n− 2\\n\\n2(2S + 1) (cid:80)\\ni\\n\\n(cid:80)\\n\\nj αiαj\\n\\n(cid:80)\\n\\n(cid:80)\\n\\n∂\\n\\ni\\nS(S+1)\\n∂αk\\n(cid:21)\\nj αiαj\\n\\n(cid:21)\\n\\n×\\n\\n∂ek\\n∂ok\\n\\n×\\n\\n∂ek\\n∂ok\\n\\n(S2 + S)2\\n\\nCase I: ReLU(.) to transform logits to evidence\\n\\nek = ReLU(ok) = max(0, ok) =⇒\\n\\n∂ek\\n∂ok\\n\\n=\\n\\n(cid:40)\\n\\n1\\no\\n\\nok > 0\\n\\nif\\notherwise\\n\\n(29)\\n\\nFor zero-evidence sample with ReLU(.) used to transform the logits to evidence, the logits ok satisfy the relationship\\nok ≤ 0 ∀ k =⇒ ∂ek\\n∂ok\\n\\n= 0 =⇒ ∂LMSE(x,y)\\n\\n= 0\\n\\n∂ok\\n\\nCase II: SoftPlus(.) to transform logits to evidence\\n\\nCase II: exp(.) to transform logits to evidence\\n\\nek = log(exp(ok) + 1) =⇒\\n\\n∂ek\\n∂ok\\n\\n= Sigmoid(ok)\\n\\n(30)\\n\\nek = exp(ok) =⇒\\n\\n∂ek\\n∂ok\\nFor zero-evidence sample with SoftPlus(.) used to transform the logits to evidence, the logits ok → −∞ =⇒\\nSigmoid(ok) → 0 & ∂ek\\n→ 0. For zero-evidence sample with exp(.) used to transform the logits to evidence, αk →\\n∂ok\\n1 =⇒ ∂ek\\n→ 0. Moreover, there is no term in the first part of the loss gradient (see Eqn. 29) to counterbalance these\\n∂ok\\nzero-approaching gradients. So, for zero-evidence samples,\\n\\n= exp(ok) = αk − 1\\n\\n(31)\\n\\nSince the gradient of the loss with respect to all the nodes is zero, there is no update to the model from such samples. Thus,\\nthe evidential models fail to learn from such zero-evidence samples.\\n\\n∂LMSE(x, y)\\n∂ok\\n\\n= 0\\n\\n(32)\\n\\n14\\n\\n\\x0cLearn to Accumulate Evidence from All Training Samples: Theory and Practice\\n\\nB.3. Evidential Model Trained using Type II Maximum Likelihood formulation of Evidential loss (i.e., Eqn. 23)\\n\\nConsider a K−class evidential classification model that trains the model using Type II Maximum Likelihood formulation of\\nthe evidential loss. Consider an input x with one-hot ground truth label of y, (cid:80)K\\nk=1 yk = 1. For this evidential framework,\\nthe Type II Maximum Likelihood loss is given by\\n\\nLLog(x, y) =\\n\\n(cid:16)\\n\\nyk\\n\\nK\\n(cid:88)\\n\\nk=1\\n\\nlog(S) − log(αk)\\n\\n(cid:17)\\n\\n= log S −\\n\\nK\\n(cid:88)\\n\\nk=1\\n\\nyk log αk\\n\\nTaking the gradient of the loss with the logits o, we get\\n\\ngradk =\\n\\n∂LLog(x, y)\\n∂ok\\n\\n=\\n\\n1\\nS\\n\\n∂S\\n∂ok\\n\\n− yk\\n\\n1\\nαk\\n\\n∂αk\\n∂ok\\n\\n=\\n\\n(cid:16) 1\\nS\\n\\n−\\n\\nyk\\nαk\\n\\n(cid:17) ∂ek\\n∂ok\\n\\nCase I: ReLU(.) to transform logits to evidence\\n\\n(33)\\n\\n(34)\\n\\nFor any zero-evidence sample with ReLU(.) used to transform the logits to evidence, the logits ok satisfy the relationship\\nok ≤ 0 ∀ k =⇒ ∂ek\\n∂ok\\n\\n= 0 =⇒ ∂LLog(x,y)\\n\\n= 0 ∀k ∈ [1, K]\\n\\n∂ok\\n\\nCase II: SoftPlus(.) to transform logits to evidence. Considering Eqn. 34 and Eqn 25, the gradient of the loss with\\nrespect to the logits becomes\\n\\ngradk =\\n\\n∂LLog(x, y)\\n∂ok\\n\\n=\\n\\n(cid:16) 1\\nS\\n\\n−\\n\\n(cid:17)\\n\\nyk\\nαk\\n\\nSigmoid(ok)\\n\\n(35)\\n\\nCase III: exp(.) to transform logits to evidence. Considering Eqn. 34 and Eqn 26, the gradient of the loss with respect to\\nthe logits becomes\\n\\ngradk =\\n\\n∂LLog(x, y)\\n∂ok\\n\\n=\\n\\n(cid:16) 1\\nS\\n\\n−\\n\\n(cid:17)\\n\\nyk\\nαk\\n\\n(ek) =\\n\\n(cid:16) 1\\nS\\n\\n−\\n\\n(cid:17)\\n\\nyk\\nαk\\n\\n(αk − 1)\\n\\n(36)\\n\\nFor zero-evidence sample with SoftPlus(.) used to transform the logits to evidence, the logits ok → −∞ =⇒\\nSigmoid(ok) → 0 & ∂ek\\n→ 0. Similarly, for zero-evidence sample with exp(.) used to transform the logits to evidence,\\n∂ok\\nαk → 1 =⇒ ∂ek\\n→ 0. Moreover, there is no term in the first part of the loss gradient (see Eqn. 35 and Eqn. 36 ) to\\n∂ok\\ncounterbalance these zero-approaching gradient terms.\\n\\nSince the gradient of the loss with respect to all the nodes is zero, there is no update to the model from such samples. Thus,\\nthe evidential models trained with Type II Maximum Likelihood formulation of the evidential loss fail to learn from such\\nzero-evidence samples.\\n\\nB.4. Evidential Model Trained using Bayes risk with cross-entropy formulation of Evidential loss (i.e., Eqn. 22)\\n\\nConsider a K−class evidential classification model that trains model using Bayes risk with cross-entropy loss for evidential\\nlearning (Eqn. 22). Consider an input x with one-hot ground truth label of y, (cid:80)K\\nk=1 yk = 1. For this evidential framework,\\nthe loss is given by\\n\\nLCE(x, y) =\\n\\n(cid:16)\\n\\nyk\\n\\nK\\n(cid:88)\\n\\nj=1\\n\\nΨ(S) − Ψ(αk)\\n\\n(cid:17)\\n\\n= Ψ(S) − Ψ(αgt)\\n\\n(37)\\n\\nWhere αgt represents the output Dirichlet parameter for the ground truth class i.e. ygt = 1, y̸=gt = 0, and Ψ(.) represents\\nthe Digamma function, and for z ≥ 1, is given by\\n\\nΨ(z) =\\n\\nd\\ndz\\n\\nlog Γ(z) =\\n\\n(cid:18)\\n\\nd\\ndz\\n\\n− γz − log z +\\n\\n∞\\n(cid:88)\\n\\nn=1\\n\\n(cid:16) z\\nn\\n\\n− log (cid:0)1 +\\n\\n(cid:1)(cid:17)(cid:19)\\n\\nz\\nn\\n\\n= −γ −\\n\\n1\\nz\\n\\n+ z\\n\\n∞\\n(cid:88)\\n\\nn=1\\n\\n1\\nn(n + z)\\n\\n15\\n\\n\\x0cLearn to Accumulate Evidence from All Training Samples: Theory and Practice\\n\\nHere, γ is the Euler–Mascheroni constant, and Γ(.) is the gamma function, Using Weierstass’s definition of gamma function\\n(Knopp, 1996) for values outside negative integers that is given by\\n\\nΓ(z) =\\n\\ne−γz\\nz\\n\\n∞\\n(cid:89)\\n\\n(cid:16)\\n\\n1 +\\n\\nn=1\\n\\n(cid:17)−1\\n\\nz\\nn\\n\\ne\\n\\nz\\nn\\n\\nUsing the definition of the digamma functions, the loss updates as\\n\\nLCE(x, y) = Ψ(S) − Ψ(αgt) =\\n\\n1\\nαgt\\n\\n−\\n\\n1\\nS\\n\\n+ S\\n\\n∞\\n(cid:88)\\n\\nn=1\\n\\n1\\nn(n + S)\\n\\n− αgt\\n\\n∞\\n(cid:88)\\n\\nn=1\\n\\n1\\nn(n + αgt)\\n\\n(38)\\n\\nThe derivative of the digamma function is bounded and is given by\\n\\n(cid:18)\\n\\n∂Ψ(z)\\n∂z\\n\\n=\\n\\n∂\\n∂z\\n\\n− γ −\\n\\n1\\nz\\n\\n+\\n\\n∞\\n(cid:88)\\n\\nn=1\\n\\n1\\nn\\n\\n(cid:19)\\n\\n−\\n\\n1\\nn + z\\n\\n=\\n\\n1\\nz2 +\\n\\n∞\\n(cid:88)\\n\\nn=1\\n\\n1\\n(n + z)2\\n\\n1\\nz2 <\\n\\n∂Ψ(z)\\n∂z\\n\\n<\\n\\n1\\nz2 +\\n\\nπ2\\n6\\n\\n,\\n\\nz ≥ 1\\n\\nWith this, we can compute the gradients of the loss with respect to the logits as\\n\\ngradk =\\n\\n∂LCE(x, y)\\n∂ok\\n\\n=\\n\\n∂\\n∂αk\\n\\n(cid:0)Ψ(S) − Ψ(αgt)(cid:1) ∂αk\\n∂ok\\n\\n=\\n\\n(cid:16) 1\\nS2 +\\n\\n∞\\n(cid:88)\\n\\ni=1\\n\\n1\\n(n + S)2 −\\n\\nyk\\nα2\\ngt\\n\\n−\\n\\n∞\\n(cid:88)\\n\\ni=1\\n\\nyk\\n(n + αgt)2\\n\\n(cid:17) ∂ek\\n∂ok\\n\\n(39)\\n\\nCase I: ReLU(.) to transform logits to evidence\\n\\nFor any zero-evidence sample with ReLU(.) used to transform the logits to evidence, the logits ok satisfy the relationship\\nok ≤ 0 ∀ k =⇒ ∂ek\\n∂ok\\n\\n= 0 =⇒ ∂LCE(x,y)\\n\\n= 0 ∀k ∈ [1, K]\\n\\n∂ok\\n\\nCase II: SoftPlus(.) to transform logits to evidence. Considering Eqn. 25 and Eqn 39, the gradient of the loss with\\nrespect to the logits becomes\\n\\ngradk =\\n\\n∂LCE(x, y)\\n∂ok\\n\\n=\\n\\n(cid:16) 1\\nS2 +\\n\\n∞\\n(cid:88)\\n\\ni=1\\n\\n1\\n(n + S)2 −\\n\\nyk\\nα2\\ngt\\n\\n−\\n\\n∞\\n(cid:88)\\n\\ni=1\\n\\nyk\\n(n + αgt)2\\n\\n(cid:17)\\n\\nSigmoid(ok)\\n\\n(40)\\n\\nCase III: exp(.) to transform logits to evidence. Considering Eqn. 26 and Eqn 39, the gradient of the loss with respect to\\nthe logits becomes\\n\\ngradk =\\n\\n∂LCE(x, y)\\n∂ok\\n\\n=\\n\\n(cid:16) 1\\nS2 +\\n\\n∞\\n(cid:88)\\n\\ni=1\\n\\n1\\n(n + S)2 −\\n\\nyk\\nα2\\ngt\\n\\n−\\n\\n∞\\n(cid:88)\\n\\ni=1\\n\\nyk\\n(n + αgt)2\\n\\n(cid:17)\\n\\n(αk − 1)\\n\\n(41)\\n\\nFor zero-evidence sample with SoftPlus(.) used to transform the logits to evidence, the logits ok → −∞ =⇒\\nSigmoid(ok) → 0 & ∂ek\\n→ 0. Similarly, for zero-evidence sample with exp(.) used to transform the logits to evidence,\\n∂ok\\nαk → 1 =⇒ ∂ek\\n→ 0. Moreover, there is no term in the first part of the loss gradient (see Eqn. 29) to counterbalance\\n∂ok\\nthese zero-approaching gradient terms.\\n\\nThe gradient of the loss with respect to all the nodes is zero for all the considered cases. Since the gradient of the loss with\\nrespect to all the nodes is zero for all three cases, there is no update to the model from such samples. Thus, the evidential\\nmodels fail to learn from such zero-evidence samples in all cases.\\n\\nC. Regularization in the Evidential Classification Models\\n\\nBased on the evidence e, beliefs b, and the Dirichlet parameters α, various regularization terms have been introduced that\\naim to penalize the incorrect evidence/incorrect belief of the model, leading to the model with accurate uncertainty estimates.\\nHere, we briefly summarize the key regurlaizations:\\n\\n16\\n\\n\\x0cLearn to Accumulate Evidence from All Training Samples: Theory and Practice\\n\\n1. Introduce a forward KL regularization term as in EDL (Sensoy et al., 2018) that regularizes the model to output no\\n\\nincorrect evidence.\\n\\nreg(x, y) = KL(cid:0)Dir(p| ˜α)||Dir(p|1)(cid:1) = log\\nLEDL\\n\\n(cid:16) Γ (cid:80)K\\nΓ(K) (cid:81)K\\n\\nk=1 ˜αk\\nk=1 Γ˜αk\\n\\n(cid:17)\\n\\nK\\n(cid:88)\\n\\n(cid:20)\\n(˜αk − 1)\\n\\n+\\n\\nψ(˜αk) − ψ\\n\\n(cid:16) K\\n(cid:88)\\n\\n(cid:17)(cid:21)\\n\\n˜αj\\n\\n(42)\\n\\nk=1\\n\\nj=1\\n\\nWhere ˜α = y + (1 − y) ⊙ α = (˜α1, ˜α2, ...˜αN ) parameterize a dirichlet distribution, ˜αi=gt = 1, ˜αi = αi∀i ̸= gt.\\nHere, the KL regularization term encourages the Dirichlet distribution based on the incorrect evidence i.e., Dir(p| ˜α)\\nto be flat which is possible when there is no incorrect evidence. From Eqn. 42, we can see that the regularization\\nterm, introduces digamma functions for the loss and may require evaluation of higher-order polygamma functions for\\nchallenging problems (e.g. involving bi-level optimizations as in MAML (Finn et al., 2017)).\\n\\n2. Introduce an incorrect evidence regularization term as in ADL (Shi et al., 2020) that is the sum of the incorrect evidence\\n\\nfor a sample\\n\\nLADL\\n\\nreg(x, y) =\\n\\nK\\n(cid:88)\\n\\nk=1\\n\\n(cid:0)e ⊙ (1 − y)(cid:1)\\n\\nk =\\n\\nK\\n(cid:88)\\n\\nk=1\\n\\nek × (1 − yk)\\n\\n(43)\\n\\nHere, ⊙ represents element-wise product. The evidence for a class ek is only restricted to be non-negative and can take\\nlarge positive values leading to large variation in the overall loss.\\n\\n3. Introduce incorrect belief-based regularization as in Units-ML (Pandey & Yu, 2022a)\\n\\nLUnits\\nreg\\n\\n(x, y) =\\n\\nK\\n(cid:88)\\n\\nk=1\\n\\n(cid:0) e\\nS\\n\\n⊙ (1 − y)(cid:1)\\n\\nk =\\n\\nK\\n(cid:88)\\n\\nk=1\\n\\nek\\nS\\n\\n× (1 − yk)\\n\\n(44)\\n\\nThe regularization value is bounded to be in a range of [0, 1] for all the data samples, no matter how severe the mistake\\nis.\\n\\nAll three regularizations aim to guide the model such that the incorrect evidence is minimized (ideally close to zero). These\\nregularizations help the evidential model acquire desired uncertainty quantification capabilities in evidential models. Such\\nguidance is expected to update the model such that it maps input samples near zero-evidence regions in the evidence space.\\nThus, the regularization does not help address the issue of learning from zero-evidence samples and is likely to hurt the\\nmodel’s learning capabilities.\\n\\nC.1. Gradient Analysis of the Incorrect Evidence Regularizations\\n\\nThe regularization terms use ground truth information to consider only the incorrect evidence. Thus, the gradient of the\\nregularization loss with respect to the ground truth node αgt is 0. In this analysis, we consider the gradient with respect to\\nnon-ground truth nodes i.e. αk, and ok, k ̸= gt.\\n\\n1. Gradient for EDL regularization (Eqn. 42 )\\n\\nreg(x, y) = KL(cid:0)Dir(p| ˜α)||Dir(p|1)(cid:1) = log\\nLEDL\\n\\n= log Γ(S − αgt) − log Γ(K) −\\n\\nK\\n(cid:88)\\n\\nk=1\\n\\n(cid:16) Γ (cid:80)K\\nΓ(K) (cid:81)K\\n\\nk=1 ˜αk\\nk=1 Γ˜αk\\nK\\n(cid:88)\\n\\nlog Γ˜αk +\\n\\n(˜αk − 1)\\n\\n(cid:17)\\n\\nK\\n(cid:88)\\n\\n(cid:20)\\n(˜αk − 1)\\n\\n+\\n\\nψ(˜αk) − ψ\\n\\nk=1\\n(cid:21)\\n(cid:20)\\nψ(˜αk) − ψ(S − αgt)\\n\\n(cid:16) K\\n(cid:88)\\n\\n(cid:17)(cid:21)\\n\\n˜αj\\n\\nj=1\\n\\n(45)\\n\\nk=1\\n\\n17\\n\\n\\x0cLearn to Accumulate Evidence from All Training Samples: Theory and Practice\\n\\n∂LEDL\\n\\nreg(x, y)\\n∂αk\\n\\n=\\n\\n∂\\n∂αk\\n\\n(cid:18)\\n\\nlog Γ(S − αgt) − log Γ(K) −\\n\\nK\\n(cid:88)\\n\\nk=1\\n\\nlog Γ˜αk +\\n\\nK\\n(cid:88)\\n\\n(˜αk − 1)\\n\\n(cid:20)\\nψ(˜αk) − ψ(S − αgt)\\n\\n(cid:21)(cid:19)\\n\\n= ψ(S − αgt) − ψ(αk) +\\n\\n∂\\n∂αk\\n\\nk=1\\n\\n(cid:18) K\\n(cid:88)\\n\\n(cid:20)\\nψ(˜αk) − ψ(S − αgt)\\n(˜αk − 1)\\n\\nk=1\\n(cid:21)(cid:19)\\n\\n= ψ(S − αgt) − ψ(αk) + ψ(αk) − ψ(S − αgt) + (αk − 1)\\n\\n(cid:18)\\n\\n∂\\n∂αk\\n\\nψ(˜αk) − ψ(S − αgt)\\n\\n(cid:19)\\n\\n= (αk − 1)\\n\\n∂\\n∂αk\\n\\n(cid:18)\\n\\n(cid:19)\\n\\nψ(αk) − ψ(S − αgt)\\n\\n= (αk − 1)(cid:0)ψ1(αk) − ψ1(S − αgt)(cid:1)\\n\\nWhere ψ1 is the trigamma function. Further, using the definition of trigamma function,\\n\\n∂LEDL\\n\\nreg(x, y)\\n∂αk\\n\\n= (αk − 1)(cid:0)ψ1(αk) − ψ1(S − αgt)(cid:1) = (αk − 1)\\n\\n(cid:18) ∞\\n(cid:88)\\n\\nn=0\\n\\n1\\n(n + αk)2 −\\n\\n1\\n(n + S − αgt)2\\n\\n(cid:19)\\n\\nNow, the gradients with respect to the logits ok becomes\\n\\n∂LEDL\\n\\nreg(x, y)\\n∂ok\\n\\n=\\n\\n∂LEDL\\n\\nreg(x, y)\\n∂αk\\n\\n∂αk\\n∂ok\\n\\n= (αk − 1)\\n\\n(cid:18) ∞\\n(cid:88)\\n\\nn=0\\n\\n1\\n(n + αk)2 −\\n\\n1\\n(n + S − αgt)2\\n\\n(cid:19)\\n\\n×\\n\\n∂ek\\n∂ok\\n\\n(46)\\n\\n(47)\\n\\nCase I: ReLU(.) to transform logits to evidence. The gradients with respect to the logits ok for zero evidence is zero.\\nFor all non-zero evidence, the gradient updates as ∂ek\\n∂ok\\n\\n= 1∀ek > 0 and\\n\\n∂LEDL\\n\\nreg(x, y)\\n∂ok\\n\\n(cid:18) ∞\\n(cid:88)\\n\\n= (αk − 1)\\n\\nn=0\\n\\n1\\n(n + αk)2 −\\n\\n1\\n(n + S − αgt)2\\n\\n(cid:19)\\n\\n(48)\\n\\nNow, when αk → ∞, the value of the gradient\\nfor very large incorrect evidence.\\nCase II: SoftPlus(.) to transform logits to evidence. The gradients with respect to the logits ok is given by the\\nsigmoid i.e. ∂ek\\n∂ok\\n\\n→ 0. There is close to zero model update from regularization\\n\\n= sigmoid(ok) , limok→∞\\n\\n= 1, and\\n\\n∂ek\\n∂ok\\n\\n∂LEDL\\n\\nreg(x,y)\\n∂ok\\n\\n∂LEDL\\n\\nreg(x, y)\\n∂ok\\n\\n= (αk − 1)\\n\\n(cid:18) ∞\\n(cid:88)\\n\\nn=0\\n\\n1\\n(n + αk)2 −\\n\\n1\\n(n + S − αgt)2\\n\\n(cid:19)\\n\\nσ(αk − 1)\\n\\n(49)\\n\\nNow, similar to ReLU, when αk → ∞, the value of the gradient\\nfrom regularization for very large incorrect evidence.\\n\\n∂LEDL\\n\\nreg(x,y)\\n∂ok\\n\\n→ 0. There is close to zero model update\\n\\nCase III: exp(.) to transform logits to evidence. When using exponential non-linearity to transform the neural network\\noutput to evidence, the αk is given by αk = exp(ok) + 1, ∂αk\\n= αk − 1. Now the gradients with respect to the neural\\n∂ok\\nnetwork output ok becomes:\\n\\n∂L2\\n\\nreg(x, y)\\n∂ok\\n\\n=\\n\\n∂L2\\n\\nreg(x, y)\\n∂αk\\n\\n×\\n\\n∂αk\\n∂ok\\n\\n= (αk − 1)2\\n\\n(cid:18) ∞\\n(cid:88)\\n\\nn=0\\n\\n1\\n(n + αk)2 −\\n\\n1\\n(n + S − αgt)2\\n\\n(cid:19)\\n\\n(50)\\n\\nHere, the gradient values increase as αk → ∞, and the gradient values do not vanish. Simply, as the incorrect evidence\\nbecomes very large, the model updates also become large in the accurate direction.\\n\\nThus, considering Case I, II, and II, we see that the incorrect evidence-based regularization with forward KL divergence\\nis not effective in regions of incorrect evidence when using ReLu and SoftPlus functions to transform logits to\\nevidence. This issue of correcting very large incorrect evidence does not appear when using exp function to transform\\nthe logits into evidence.\\n\\n18\\n\\n\\x0cLearn to Accumulate Evidence from All Training Samples: Theory and Practice\\n\\n2. Gradient for ADL regularization ((Shi et al., 2020) )\\n\\nLADL\\n\\nreg(x, y) =\\n\\nK\\n(cid:88)\\n\\nk=1\\n\\n(cid:0)e ⊙ (1 − y)(cid:1)\\n\\nk =\\n\\nK\\n(cid:88)\\n\\nk=1\\n\\nek × (1 − yk) = S − K − αgt + 1\\n\\n(51)\\n\\nConsidering the gradient of the regularization with respect to the parameters αk, k ̸= gt, and corresponding logits ok,\\nwe get\\n\\n∂LADL\\n\\nreg(x, y)\\n∂αk\\n\\n= 1 =⇒\\n\\n∂LADL\\n\\nreg(x, y)\\n∂ok\\n\\n=\\n\\n∂ek\\nok\\n\\n(52)\\n\\nWhen considering the exp function to transform logits to evidence, ∂ek\\n= ek = exp(ok) and the gradient value\\nok\\nbecomes very large when the model’s predicted incorrect evidence value is large. This may lead to exploding gradients\\nand stability issues in the model training. For ReLU and SoftPlus functions, the gradients in positive evidence\\nregions are ∂ek\\n= σ(ok) respectably. Thus, the gradient and corresponding model updates for high\\nok\\nincorrect evidence are as desired.\\n\\n= 1, and ∂ek\\nok\\n\\n3. Gradient analysis of incorrect belief regularization term as in Units-ML(Pandey & Yu, 2022a)\\n\\nLUnits\\nreg\\n\\n(x, y) =\\n\\nK\\n(cid:88)\\n\\nk=1\\n\\n(cid:0) e\\nS\\n\\n⊙ (1 − y)(cid:1)\\n\\nk =\\n\\nK\\n(cid:88)\\n\\nk=1\\n\\nek\\nS\\n\\n× (1 − yk) =\\n\\n(cid:0)S − K − αgt + 1(cid:1)\\n\\n1\\nS\\n\\n(53)\\n\\nThe regularization value is bounded to be in a range of [0, 1] for all the data samples, no matter how severe the mistake\\nwhich may limit its effectiveness. Next, the gradient of the regularization with respect to the parameters αk, and logits\\nok is given by\\n\\n(x, y)\\n\\n∂LUnits\\nreg\\n∂αk\\n\\n=\\n\\n(cid:16) 1\\nS\\n\\n∂\\n\\n(cid:0)S − K − αgt + 1(cid:1)(cid:17)\\n\\n∂αk\\n\\n=\\n\\nαgt + K − 1\\nS2\\n\\n=\\n\\negt + K\\n\\n(K + (cid:80)K\\n\\nk=1 ek)2\\n\\n∂L3\\n\\nreg(x, y)\\n∂ok\\n\\n=\\n\\n(x, y)\\n\\n∂LUnits\\nreg\\n∂αk\\n\\n×\\n\\n∂αk\\n∂ok\\n\\n=\\n\\negt + K\\n\\nS2 ×\\n\\n∂ek\\n∂ok\\n\\n(54)\\n\\n(55)\\n\\nThe gradient value decreases as the number of classes K in the classification problem increases. For all three\\ntransformations: ReLU, SoftPlus, and exp to transform logits to evidence, the gradients will go to zero as the\\nincorrect evidence increases i.e. ek → ∞ and S → ∞ =⇒\\n→ 0. So, the regularization may be ineffective\\nwhen the incorrect evidence is very high.\\n\\nreg(x,y)\\n∂ok\\n\\n∂L3\\n\\nD. Impact of Non-linear Transformation\\n\\nTheorem 2: For a data sample x, if an evidential model outputs logits ok ≤ 0 ∀k ∈ [0, K], the exponential activation\\nfunction leads to a larger gradident update on the model parameters than softplus and ReLu.\\n\\nProof. Consider an evidential loss L, which is formally defined in Eqns. (21), (22), and (23), is used to train the evidential\\nmodel, let o, e ∈ RK denote the neural network output vector before applying the activation A, and the evidence vector,\\nrespectively, for a network with weight w. For a data sample x, if the network outputs ok < 0, ∀k ∈ [K], we have:\\n\\n1. ReLu:\\n\\n2. SoftPlus:\\n\\n∂L1\\n∂w\\n\\n(cid:88)\\n\\n=\\n\\nk\\n\\n∂L1\\n∂ek\\n\\n∂ek\\n∂ok\\n\\n∂ok\\n∂w\\n\\n= 0\\n\\n(see Eqn. 8),\\n\\n∂L2\\n∂w\\n\\n(cid:88)\\n\\n=\\n\\nk\\n\\n∂L2\\n∂ek\\n\\n∂ek\\n∂ok\\n\\n∂ok\\n∂w\\n\\n(cid:88)\\n\\n=\\n\\nk\\n\\n∂L2\\n∂ek\\n\\n∂ok\\n∂w\\n\\nSigmoid(ok)\\n\\n( see Eqn. 9),\\n\\n19\\n\\n\\x0cLearn to Accumulate Evidence from All Training Samples: Theory and Practice\\n\\n3. Exponential:\\n\\n∂L3\\n∂w\\n\\n(cid:88)\\n\\n=\\n\\nk\\n\\n∂L3\\n∂ek\\n\\n∂ek\\n∂ok\\n\\n∂ok\\n∂w\\n\\n(cid:88)\\n\\n=\\n\\nk\\n\\n∂L3\\n∂ek\\n\\n∂ok\\n∂w\\n\\nexp(ok) =\\n\\n(cid:88)\\n\\nk\\n\\n∂L3\\n∂ek\\n\\n∂ok\\n∂w\\n\\n{[1 + exp(ok)]Sigmoid(ok)}\\n\\n(see Eqn. 10)\\n\\nThus, we have ∂L3\\nand ReLu. This completes the proof. Now we carry out an analysis of the three activations.\\n\\n∂w , which implies that A = exp leads to a larger update to the network than both Softplus\\n\\n∂w ≥ ∂L2\\n\\n∂w ≥ ∂L1\\n\\nAnalysis:\\n\\nConsider a representative K−class evidential classification model that trains using Type II Maximum Likelihood evidential\\nloss. Consider an input x with one-hot label of y, (cid:80)K\\nk=1 yk = 1. For this evidential framework, the Type II Maximum\\nLikelihood loss (LLog(x, y)) and its gradient with the logits o ( Eqn. 34) are given by\\n\\nLLog(x, y) = log S −\\n\\nK\\n(cid:88)\\n\\nk=1\\n\\nyk log αk & gradk =\\n\\n∂LLog(x, y)\\n∂ok\\n\\n=\\n\\n(cid:16) 1\\nS\\n\\n−\\n\\nyk\\nαk\\n\\n(cid:17) ∂ek\\n∂ok\\n\\n(56)\\n\\nCase I and II: ReLU(.) and SoftPlus(.) to transform logits to evidence.\\n\\n• Zero evidence region: For ReLU(.) based evidential models, if the logits value for class k i.e. ok is negative, then\\nthe corresponding evidence for class k i.e. ek = 0, ∂ek\\n= 0. So, there is no update to\\n∂ok\\nthe model through the nodes that output negative logits value. In the case of SoftPlus(.) based evidential models,\\nthere is no update to the model when training samples lie in zero-evidence regions. This is possible in the condition of\\nok → −∞. In other cases, there will be some small finite small update in the accurate direction from the gradient.\\n\\n= 0 & gradk = ∂LLog(x,y)\\n\\n∂ok\\n\\n• Range of gradients: The range of gradients for both ReLU(.) and SoftPlus(.) based evidential models are\\nidentical. Considering the gradient for the ground truth node i.e.yk = 1, the range of gradients is [ 1\\nK − 1, 0]. For\\nall other nodes other than the ground truth node i.e. yk = 0, the range of gradients is [0, 1\\nK ]. So, for classification\\nproblems with a large number of classes, the gradient updates to the nodes that do not correspond to the ground truth\\nclass will be bounded in a small range and is likely to be very small.\\n\\n= Sigmoid(ok) → 1, 1\\nαk\\n\\n• High incorrect evidence region: If the evidence for class k is very large i.e. ek → ∞, then for ReLU(.), ∂ek\\nok\\nand for SoftPlus(.), ∂ek\\n= 1\\n→ 0. For large\\nok\\npositive model evidence, there is no update to the corresponding node of the neural network. The evidence can be\\nfurther broken down into correct evidence (corresponding to the evidence for the ground truth class), and incorrect\\nevidence (corresponding to the evidence for any other class other than the ground truth class). When the correct class\\nevidence is large, the corresponding gradient is close to zero and there is no update to the model parameters which\\nis desired. When the incorrect evidence is large, the model should be updated to minimize such incorrect evidence.\\nHowever, the evidential models with ReLU and Softplus fail to minimize incorrect evidence when the incorrect\\nevidence value is large. These necessities the need for incorrect evidence regularization terms.\\n\\nS → 0, & gradk = ∂LLog(x,y)\\n\\nek+1 → 0, 1\\n\\n= 1,\\n\\n∂ok\\n\\nCase III: exp(.) to transform logits to evidence. Considering Eqn. Eqn. 34 and Eqn 26, the gradient of the loss with respect\\nto the logits becomes\\n\\ngradk =\\n\\n∂LLog(x, y)\\n∂ok\\n\\n=\\n\\n(cid:16) 1\\nS\\n\\n−\\n\\n(cid:17)\\n\\nyk\\nαk\\n\\n(ek) =\\n\\n(cid:16) 1\\nS\\n\\n−\\n\\n(cid:17)\\n\\nyk\\nαk\\n\\n(αk − 1)\\n\\n(57)\\n\\n• Zero evidence region: In case of exp(.) based evidential models, except in the extreme cases of αk → ∞, there will\\nbe some signal to guide the model. In cases outside the zero-evidence region (i.e. outside αk → ∞), there will be\\nsome finite small update in the accurate direction from the gradient. Moreover, for same evidence values, the gradient\\nof exp based model is larger than the SoftPlus based evidential model by a factor of 1 + exp(ok). Compared to\\nSoftPlus models, the larger gradient is expected to help the model learn faster in low-evidence regions.\\n\\n• Range of gradients: For the ground truth node i.e.yk = 1, the range of gradients is [−1, 0]. For all nodes other than\\nthe ground truth node i.e. yk = 0, the range of gradients is [0, 1]. Thus, the gradients are expected to be more expressive\\nand accurate in guiding the evidential model compared to ReLU and SoftPlus based evidential models.\\n\\n20\\n\\n\\x0cLearn to Accumulate Evidence from All Training Samples: Theory and Practice\\n\\n• High evidence region: If the evidence for class k is very large i.e. ek → ∞, then αk − 1 ≈ αk and gradk = smk − yk.\\nIn other words, the model’s gradient updates become identical to the standard classification model (see Section A)\\nwithout any learning issues.\\n\\nDue to smaller zero-evidence region, more expressive gradients, and no issue of learning in high incorrect evidence region,\\nthe exponential-based evidential models are expected to be more effective compared to ReLU and SoftPlus based\\nevidential models.\\n\\nE. Analysis of Evidential Losses\\n\\nHere, we analyze the three variants of evidential loss. As seen in Section D, exp function is expected to be superior to\\nReLU and SoftPlus functions to transform the logits to evidence. Thus, in this section, we consider exp function to\\ntransform the logits into evidence. However, the analysis holds true for all three functions.\\n\\n1. Bayes risk with the sum of squares loss (Eqn. 21)\\n\\nLMSE(x, y) =\\n\\nK\\n(cid:88)\\n\\n(yj −\\n\\nj=1\\n\\nαj\\nS\\n\\n)2 +\\n\\nαj(S − αj)\\nS2(S + 1)\\n\\nThe loss can be simplified as\\n\\nLMSE(x, y) =\\n\\nK\\n(cid:88)\\n\\n(yj −\\n\\nj=1\\n\\nαj\\nS\\n\\n)2 +\\n\\nαj(S − αj)\\nS2(S + 1)\\n2 (cid:80)\\ni\\n\\n(cid:80)\\n\\n(cid:80)\\n\\n(cid:80)\\n\\nk α2\\nk\\nS2 +\\nk α2\\n\\nj αiαj\\n\\nS2(S + 1)\\n(cid:80)\\nj αiαj\\n\\ni\\n\\n= 1 −\\n\\n= 1 −\\n\\n= 2 −\\n\\n= 2 −\\n\\n2αgt\\nS\\n\\n2αgt\\nS\\n\\n2αgt\\nS\\n\\n2αgt\\nS\\n\\n+\\n\\n+\\n\\n+\\n\\n−\\n\\nk + 2 (cid:80)\\nS2\\nj αiαj\\n\\n(cid:80)\\n\\nS2\\n(cid:80)\\n\\nj αiαj\\n\\n2 (cid:80)\\n\\n2 (cid:80)\\n\\ni\\n\\ni\\n\\nS(S + 1)\\n\\n(cid:104)\\n\\n1\\n(S + 1)\\n\\n(cid:105)\\n− 1\\n\\n2 (cid:80)\\ni\\n\\n(cid:80)\\n\\nj αiαj\\n\\nS2(S + 1)\\n\\n+\\n\\n−\\n\\n2 (cid:80)\\ni\\n\\n(cid:80)\\n\\nj αiαj\\n\\nS2\\n\\n(58)\\n\\n(59)\\n\\n(60)\\n\\n(61)\\n\\n(62)\\n\\n(63)\\n\\nThe range of the two components in the loss is 0 ≤ 2αgt\\nS(S+1) ≤ 2 and the loss is bounded in the range [0, 2].\\nIn other words, the loss for any sample in the entire sample space is bounded in the range of [0, 2] no matter how severe\\nthe mistake is. Such bounded loss is expected to restrict the model’s learning capacity.\\n\\nS +\\n\\n2 (cid:80)\\ni\\n\\n(cid:80)\\n\\nj αiαj\\n\\n2. Bayes risk with cross-entropy loss (Eqn. 22)\\n\\nLCE(x, y) =\\n\\n(cid:16)\\n\\nyk\\n\\nK\\n(cid:88)\\n\\nj=1\\n\\nΨ(S) − Ψ(αk)\\n\\n(cid:17)\\n\\n= Ψ(S) − Ψ(αgt)\\n\\n(64)\\n\\nWhere Ψ(.) is the Digamma function, and Γ is the gamma function. The functions and their gradients are defined as\\n\\nΓ(z) =\\n\\ne−γz\\nz\\n\\n∞\\n(cid:89)\\n\\n(cid:16)\\n\\n1 +\\n\\nn=1\\n\\nΨ(z) =\\n\\nd\\ndz\\n\\nlog Γ(z) =\\n\\nz\\nn\\n\\nd\\ndz\\n\\n(cid:17)−1\\n\\nz\\nn\\n\\ne\\n\\n(cid:18)\\n\\n− γz − log z +\\n\\n= −γ −\\n\\n1\\nz\\n\\n+\\n\\n∞\\n(cid:88)\\n\\nn=1\\n\\n1\\nn\\n\\n−\\n\\n1\\nn + z\\n\\n∞\\n(cid:88)\\n\\nn=1\\n\\n(cid:16) z\\nn\\n\\n− log (cid:0)1 +\\n\\n(cid:1)(cid:17)(cid:19)\\n\\nz\\nn\\n\\n(cid:18)\\n\\n∂Ψ(z)\\n∂z\\n\\n=\\n\\n∂\\n∂z\\n\\n− γ −\\n\\n1\\nz\\n\\n+\\n\\n∞\\n(cid:88)\\n\\nn=1\\n\\n1\\nn\\n\\n(cid:19)\\n\\n−\\n\\n1\\nn + z\\n\\n=\\n\\n1\\nz2 +\\n\\n∞\\n(cid:88)\\n\\nn=1\\n\\n1\\n(n + z)2\\n\\n21\\n\\n(65)\\n\\n(66)\\n\\n(67)\\n\\n(68)\\n\\n\\x0cLearn to Accumulate Evidence from All Training Samples: Theory and Practice\\n\\nNow, the Bayes risk with cross-entropy loss becomes\\n\\nLCE(x, y) = Ψ(S) − Ψ(αgt)\\n∞\\n1\\n(cid:88)\\nαgt\\n\\n+ S\\n\\n1\\nS\\n\\n−\\n\\n=\\n\\nn=1\\n\\n1\\nn(n + S)\\n\\n− αgt\\n\\n∞\\n(cid:88)\\n\\nn=1\\n\\n1\\nn(n + αgt)\\n\\n(69)\\n\\n(70)\\n\\nBoth the infinite sums ((cid:80)∞\\n6 . The minimum\\npossible value of this loss is 0 when αgt → ∞&S ≈ αgt. The maximum possible value is ∞ when only S → ∞. The\\nloss lies in the range [0, ∞] and is more expressive compared to MSE-based evidential loss.\\n\\nn(n+αgt) ) converge and lie in the range of 0 to π2\\n\\nn(n+S) and (cid:80)∞\\n\\nn=1\\n\\nn=1\\n\\n1\\n\\n1\\n\\nConsidering the gradient of the loss with respect to the ground truth node (i.e. αgt, ygt = 1),\\n\\n∂LCE(x, y)\\n∂αgt\\n\\n=\\n\\n∂\\n∂αgt\\n\\nΨ(S) − Ψ(αgt) =\\n\\n1\\nS2 +\\n\\n∞\\n(cid:88)\\n\\nn=1\\n\\n1\\n(n + S)2 −\\n\\n1\\nα2\\ngt\\n\\n−\\n\\n∞\\n(cid:88)\\n\\nn=1\\n\\n1\\n(n + αgt)2\\n\\n(71)\\n\\nAs αgt < S, the gradient is always negative. Thus, the model aims to maximize the correct evidence αgt. Considering\\nthe gradient of the loss with respect to nodes not corresponding to the ground truth (i.e. αk, k ̸= gt, yk = 0),\\n\\n∂LCEx, y)\\n∂αk\\n\\n∂LCEx, y)\\n∂ok\\n\\n=\\n\\n=\\n\\n∂\\n∂αk\\n\\nΨ(S) − Ψ(αgt) =\\n\\n∂Ψ(S)\\n∂S\\n\\n∂S\\n∂αk\\n\\n=\\n\\n1\\nS2 +\\n\\n∞\\n(cid:88)\\n\\nn=1\\n\\n1\\n(n + S)2\\n\\n∂LCEx, y)\\n∂αk\\n\\n×\\n\\nαk\\nok\\n\\n=\\n\\n(cid:16) 1\\nS2 +\\n\\n∞\\n(cid:88)\\n\\nn=1\\n\\n(cid:17)\\n\\n1\\n(n + S)2\\n\\n(αk − 1)\\n\\n(72)\\n\\n(73)\\n\\nThe gradient at nodes that do not correspond to ground truth is always non-negative. However, this gradient is also\\nminimum and 0 when S → ∞ & αk → ∞. This is an undesired behavior as the model may be encouraged to always\\nincrease the evidence for all the classes. Moreover, the gradient is zero and there is no update to the nodes when\\nS → ∞, & αk → ∞. So, the incorrect evidence regularization to penalize the incorrect evidence is essential for the\\nevidential model trained with this loss.\\n\\n3. Type II Maximum Likelihood loss (Eqn. 23)\\n\\nLLog(x, y) =\\n\\n(cid:16)\\n\\nyk\\n\\nK\\n(cid:88)\\n\\nk=1\\n\\nlog(S) − log(αk)\\n\\n(cid:17)\\n\\n= log(S) − log(αgt)\\n\\n(74)\\n\\nThe loss is bounded in the range of [0, ∞] as the loss is minimum and 0 when αgt → S → ∞, and maximum loss\\nwhen αgt << S & S → ∞. Thus, the loss is more expressive compared to MSE based evidential loss. Now, the\\ngradient of the loss is given by\\n\\n∂LLog(x, y)\\n∂ok\\n\\n=\\n\\n1\\nS\\n\\n∂S\\n∂ok\\n\\n− yk\\n\\n1\\nαk\\n\\n∂αk\\n∂ok\\n\\n=\\n\\n(cid:16) 1\\nS\\n\\n−\\n\\nyk\\nαk\\n\\n(cid:17) ∂ek\\n∂ok\\n\\n=\\n\\n(cid:16) 1\\nS\\n\\n−\\n\\n(cid:17)\\n\\nyk\\nαk\\n\\n(αk − 1)\\n\\n(75)\\n\\nHere, when S → ∞ & αk → ∞, the gradient becomes ∂LLog(x,y)\\n→ (1 − yk). This is highly desirable behavior for the\\n∂ok\\nmodel as it aims to minimize the evidence for the incorrect class and there will be no update to the node corresponding\\nto the ground truth class if αk = αgt, ygt = 1. Thus, the Type II based issue is expected to be superior to the other\\ntwo losses as the range of loss is optimal (i.e. in the range [0, ∞]), and no learning issue arises for samples with high\\nincorrect evidence.\\n\\nF. Additional Experiments and Results\\n\\nWe first present the details of the models, hyperparameter settings, clarification regarding dead neuron issue, and experiments\\nused in the work in Section F.1. We then present additional results and discussions, including Few-shot classification,\\nand 200-class tiny-ImageNet Classification results, that show the effectiveness of the proposed model RED in Section F.3.\\nFinally discuss some limitations and potential future works in Section F.4.\\n\\n22\\n\\n\\x0cLearn to Accumulate Evidence from All Training Samples: Theory and Practice\\n\\nF.1. Hyperparameter details\\n\\nFor Table 1 results, λ1 = 1.0 was used for MNIST experiments, λ1 = 0.1 was used for Cifar10 experiments, and\\nλ1 = 0.001 was used for Cifar100 experiments. Table 8, 9, and 10 present complete results across the hyperparameter\\nvalues and experiment settings. MNIST model was trained on the LeNet model (Sensoy et al., 2018) for 50 epochs, and\\nCifar10/Cifar100 models were trained on Resnet-18 based classifier (He et al., 2016) for 200 epochs. Few-shot classification\\nexperiments were carried out with λ1 = 0.1 using Resnet-12 based classifier (Chen et al., 2021). All results presented\\nin this work are from local reproduction. MNIST models were trained with learning rate of 0.0001 and Adam optimizer\\n(Kingma & Ba, 2014), and all remaining models were trained with learning rate of 0.1 and Stochastic Gradient Descent\\noptimizer with momentum. Tabular results represent the mean and standard deviation from 3 independent runs of the model.\\nIn the proposed model RED, correct evidence regularization is weighted by the parameter λcor whose value is given by the\\npredicted vacuity ν. λcor is treated as hyperparameter, i.e., constant weighting term in the loss during model update.\\n\\nF.2. Dead Neuron Issue Clarification\\n\\nInstead of using ReLU as an activation function in a standard deep neural network, evidential models introduce ReLU as\\nnon-negative transformation function in the output layer to ensure that the predicted evidence is non-negative to satisfy\\nthe requirement of evidential theory. This non-negative evidence vector parameterizes a Dirichlet prior for fine-grained\\nuncertainty quantification that covers second-order uncertainty, including vacuity and dissonance. We theoretically and\\nempirically show the learning deficiency of ReLU based evidential models and justify the advantage of using an exponential\\nfunction to output (non-negative) evidence. We further introduce a correct evidence regularization term in the loss that\\naddresses the learning deficiency from zero-evidence samples. The “dead neuron” issue in the activation functions has been\\nstudied, and ReLU variations such as Exponential Linear Unit, Parametric ReLU, and Leaky ReLU have been developed to\\naddress the issue. But, these activation functions will not be theoretically sound in the evidential framework as they are can\\nlead to negative evidences. In this case, they can not serve as Dirichlet parameters that are interpreted as pseudo counts.\\n\\nF.3. Effectiveness of Regularized Evidential Model (RED)\\n\\nF.3.1. EVIDENTIAL ACTIVATION FUNCTION.\\n\\nIn this section, we present additional results (for section 5.2) with the MNIST classification problem using the LeNet\\nmodel to empirically validate Theorem 2. We carry out experiments for evidential models trained using all three evidential\\nlosses: Evidential MSE loss in (21), Evidential cross-entropy loss in (22), and Evidential Log loss in (23) with λ1 =\\n{0.0, 1.0, &10.0}. As can be seen in Figure 15, 16, and 17, using exp activation for transforming logits to evidence leads to\\nsuperior performance in all settings compared to ReLU and Softplus based evidential models that empirically validates\\nTheorem 2.\\n\\n(a) Trend for λ1 = 0.0\\n\\n(b) Trend for λ1 = 1.0\\n\\n(c) Trend for λ1 = 10.0\\n\\nFigure 15. Impact of Evidential Activation to the test set accuracy of the model trained with MSE based evidential loss (Eqn. 21)\\n\\nF.3.2. CORRECT EVIDENCE REGULARIZATION\\n\\nWe introduce the novel correct evidence regularization term to train the evidential model (Section 4.1). In this section, we\\npresent additional results for the evidential model that uses exp activation. We trained the model using evidential losses\\nwith different incorrect evidence regularization strengths ( λ1 = 0, 1.0 & 10.0). As can be seen( Figure 18, and 19), the\\nmodel with proposed correct-evidence regularization leads to improved generalization compared to the baseline model\\n\\n23\\n\\n\\x0cLearn to Accumulate Evidence from All Training Samples: Theory and Practice\\n\\n(a) Trend for λ1 = 0.0\\n\\n(b) Trend for λ1 = 1.0\\n\\n(c) Trend for λ1 = 10.0\\n\\nFigure 16. Impact of Evidential Activation to test set accuracy of the model trained with cross-entropy based evidential loss (Eqn. 22)\\n\\n(a) Trend for λ1 = 0.0\\n\\n(b) Trend for λ1 = 1.0\\n\\n(c) Trend for λ1 = 10.0\\n\\nFigure 17. Impact of Evidential Activation to the test set accuracy of the model trained with Type II based evidential loss (Eqn. 23)\\n\\nas the proposed correct-evidence regularization term enables the evidential model to learn from zero-evidence samples\\ninstead of ignoring them. Moreover, even though strong incorrect evidence regularization hurts both model’s generalization,\\nthe proposed regularization leads to a more robust model that generalizes better. Finally, the MSE-based evidential model\\nis hurt the most with strong incorrect evidence regularization as thee MSE based evidential loss is bounded in the range\\n[0, 2], and the incorrect evidence-regularization term may easily dominate the overall loss compared to other evidential\\nlosses. This can be seen in Figure 18(c) where the incorrect evidence regularization strength is large i.e. λ1 = 10.0 and\\nthe evidential model fails to train. Due to strong incorrect evidence regularization, the model may have learned to map all\\ntraining samples to zero-evidence region. However, with the proposed regularization, the model continues to learn and\\nachieves good generalization performance.\\n\\n(a) Trend for λ1 = 0.0\\n\\n(b) Trend for λ1 = 1.0\\n\\n(c) Trend for λ1 = 10.0\\n\\nFigure 18. Impact of proposed Correct Evidence Regularization to the test set accuracy of the evidential model( Trained with Eqn. 21)\\n\\nF.3.3. FEW-SHOT CLASSIFICATION EXPERIMENTS\\n\\nIdeas presented in this work address the fundamental limitation of evidential classification framework that enables the\\nevidential model to acquire knowledge from all the training samples. Using these ideas, evidential framework can be\\nextended to challenging classification problems to the reasonable predictive performance. To this end, we experiment\\nwith few-shot classification using 1-shot and 5-shot classification for the mini-ImageNet dataset (Vinyals et al., 2016). We\\n\\n24\\n\\n\\x0cLearn to Accumulate Evidence from All Training Samples: Theory and Practice\\n\\n(a) Trend for λ1 = 0.0\\n\\n(b) Trend for λ1 = 1.0\\n\\n(c) Trend for λ1 = 10.0\\n\\nFigure 19. Impact of proposed Correct Evidence Regularization to the test set accuracy of the evidnetial model (Trained with Eqn. 22)\\n\\nconsider the ResNet-12 backbone, classifier-baseline model (Chen et al., 2021), and its evidential extension. Table 7 shows\\nthe results for 1-shot and 5-shot classification experiments. As can be seen, the ReLU and Softplus based evidential\\nmodels have suboptimal performance as they avoid many training samples of the zero-evidence region. In contrast, the exp\\nmodel has a better learning capacity that leads to superior performance. Finally, the proposed model RED can learn from all\\ntraining samples, which leads to the best generalization performance among all the evidential models.\\n\\nTable 7. Few-Shot Classification Accuracy comparison: mini-ImageNet dataset\\nStandard CE Model: 1 Shot: 57.9±0.2%; 5-Shot: 76.9±0.2%\\n\\n1-Shot Experiments\\nRegularization\\nλ1 = 0.000\\nλ1 = 0.100\\nλ1 = 1.000\\n5-Shot Experiments\\nRegularization\\nλ1 = 0.000\\nλ1 = 0.100\\nλ1 = 1.000\\n\\nReLU\\n38.78±3.75\\n31.15±1.69\\n20.00±0.00\\n\\nReLU\\n52.66±5.32\\n43.95±3.72\\n20.00±0.00\\n\\nSoftPlus\\n51.60±0.40\\n48.87±0.21\\n43.81±0.56\\n\\nSoftPlus\\n67.22±0.17\\n66.14±0.05\\n61.96±0.61\\n\\nexp\\n57.11±0.09\\n56.43±0.03\\n27.43±0.88\\n\\nexp\\n75.87±0.09\\n74.08±0.13\\n34.01±1.46\\n\\nRED (Ours)\\n56.27±0.15\\n58.03±0.39\\n54.68±0.45\\n\\nOurs\\n75.31±0.13\\n76.05±0.17\\n72.32±0.20\\n\\nF.3.4. COMPLEX DATASET/MODEL EXPERIMENTS\\n\\nWe also carry out experiment for a challenging 200-class classification problem over Tiny-ImageNet based on (Huynh,\\n2022). We adapt the Swin Transformer to be evidential, and train all the models for 20 epochs with Evidential log loss\\n(Eqn. 23). In this setting, ReLU based evidential model achieves 85.25% accuracy, softplus based model achieves 85.15 %\\naccuracy, the exponential model improves over both to achieve 89.93 % accuracy, and our proposed model RED outperforms\\nall the evidential models to achieve the greatest accuracy of 90.14%, empirically validating our theoretical analysis.\\n\\nF.4. Limitations and Future works\\n\\nWe carried out a theoretical investigation of the Evidential Classification models to identify their fundamental limitation:\\ntheir inability to learn from zero evidence regions. The empirical study in this work is based on classification problems.\\nWe next plan to extend the ideas to develop Evidential Segmentation and Evidential Object Detection models. Moreover,\\nthis work identifies limitations of Evidential MSE loss in (21), and we plan to carry out a thorough theoretical analysis\\nto analyze other evidential losses given in (23) and (22)). The proposed evidential model, similar to existing evidential\\nclassification models, requires hyperparameter tuning for λ1 i.e. the incorrect evidence regularization hyperparameter.\\n\\nIn addition, extending evidential models to noisy and incomplete data settings and investigating the benefits of leveraging\\nuncertainty information could be interesting future work. Finally, It will be an interesting future work to extend the analysis\\nand evidential models to tasks beyond classification, for instance to build effective evidential segmentation and object\\ndetection models.\\n\\n25\\n\\n\\x0cLearn to Accumulate Evidence from All Training Samples: Theory and Practice\\n\\nTable 8. Classification performance comparison: MNIST dataset\\nStandard CE Model: 99.21±0.03%\\n\\nLog loss\\nRegularization\\nλ1 = 0.000\\nλ1 = 1.000\\nλ1 = 10.000\\nEvidential CE loss\\nλ1 = 0.000\\nλ1 = 1.000\\nλ1 = 10.000\\nEvidential MSE loss\\nλ1 = 0.000\\nλ1 = 1.000\\nλ1 = 10.000\\n\\nReLU\\n97.06±0.19\\n98.19±0.08\\n83.17±4.54\\n\\n97.03±0.21\\n98.27±0.02\\n97.46±1.02\\n\\n96.18±0.02\\n97.41±0.22\\n19.93±6.98\\n\\nSoftPlus\\n97.07±0.24\\n98.21±0.05\\n80.37±18.70\\n\\n97.09±0.21\\n98.36±0.02\\n97.14±1.42\\n\\n96.20±0.03\\n97.45±0.16\\n27.14±6.37\\n\\nexp\\n98.85±0.03\\n98.79±0.02\\n98.14±0.07\\n\\n98.84±0.02\\n98.87±0.03\\n98.31±0.07\\n\\n98.42±0.03\\n98.35±0.05\\n27.17±3.72\\n\\nRED (Ours)\\n98.82±0.04\\n99.10±0.02\\n98.84±0.03\\n\\n98.81±0.01\\n99.12±0.02\\n98.84±0.04\\n\\n98.41±0.06\\n99.02±0.00\\n98.76±0.03\\n\\nTable 9. Classification performance comparison: Cifar10 Dataset\\nStandard CE Model: 95.43±0.02%\\n\\nLog loss\\nRegularization\\nλ1 = 0.000\\nλ1 = 0.100\\nλ1 = 1.000\\nλ1 = 10.000\\nλ1 = 50.000\\nEvidential CE loss\\nλ1 = 0.000\\nλ1 = 0.100\\nλ1 = 1.000\\nλ1 = 10.000\\nλ1 = 50.000\\nEvidential MSE loss\\nλ1 = 0.000\\nλ1 = 0.100\\nλ1 = 1.000\\nλ1 = 10.000\\nλ1 = 50.000\\n\\nReLU\\n43.83±14.60\\n41.43±19.60\\n38.42±15.64\\n10.00±0.00\\n10.00±0.00\\n\\n79.19±16.06\\n75.97±20.56\\n75.83±20.74\\n10.00±0.00\\n10.00±0.00\\n\\n95.43±0.05\\n95.15±0.10\\n49.68±29.48\\n10.00±0.00\\n10.00±0.00\\n\\nexp\\n95.35±0.02\\n95.11±0.10\\n93.95±0.06\\n23.29±5.24\\n12.47±3.49\\n\\n95.38±0.10\\n95.33±0.03\\n94.65±0.04\\n56.54±4.80\\n25.33±6.66\\n\\n95.10±0.04\\n95.14±0.03\\n18.98±1.82\\n10.00±0.00\\n10.00±0.00\\n\\nRED (Ours)\\n95.03±0.14\\n95.24±0.06\\n94.78±0.17\\n90.96±0.35\\n65.09±0.74\\n\\n95.40±0.14\\n95.08±0.07\\n94.74±0.11\\n91.71±0.23\\n62.98±0.84\\n\\n94.92±0.12\\n95.03±0.13\\n94.90±0.20\\n90.15±0.71\\n27.11±24.20\\n\\nSoftPlus\\n95.19±0.10\\n95.18±0.11\\n94.94±0.22\\n32.42±6.99\\n10.00±0.00\\n\\n95.32±0.17\\n95.12±0.05\\n94.99±0.08\\n89.63±0.38\\n27.03±2.62\\n\\n95.35±0.15\\n95.04±0.05\\n93.51±0.03\\n10.00±0.00\\n10.00±0.00\\n\\n26\\n\\n\\x0cLearn to Accumulate Evidence from All Training Samples: Theory and Practice\\n\\nTable 10. Classification performance comparison: Cifar100 dataset\\nStandard CE Model: 75.67 ± 0.11\\n\\nLog loss\\nRegularization\\nλ1 = 0.000\\nλ1 = 0.001\\nλ1 = 0.010\\nλ1 = 0.100\\nλ1 = 1.000\\nλ1 = 2.000\\nEvidential CE loss\\nλ1 = 0.000\\nλ1 = 0.001\\nλ1 = 0.010\\nλ1 = 0.100\\nλ1 = 1.000\\nλ1 = 2.000\\nEvidential MSE loss\\nλ1 = 0.000\\nλ1 = 0.001\\nλ1 = 0.010\\nλ1 = 0.100\\nλ1 = 1.000\\nλ1 = 2.000\\n\\nReLU\\n56.69±5.83\\n61.27±3.79\\n54.20±5.93\\n20.29±4.54\\n1.00±0.00\\n1.00±0.00\\n\\n66.37±3.47\\n68.62±2.41\\n71.94±0.66\\n67.25±1.84\\n1.00±0.00\\n1.00±0.00\\n\\n35.76±2.81\\n31.49±0.31\\n13.60±2.44\\n1.00±0.00\\n1.00±0.00\\n1.00±0.00\\n\\nSoftPlus\\n73.85±0.20\\n74.48±0.17\\n75.56±0.43\\n75.67±0.22\\n37.60±0.82\\n1.57±0.35\\n\\n73.73±0.38\\n74.44±0.08\\n75.45±0.12\\n75.75±0.21\\n73.10±0.20\\n52.99±0.56\\n\\n20.45±1.41\\n15.74±0.47\\n1.00±0.00\\n1.00±0.00\\n1.00±0.00\\n1.00±0.00\\n\\nexp\\n76.25±0.16\\n76.12±0.04\\n76.02±0.16\\n72.72±0.26\\n2.59±0.52\\n0.97±0.06\\n\\n75.91±0.20\\n76.23±0.09\\n75.95±0.14\\n74.02±0.09\\n37.36±0.73\\n12.94±1.11\\n\\n75.70±0.47\\n42.95±0.76\\n1.00±0.00\\n1.00±0.00\\n1.00±0.00\\n1.00±0.00\\n\\nRED (Ours)\\n76.26±0.27\\n76.43±0.21\\n76.14±0.09\\n74.62±0.21\\n68.62±0.03\\n62.33±0.52\\n\\n76.19±0.22\\n76.35±0.06\\n76.13±0.24\\n74.69±0.13\\n69.40±0.16\\n63.93±0.34\\n\\n75.55±0.24\\n75.73±0.27\\n75.35±0.16\\n74.00±0.13\\n66.61±0.46\\n63.01±0.83\\n\\n27\\n\\n'}]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arxiv_search(\"recent advances in deep learning optimizers\",max_results=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "7d9b9d65-0a11-45bc-89b8-2aefe06967ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_client = OpenAIChatCompletionClient(\n",
    "    model=\"o3-mini\",\n",
    "    api_key = openai_api_key\n",
    ")\n",
    "\n",
    "    s2_search_agent = AssistantAgent(\n",
    "        name=\"Semantic_Scholar_Search_Agent\",\n",
    "        tools=[s2_search_tool],\n",
    "        model_client=model_client,\n",
    "        description=\"An agent that can search Semantic scholar paper database using keywords related to given topic\",\n",
    "        system_message=\"You are a helpful AI assistant. Solve tasks using your tools.\",\n",
    "    )\n",
    "    \n",
    "    arxiv_search_agent = AssistantAgent(\n",
    "        name=\"Arxiv_Search_Agent\",\n",
    "        tools=[arxiv_search_tool],\n",
    "        model_client=model_client,\n",
    "        description=\"An agent that can search Arxiv for papers related to a given topic, including abstracts and full text\",\n",
    "        system_message=\"You are a helpful AI assistant. Solve tasks using your tools. Specifically, you can take into consideration the user's request and craft a search query that is most likely to return relevant academic papers.\",\n",
    "    )\n",
    "    \n",
    "    summarizer_agent = AssistantAgent(\n",
    "        name=\"Summarizer_Agent\",\n",
    "        model_client=model_client,\n",
    "        description=\"An agent that can summarize one scientific paper in a time. The paper should be provided as title, list of authors, abstaract and full text. Summarization will be build in context of general task query\",\n",
    "        system_message=\"You are a helpful AI assistant. Summarize content of scientific paper provided in no more that 2000 words. Build a summarization in context of goal of literature search\",\n",
    "    )\n",
    "    \n",
    "    report_agent = AssistantAgent(\n",
    "        name=\"Report_Agent\",\n",
    "        model_client=model_client,\n",
    "        description=\"Generate a report based on a given topic\",\n",
    "        system_message=\"You are a helpful assistant. Your task is to synthesize data extracted into a high quality literature review including CORRECT references. You MUST write a final report that is formatted as a literature review with CORRECT references.  Your response should end with the word 'TERMINATE'\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "6295cea2-e498-463e-af43-390cd2234c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "termination = TextMentionTermination(\"TERMINATE\")\n",
    "team = MagenticOneGroupChat(\n",
    "    participants=[arxiv_search_agent, summarizer_agent, report_agent], \n",
    "    termination_condition=termination,\n",
    "    model_client = model_client\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0507dad8-07c6-48e8-9653-b9166a5cf33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "await Console(\n",
    "    team.run_stream(\n",
    "        task=\"Make a search of 3 recent publications and write a literature review on recent advances in deep learning optimizers with an accent on novel optimization algorythms\",\n",
    "    )\n",
    ");\n",
    "\n",
    "#await model_client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff3b9c8-723e-4fec-9059-dfe2a88abf05",
   "metadata": {},
   "source": [
    "## No team application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "3436dba7-bdde-4cf3-8584-ebc30e514338",
   "metadata": {},
   "outputs": [],
   "source": [
    "arxiv_search_agent = AssistantAgent(\n",
    "    name=\"Arxiv_Search_Agent\",\n",
    "    tools=[arxiv_search_tool],\n",
    "    model_client=model_client,\n",
    "    description=\"An agent that can search Arxiv for papers related to a given topic, including abstracts and full text\",\n",
    "    system_message=\"You are a helpful AI assistant. Given the user input define the search topic for scientific articles in arxiv, and search arxive for the topic. return list of found articles and topic\",\n",
    ")\n",
    "\n",
    "summarizer_agent = AssistantAgent(\n",
    "    name=\"Summarizer_Agent\",\n",
    "    model_client=model_client,\n",
    "    description=\"An agent that can summarize one scientific paper in a time. The paper is provided as title, list of authors, abstaract and full text. Summarization is to be builtin context of provided user query\",\n",
    "    system_message=\"You are a helpful AI assistant. Summarize content of scientific paper provided in no more that 2000 words.  \" +\n",
    "    \"The paper is provided as title, list of authors, abstaract and full text. Summarization is to be built in context of provided user query\",\n",
    ")\n",
    "\n",
    "report_agent = AssistantAgent(\n",
    "    name=\"Report_Agent\",\n",
    "    model_client=model_client,\n",
    "    description=\"Generate a report based on a given topic\",\n",
    "    system_message=\"You are a helpful assistant. Your task is to synthesize data extracted into a high quality literature review containing no more than 5000 words including CORRECT references. You MUST write a final report that is formatted as a literature review with CORRECT references.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "bb15333a-9dbf-40f4-9d03-bd113496db4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "task = \"Make a search of 10 recent publications and write a literature review on recent advances in deep learning optimizers with an accent on novel optimization algorythms\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "1cd12838-00e0-4fe8-9787-b0675c34d83a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2008.05730v1 was successfully saved!\n",
      "2105.04026v2 was successfully saved!\n",
      "2306.11113v2 was successfully saved!\n",
      "2504.20096v1 was successfully saved!\n",
      "2301.00942v1 was successfully saved!\n",
      "2007.15745v3 was successfully saved!\n",
      "Could not download 2302.09566v2,\n",
      "HTTP response status code: 404\n",
      "1903.03040v2 was successfully saved!\n",
      "2103.07585v1 was successfully saved!\n",
      "1910.08476v2 was successfully saved!\n"
     ]
    }
   ],
   "source": [
    "arxiv_search = await arxiv_search_agent.run(task = task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "8f9c14da-d642-4126-a98c-fdf3c3c3945a",
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_list = ast.literal_eval(arxiv_search.messages[-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "1a4fc4f0-715a-48e1-bf37-814dce149f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#paper_list[0][\"topic\"] = task\n",
    "for paper in paper_list:\n",
    "    paper[\"topic\"] = task\n",
    "    paper_summary = await summarizer_agent.run(\n",
    "        task = f\"Write a summary of provided article : {paper_list[0]}\",\n",
    "    )\n",
    "    paper[\"summary\"] = paper_summary.messages[-1].content\n",
    "    await summarizer_agent.on_reset(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "1065f7d6-2c13-4bc8-ac44-369fc68c01b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for paper in paper_list:\n",
    "    del paper[\"full_text\"]\n",
    "    del paper[\"topic\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "f86b6a7f-1d6c-45ca-b865-490724d44d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lit_review_output = await report_agent.run(task = f\"Synthesize literature revew based ion provided topic \\\"{task}\\\" and gathered data {paper_list}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "773cd5bc-205c-49d2-8b1b-3a4beaf2012f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is a literature review that synthesizes recent advances in deep learning optimizers, with a special emphasis on novel optimization algorithms. In what follows, we review advances spanning adaptive gradient methods, surrogate‐based and active learning strategies, second‐order techniques, and connections with reinforcement learning. This survey builds on and integrates insights from ten recent publications from 2019 to 2025.\n",
      "\n",
      "──────────────────────────────\n",
      "1. Introduction\n",
      "\n",
      "Deep learning has transformed many domains by enabling the solution of highly complex problems. At the same time, the success of deep learning critically depends on optimization algorithms that are both efficient and robust. Early methods—such as stochastic gradient descent (SGD)—have been augmented with adaptive schemes (e.g., Adam) and momentum techniques to tackle issues of high variance and slow convergence. More recently, novel approaches have emerged that not only refine per-iteration update rules but also incorporate surrogate modeling and active learning principles to address expensive evaluation problems. Such challenges frequently occur in settings where each function evaluation involves a costly simulation (for example, when the objective is defined by partial differential equation (PDE) solvers) or when the underlying optimization landscape is structured on a low-dimensional manifold. In this review, we survey recent contributions that extend standard optimization ideas to these challenging regimes and discuss how they connect with broader research trends in both deep learning and reinforcement learning.\n",
      "\n",
      "──────────────────────────────\n",
      "2. Novel Surrogate-Based and Active Learning Approaches\n",
      "\n",
      "A number of recent works have proposed hybrid optimization strategies that combine surrogate model approximations with active learning techniques. In PDE-constrained optimization—where each evaluation of a high-fidelity solver is expensive—Lye et al. (2020) propose an innovative method known as Iterative Surrogate Model Optimization (ISMO). In their approach, an initial deep neural network (DNN) surrogate (termed “DNNopt”) is trained on a fixed set of parameter–observable pairs, and then used in a standard optimizer (such as a quasi-Newton algorithm). However, recognizing that a fixed training set yields only algebraic error decay and high sensitivity in the computed minimizers, ISMO iteratively augments the training data. By “asking” the optimizer where promising minimizers lie and then retraining the surrogate in these regions, ISMO achieves exponential error reduction and reduced variance (Lye et al., 2020; also see Lye, Mishra, & Deep Ray, 2019 for complementary work on deep learning observables in computational fluid dynamics). This active, feedback-based approach demonstrates that focusing approximation efforts in regions of interest can overcome the curse of dimensionality inherent to fixed training schemes.\n",
      "\n",
      "A similar philosophy is adopted in hybrid surrogate-model based optimization, which is further motivated by applications in computational physics. For example, lecture notes by Deep Ray, Pinti, and Oberai (2023) link deep learning with computational physics, showing that concepts from numerical simulation can guide the development of robust surrogate models for complex optimization tasks. Moreover, in quantum circuit optimization, Fösel et al. (2021) use deep reinforcement learning to “learn” efficient circuit realizations on specific hardware, an idea conceptually related to actively refining a surrogate model in high-cost settings.\n",
      "\n",
      "──────────────────────────────\n",
      "3. Advances in Adaptive First-Order and Momentum Methods\n",
      "\n",
      "Much of the progress in deep learning optimization has historically centered on adaptive gradient methods, including Adam (Kingma & Ba, 2015) and its variants. Adam’s widespread adoption is due to its use of adaptive first- and second-moment estimates, which help manage the noisiness of stochastic gradients. Subsequent variants such as AMSGrad (Reddi et al., 2018) were proposed to remedy certain convergence issues observed in Adam by enforcing monotonicity in the second moment estimation. RAdam (Liu et al., 2019) further refines the adaptive learning rate by rectifying the variance, thereby reducing the need for manual warm-up.\n",
      "\n",
      "Complementary approaches—like Lookahead (Zhang et al., 2019) and AdaBound (Luo et al., 2019)—seek to mix the rapid initial progress of Adam with the long-term convergence characteristics of SGD, by either maintaining a secondary “slow” weight set or by gradually bounding the learning rates. LAMB (You et al., 2019) extends these ideas for large-batch training, applying layer-wise adaptive adjustments which have been crucial in training state-of-the-art transformer models. Ranger (Wright et al., 2019) represents a hybrid optimizer that merges the benefits of RAdam with Lookahead to further lower update variance and improve robustness. These developments have been widely applied across deep learning tasks; however, they mostly address settings in which function evaluations are inexpensive and do not explicitly consider the cost of obtaining the ground truth (as is the case with surrogate or PDE-constrained optimization).\n",
      "\n",
      "──────────────────────────────\n",
      "4. Second-Order Methods and Novel Adaptive Optimizers\n",
      "\n",
      "Complementing first-order methods, recent work has reinvigorated interest in employing second-order or quasi-Newton approaches for deep learning. In a forward-looking thesis, Gomes (2025) introduces AdaFisher—a novel adaptive second-order optimizer based on diagonal block-Kronecker approximations of the Fisher information matrix. AdaFisher draws on curvature information to precondition gradients adaptively, aiming to bridge the gap between the superior convergence properties of second-order methods and the computational efficiency required for large-scale deep learning. Such methods, although traditionally expensive, are increasingly made practical by efficient approximations and have shown promise in areas such as image classification and language modeling.\n",
      "\n",
      "Additionally, evidential deep learning (Pandey & Yu, 2023) contributes to the discussion by identifying limitations in standard evidential activation functions and proposing novel regularizers that help mitigate learning inefficiencies. Although primarily focused on uncertainty quantification, their work indirectly impacts optimization performance by ensuring that the network reliably accumulates evidence from all training samples.\n",
      "\n",
      "──────────────────────────────\n",
      "5. Connections with Reinforcement Learning and Hyperparameter Optimization\n",
      "\n",
      "Beyond standalone deep learning optimizers, several recent studies have explored connections between optimization in reinforcement learning (RL) and constrained or surrogate-based optimization methods. Vieillard, Pietquin, and Geist (2019) draw formal links between dynamic programming schemes and constrained convex optimization, thereby relating methods like Conservative Policy Iteration to classical optimization algorithms such as Frank–Wolfe. Similarly, Fösel et al. (2021) adopt reinforcement learning techniques for quantum circuit optimization, underscoring how RL can be harnessed in optimization tasks that require decision-making under uncertainty.\n",
      "\n",
      "Hyperparameter optimization also plays a significant role in the overall performance of deep learning algorithms. Yang and Shami (2020) provide an extensive survey of optimization techniques tailored to hyperparameter tuning, emphasizing that the choice of hyperparameters directly affects the behavior of deep network optimizers. Effective hyperparameter selection can synergize with novel iterative methods such as ISMO to further enhance convergence and performance in high-dimensional, expensive-to-evaluate applications.\n",
      "\n",
      "──────────────────────────────\n",
      "6. Discussion and Future Directions\n",
      "\n",
      "The recent literature reveals two major trends. First, advances in adaptive first-order methods—exemplified by Adam variants, Lookahead, AdaBound, LAMB, and Ranger—have significantly improved convergence speed and robustness in standard deep learning settings. Second, in contexts where objective evaluations are costly (such as PDE-constrained optimization), hybrid surrogate-based and active learning approaches have emerged as powerful tools. The ISMO algorithm (Lye et al., 2020) is representative of this second trend; its iterative enrichments lead to exponential convergence of both error and variance under suitable assumptions, offering dramatic computational savings.\n",
      "\n",
      "Looking ahead, future work may well lie in combining these strategies—integrating second-order and adaptive techniques (e.g., AdaFisher) with active surrogate refinement (e.g., ISMO) to design optimizers that are both data efficient and robust in high-dimensional, expensive-simulation settings. Moreover, exploring connections with reinforcement learning and improving strategies for hyperparameter optimization will further advance our understanding and implementation of deep learning optimizers.\n",
      "\n",
      "──────────────────────────────\n",
      "7. Conclusions\n",
      "\n",
      "Recent advances in deep learning optimization have evolved from standard adaptive gradient methods to newer, hybrid approaches that incorporate surrogate modeling and active learning. Classical methods like Adam and its variants have reshaped neural network training by efficiently managing gradient noise, while second-order methods and novel adaptive schemes (such as AdaFisher) are pushing the envelope for faster convergence. In parallel, the ISMO algorithm demonstrates that actively refining a DNN surrogate through iterative data augmentation can lead to exponential improvements in solving PDE-constrained optimization problems. Together, these developments highlight a promising future where hybrid and intelligent optimization methods—combining the strengths of adaptive gradient techniques and surrogate-based active learning—will play an increasingly important role in addressing computationally intensive, high-dimensional problems.\n",
      "\n",
      "──────────────────────────────\n",
      "References\n",
      "\n",
      "Berner, J., Grohs, P., Kutyniok, G., & Petersen, P. (2021). The Modern Mathematics of Deep Learning. Retrieved from [DOI/URL].\n",
      "\n",
      "Fösel, T., Niu, M. Y., Marquardt, F., & Li, L. (2021). Quantum circuit optimization with deep reinforcement learning. Retrieved from [DOI/URL].\n",
      "\n",
      "Gomes, D. M. (2025). Towards Practical Second-Order Optimizers in Deep Learning: Insights from Fisher Information Analysis. Retrieved from [DOI/URL].\n",
      "\n",
      "Kingma, D. P., & Ba, J. (2015). Adam: A Method for Stochastic Optimization. ICLR.\n",
      "\n",
      "Lye, K. O., Mishra, S., Deep Ray, & Chandrasekhar, P. (2020). Iterative Surrogate Model Optimization (ISMO): An active learning algorithm for PDE constrained optimization with deep neural networks. Retrieved from [DOI/URL].\n",
      "\n",
      "Lye, K. O., Mishra, S., & Deep Ray. (2019). Deep learning observables in computational fluid dynamics. Retrieved from [DOI/URL].\n",
      "\n",
      "Liu, L., et al. (2019). On the Variance of the Adaptive Learning Rate and Beyond (RAdam). Retrieved from [DOI/URL].\n",
      "\n",
      "Luo, L., et al. (2019). Adaptive Gradient Methods with Dynamic Bound of Learning Rate (AdaBound). ICLR.\n",
      "\n",
      "Pandey, D., & Yu, Q. (2023). Learn to Accumulate Evidence from All Training Samples: Theory and Practice. Retrieved from [DOI/URL].\n",
      "\n",
      "Reddi, S. J., Kale, S., & Kumar, S. (2018). On the Convergence of Adam and Beyond: AMSGrad. ICLR.\n",
      "\n",
      "Vieillard, N., Pietquin, O., & Geist, M. (2019). On Connections between Constrained Optimization and Reinforcement Learning. Retrieved from [DOI/URL].\n",
      "\n",
      "Wright, L., et al. (2019). Ranger Optimizer: Combining RAdam and Lookahead. Retrieved from [DOI/URL].\n",
      "\n",
      "Zhang, M., et al. (2019). Lookahead Optimizer: k Steps Forward, 1 Step Back. NeurIPS.\n",
      "\n",
      "Yang, L., & Shami, A. (2020). On Hyperparameter Optimization of Machine Learning Algorithms: Theory and Practice. Retrieved from [DOI/URL].\n",
      "\n",
      "Deep Ray, O. P., & Oberai, A. A. (2023). Deep Learning and Computational Physics (Lecture Notes). Retrieved from [DOI/URL].\n",
      "\n",
      "Shulman, D. (2023). Optimization Methods in Deep Learning: A Comprehensive Overview. Retrieved from [DOI/URL].\n",
      "\n",
      "──────────────────────────────\n",
      "This review demonstrates how novel optimization algorithms—ranging from adaptive gradient methods to active surrogate-based strategies—are reshaping the practice of deep learning, particularly in scenarios where evaluations are expensive and high-dimensional, paving the way for more efficient and robust future systems.\n"
     ]
    }
   ],
   "source": [
    "print(lit_review_output.messages[-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "f67eb19a-ef2d-4c08-8661-43c997702654",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is a literature‐style summary of the article “Iterative Surrogate Model Optimization (ISMO): An active learning algorithm for PDE constrained optimization with deep neural networks” written in the context of recent advances in deep learning optimizers and novel optimization algorithms. This summary is meant both to capture the contributions of ISMO and to situate its ideas alongside other recent publications in the field.\n",
      "\n",
      "──────────────────────────────\n",
      "1. Background and Motivation\n",
      "\n",
      "Traditional optimization methods for problems constrained by partial differential equations (PDEs) face a major computational bottleneck: each function evaluation (and its derivatives) requires solving a PDE numerically, a task that is often extremely expensive. To overcome this challenge, surrogate models—especially deep neural networks (DNNs)—have been used to approximate the mapping y → L(y) from design parameters y (which might be high-dimensional) to observables L(y) (which are outputs of PDE solvers). The efficiency of DNNs in performing fast evaluations and backpropagation-based differentiation has made them attractive replacements for costly PDE solvers. However, one drawback of a “fixed‐training-set” approach (exemplified by the baseline “DNNopt” algorithm) is that the resulting surrogate can be inaccurate in regions crucial to optimization (namely, around the minimizers). In addition, theoretical error bounds in such scenarios decay only algebraically—with a rate of roughly O(N^(–1/d)) in d dimensions—leading to high sensitivity (or variance) of the computed minimizers with respect to the optimizer’s starting values.\n",
      "\n",
      "In a broader context, recent literature in deep learning optimizers and surrogate‐assisted optimization (see, for example, publications by Raissi et al. [2019 on PINNs], Han et al. [2018] for high‐dimensional PDE solvers, and recent works on active learning for physics‐informed optimization) has stressed the need for optimization methods that automatically refine the approximation where it matters most. Against this backdrop, the ISMO algorithm represents an innovative step.\n",
      "\n",
      "──────────────────────────────\n",
      "2. Main Contributions of the ISMO Algorithm\n",
      "\n",
      "The paper proposes two related algorithms:\n",
      "  \n",
      "A. DNNopt:  \n",
      " • A straightforward combination of a deep neural network–based surrogate and a standard (quasi‐Newton) optimization scheme.  \n",
      " • The surrogate L* is trained on a fixed training set S (obtained by random sampling or via low-discrepancy sequences) and then substituted into a cost function G*(y) = G(L*(y)).  \n",
      " • When the optimization algorithm (e.g., using gradient descent or quasi-Newton methods) is applied to G*, convergence to a local minimizer is achieved provided that the neural network approximates L sufficiently well.  \n",
      " • However, the analysis shows that the error between the optimizer of the surrogate and the true optimizer decays only algebraically with N (the number of training samples) and is sensitive to the spatial dimension d—as predicted by error bounds that include terms like N^(–1/d). This “curse of dimensionality” also results in high variance of the computed approximate minimizers.\n",
      "\n",
      "B. ISMO (Iterative Surrogate Model Optimization):  \n",
      " • Recognizing that fixed training sets may poorly capture optimal regions, ISMO introduces an active learning loop.  \n",
      " • At each iteration, the current surrogate is optimized with a standard algorithm to identify candidate minimizers. These candidate points (which are typically located near the regions of low cost) are then added to the training set.  \n",
      " • A new DNN surrogate is retrained on this enriched training set and then used in the next iteration of optimization.  \n",
      " • The iterative “query” mechanism means that the surrogate model is refined particularly in the areas that matter for reducing the cost function G.  \n",
      " • Under suitable regularity assumptions on the observable L and cost function G (e.g., smoothness and convexity assumptions), the theory shows that the error at iteration k obeys a bound of the form |ȳ – ȳₖ^j| ≤ Cσₖ; here, σₖ decays exponentially with the number of iterations. In addition, the variance among computed minimizers also drops exponentially, in sharp contrast to the algebraic decay (and high sensitivity) encountered with DNNopt.\n",
      "\n",
      "──────────────────────────────\n",
      "3. Theoretical Underpinnings\n",
      "\n",
      "The article lays out several hypotheses and assumptions:  \n",
      "\n",
      "• It assumes that the map L (from parameters to PDE observables) is sufficiently smooth (belonging to a Sobolev space like W²,∞) so that its pointwise value and derivative errors are controllable.  \n",
      "• The cost function G is assumed to be smooth and strictly convex (or to have a unique global minimizer), ensuring that a unique optimum exists.  \n",
      "• To train the DNN surrogate, the authors use a Lipschitz loss function. In addition to penalizing errors |L(y) – Lθ(y)|, the loss also controls differences between the gradients ∇L(y) and ∇Lθ(y) through an appropriate regularization. This choice is crucial because it ensures that the surrogate’s behavior near minima is well approximated.  \n",
      "• For DNNopt, the convergence analysis shows that the distance between the computed minimizer and the true minimizer is bounded in a way that includes a term proportional to N^(–1/d); thus, when d is large, many training samples are needed to get a small error.  \n",
      "• In contrast, the analysis for ISMO shows that if the initially trained surrogate is “good enough” (ensuring that the “newly added” training points lie sufficiently close to the previously computed minimizers), then the error in subsequent iterations decays exponentially with the number of iterations. This active refinement allows the overall method not only to be more accurate but also to have dramatically reduced sensitivity with respect to the starting values.\n",
      "\n",
      "──────────────────────────────\n",
      "4. Numerical Validation and Comparisons\n",
      "\n",
      "The authors validate their theoretical claims on three representative problems:\n",
      "\n",
      "A. Optimal Control of Projectile Motion:  \n",
      " • Here, the goal is to determine the projectile’s release angle and initial velocity so that its horizontal range exactly meets a target value.  \n",
      " • Using DNNopt, the mean of the cost function approaches zero as more training samples are accumulated; however, the standard deviation (a measure of variability among multiple optimization runs) decays slowly.  \n",
      " • When using ISMO, the mean cost drops to nearly zero much more quickly, and—more notably—the variance among computed minimizers is reduced significantly (exhibiting an exponential decay).  \n",
      " • A visualization shows that as ISMO iterates, the new training points concentrate around the “parabola” of true minimizers.\n",
      "\n",
      "B. Inverse Problem for the Heat Equation:  \n",
      " • In this data assimilation task, the authors address the reconstruction of certain parameters (for instance, the diffusion coefficient and parameters in the initial condition) from measurements at the final time.  \n",
      " • For a moderately high-dimensional parameter space (10 dimensions), the fixed-set surrogate (DNNopt) converges very slowly with high variance.  \n",
      " • With ISMO, both the mean of the cost function and the statistics (range/network of approximated values) improve more steadily, suggesting that the method can robustly recover the unknown parameters.\n",
      "\n",
      "C. Airfoil Shape Optimization:  \n",
      " • In aerodynamic design, one typical goal is to reduce drag while maintaining lift near a prescribed value. In this example, the shape of an airfoil (parametrized via Hicks-Henne bump functions) is optimized so that, when embedded in the compressible Euler equations for a given flow, the drag is minimized subject to lift constraints.  \n",
      " • DNNopt shows an oscillatory error decay with persistent sensitivity. In contrast, ISMO steadily lowers both the mean cost and its variance.  \n",
      " • Moreover, when compared with a black-box truncated Newton (TNC) optimizer (which requires many calls to the full PDE solver), ISMO achieves an order-of-magnitude lower cost function while using significantly fewer evaluations.  \n",
      " • Flow visualizations further indicate that the improved designs (obtained using ISMO) lead to physically relevant reductions in shock strength – a key mechanism for drag reduction.\n",
      "\n",
      "──────────────────────────────\n",
      "5. Relation to Recent Advances in Deep Learning Optimizers\n",
      "\n",
      "Recent publications in the area of deep learning for physics and optimization (for example, works by Han et al. on solving high-dimensional PDEs and by Raissi and Karniadakis on physics-informed neural networks) have shown that training strategies and active learning principles can remediate the “curse of dimensionality” and improve convergence rates. In addition, other recent research has used surrogate models such as Gaussian process regression to guide expensive optimization tasks in engineering design.\n",
      "\n",
      "By contrast, the ISMO algorithm specifically addresses two intertwined issues in surrogate-assisted optimization:\n",
      "  \n",
      " – How to ensure that the surrogate (here, a DNN) is sufficiently accurate where it matters most (i.e., near optimality).  \n",
      " – How to progressively and actively update the training set so that the surrogate “learns” from its own optimization process and thereby avoids the pitfalls of a fixed training regime.\n",
      "\n",
      "In this sense, ISMO can be seen as part of a growing trend in novel optimization algorithms for deep learning. It bridges ideas from active learning, surrogate-based optimization, and classical gradient-based minimization. Compared to other novel optimizers (for instance, those that adaptively adjust learning rates or leverage higher-order information), ISMO is unique in that it uses an iterative loop between a deep learner and a “teacher” (an optimizer) to query new data in regions that are most critical to lowering the cost function.\n",
      "\n",
      "This approach resonates with recent studies that emphasize “learning from scratch” in settings where the optimizer itself informs the data selection process. The exponential improvement in convergence (both in terms of the objective value and the variance among minimizers) achieved by ISMO stands in contrast to the algebraic convergence rates typical of fixed-training-set approaches. In practice, as confirmed by the numerical experiments, such improvements can translate into orders-of-magnitude savings in computational cost for PDE-constrained design tasks.\n",
      "\n",
      "──────────────────────────────\n",
      "6. Concluding Remarks\n",
      "\n",
      "The article makes a significant contribution by addressing a central challenge in surrogate-assisted PDE-constrained optimization. Whereas standard surrogate approaches (such as DNNopt) suffer from slow (algebraic) error decay and high sensitivity owing to fixed training sets, ISMO uses active learning to “zoom in” on the regions near the optimum. The theoretical analysis shows that under suitable regularity and convexity assumptions the approximation error, as well as the variance with respect to starting points, decays exponentially with the number of iterations. This is validated on three practical problems ranging from optimal control and inverse problems to aerodynamic shape optimization.\n",
      "\n",
      "In the broader landscape of deep learning optimizers, ISMO helps shift the focus from merely tuning gradient descent or adaptive learning rates to integrating active-learning loops and surrogate models. This strategy not only makes the surrogate more accurate where it counts but also mitigates the curse of dimensionality encountered in many high-dimensional optimization problems.\n",
      "\n",
      "By coupling deep neural network approximators with iterative augmentation of training data (guided by the outcomes of a standard optimizer), ISMO represents a promising framework that complements recent advances in deep learning for scientific computing and engineering design. Such active learning–based frameworks may also be extended further to incorporate gradient-free methods, uncertainty quantification, or even other surrogate approaches like Gaussian process regression. In any case, the ISMO algorithm stands as a strong example of how novel optimization algorithms can harness both learning and traditional optimization techniques to solve problems that would otherwise be computationally prohibitive.\n",
      "\n",
      "──────────────────────────────\n",
      "To summarize, this work is emblematic of a new wave of novel optimization algorithms in deep learning—by intertwining active learning with surrogate-assisted optimization, it presents a method that is both more robust and computationally efficient than traditional methods. In light of other recent publications in the field that focus on high-dimensional PDE solvers (e.g., physics-informed neural networks) and adaptive data sampling strategies, ISMO provides a compelling blueprint for how deep learning optimizers can evolve to meet the challenges of complex, high-cost applications.\n",
      "\n",
      "This paper, along with selected recent work on adaptive surrogate modeling and active learning approaches in optimization, marks a significant step forward in developing novel deep learning optimization frameworks that reduce computational cost while enhancing robustness and accuracy.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(paper_list[1][\"summary\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e4df26-cb41-4aef-a990-e34c7007306b",
   "metadata": {},
   "source": [
    "# Literature Review on Recent Advances in Machine Learning Optimizers\n",
    "\n",
    "The field of optimization in machine learning has seen significant advancements in recent years. These developments include novel algorithms, techniques for integrating machine learning with traditional optimization methods, and applications across various domains. This literature review synthesizes the key findings from recent studies highlighting innovative approaches to optimization problems in machine learning.\n",
    "\n",
    "## 1. Novel Optimization Techniques\n",
    "\n",
    "One notable advancement is the integration of Optimal Transport (OT) theory into machine learning optimization methodologies. OT provides a probabilistic framework for comparing probability distributions, which has proven useful for data-driven tasks such as generative modeling and transfer learning. Montesuma et al. (2023) discussed significant contributions in this area, emphasizing recent innovations in computational OT methods and their implications for various aspects of machine learning, including supervised and unsupervised learning paradigms (Montesuma, E. F., Mboula, F. N., & Souloumiac, A. (2023). *Recent Advances in Optimal Transport for Machine Learning*. arXiv:2306.16156).\n",
    "\n",
    "Further, recent studies have harnessed machine learning to enhance conventional optimization techniques, specifically in areas like model predictive control (MPC). E et al. (2022) highlighted how machine learning can empower control solvers, addressing challenges inherent in tackling complex optimal control problems (E, W., Han, J., & Long, J. (2022). *Empowering Optimal Control with Machine Learning: A Perspective from Model Predictive Control*. arXiv:2205.07990).\n",
    "\n",
    "## 2. Constrained Optimization\n",
    "\n",
    "The challenge of integrating machine learning with combinatorial optimization is another area garnering attention. Kotary et al. (2021) provided a comprehensive survey on leveraging machine learning to solve constrained optimization problems. They emphasized the potential for hybrid models that can generate fast, approximate solutions through the combination of machine learning architectures and combinatorial optimizers (Kotary, J., Fioretto, F., Van Hentenryck, P., & Wilder, B. (2021). *End-to-End Constrained Optimization Learning: A Survey*. arXiv:2103.16378).\n",
    "\n",
    "## 3. Process Optimization\n",
    "\n",
    "The application of machine learning in process optimization, especially within chemical engineering, has also seen notable advances. Mitrai and Daoutidis (2024) reviewed strategies for automating the selection and tuning of optimization algorithms based on learned behaviors from numerical solvers. They discussed the importance of representing decision-making problems effectively for machine learning tasks (Mitrai, I., & Daoutidis, P. (2024). *Accelerating Process Control and Optimization via Machine Learning: A Review*. arXiv:2412.18529).\n",
    "\n",
    "## 4. The Interplay Between Machine Learning and Mathematical Programming\n",
    "\n",
    "The integration of machine learning and mathematical programming has opened up new avenues for optimization under uncertainty. Ning and You (2019) explored how data-driven optimization can synergistically link machine learning with traditional optimization frameworks, facilitating better decision-making under uncertain conditions. Their review covered various data-driven approaches, identifying critical research opportunities in this emerging inter-disciplinary field (Ning, C., & You, F. (2019). *Optimization under Uncertainty in the Era of Big Data and Deep Learning: When Machine Learning Meets Mathematical Programming*. arXiv:1904.01934).\n",
    "\n",
    "## 5. Hybrid Models and Future Directions\n",
    "\n",
    "The emergent trend toward hybrid models that combine classical optimization techniques with machine learning is becoming increasingly prominent. These models have demonstrated their capability in tackling intricate systems where conventional methods fall short. As research continues to evolve, scholars advocate for more adaptive algorithms that can learn from their performance dynamically, which has significant implications for real-time optimization scenarios.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "In conclusion, recent advancements in machine learning optimizers reveal a clear trajectory toward the integration of traditional optimization strategies with machine learning innovations. This synthesis of techniques not only enhances the efficiency of optimization processes but also broadens their applicability across diverse fields. Future research is expected to delve deeper into hybrid models, the interactions between learning and optimization, and uncertainty handling mechanisms in complex environments.\n",
    "\n",
    "### References\n",
    "\n",
    "E, W., Han, J., & Long, J. (2022). Empowering Optimal Control with Machine Learning: A Perspective from Model Predictive Control. *arXiv:2205.07990*.  \n",
    "\n",
    "Mitrai, I., & Daoutidis, P. (2024). Accelerating Process Control and Optimization via Machine Learning: A Review. *arXiv:2412.18529*.  \n",
    "\n",
    "Montesuma, E. F., Mboula, F. N., & Souloumiac, A. (2023). Recent Advances in Optimal Transport for Machine Learning. *arXiv:2306.16156*.  \n",
    "\n",
    "Ning, C., & You, F. (2019). Optimization under Uncertainty in the Era of Big Data and Deep Learning: When Machine Learning Meets Mathematical Programming. *arXiv:1904.01934*.\n",
    "\n",
    "Kotary, J., Fioretto, F., Van Hentenryck, P., & Wilder, B. (2021). End-to-End Constrained Optimization Learning: A Survey. *arXiv:2103.16378*.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "66f166d1-5fe1-43f7-b3c7-c72feba4eae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "response = requests.get(\n",
    "    \"https://api.aimlapi.com/models\",\n",
    "    headers={\"Accept\":\"*/*\"},\n",
    ")\n",
    "\n",
    "data = response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "216664b3-4464-4cdc-86ca-3dcfa93217b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "df_models = pd.DataFrame(json.loads(response.content)[\"data\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "503c391b-cf9d-4268-bdf1-fc5fe9fd21ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_models = pd.concat([df_models,pd.json_normalize(df_models[\"info\"])], axis = 1).drop(\"info\", axis = \"columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "8e1cd405-f90e-4b28-a461-d73c27d5037e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>type</th>\n",
       "      <th>features</th>\n",
       "      <th>endpoints</th>\n",
       "      <th>name</th>\n",
       "      <th>developer</th>\n",
       "      <th>description</th>\n",
       "      <th>contextLength</th>\n",
       "      <th>maxTokens</th>\n",
       "      <th>url</th>\n",
       "      <th>docs_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>openai/gpt-4o</td>\n",
       "      <td>chat-completion</td>\n",
       "      <td>[openai/chat-completion, openai/response-api, ...</td>\n",
       "      <td>[/v1/chat/completions, /v1/responses]</td>\n",
       "      <td>GPT 4o</td>\n",
       "      <td>Open AI</td>\n",
       "      <td>GPT-4o integrates text, vision, and audio for ...</td>\n",
       "      <td>128000.0</td>\n",
       "      <td>16384.0</td>\n",
       "      <td>https://aimlapi.com/models/chat-gpt-4-omni</td>\n",
       "      <td>https://docs.aimlapi.com/api-references/text-m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gpt-4o-2024-08-06</td>\n",
       "      <td>chat-completion</td>\n",
       "      <td>[openai/chat-completion, openai/response-api, ...</td>\n",
       "      <td>[/v1/chat/completions, /v1/responses]</td>\n",
       "      <td>GPT 4o 2024-08-06</td>\n",
       "      <td>Open AI</td>\n",
       "      <td>Multimodal AI model by OpenAI enhancing human-...</td>\n",
       "      <td>128000.0</td>\n",
       "      <td>16384.0</td>\n",
       "      <td>https://aimlapi.com/models/gpt-4o-2024-08-06-api</td>\n",
       "      <td>https://docs.aimlapi.com/api-references/text-m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gpt-4o-2024-05-13</td>\n",
       "      <td>chat-completion</td>\n",
       "      <td>[openai/chat-completion, openai/response-api, ...</td>\n",
       "      <td>[/v1/chat/completions, /v1/responses]</td>\n",
       "      <td>GPT 4o 2024-05-13</td>\n",
       "      <td>Open AI</td>\n",
       "      <td>GPT-4o-2024-05-13 is the initial release versi...</td>\n",
       "      <td>128000.0</td>\n",
       "      <td>4096.0</td>\n",
       "      <td>https://aimlapi.com/models/gpt-4o-2024-05-13-api</td>\n",
       "      <td>https://docs.aimlapi.com/api-references/text-m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gpt-4o-mini</td>\n",
       "      <td>chat-completion</td>\n",
       "      <td>[openai/chat-completion, openai/response-api, ...</td>\n",
       "      <td>[/v1/chat/completions, /v1/responses]</td>\n",
       "      <td>GPT 4o mini</td>\n",
       "      <td>Open AI</td>\n",
       "      <td>GPT-4o Mini: Cost-efficient, advanced model fo...</td>\n",
       "      <td>128000.0</td>\n",
       "      <td>16384.0</td>\n",
       "      <td>https://aimlapi.com/models/chat-gpt-4o-mini</td>\n",
       "      <td>https://docs.aimlapi.com/api-references/text-m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>gpt-4o-mini-2024-07-18</td>\n",
       "      <td>chat-completion</td>\n",
       "      <td>[openai/chat-completion, openai/response-api, ...</td>\n",
       "      <td>[/v1/chat/completions, /v1/responses]</td>\n",
       "      <td>GPT 4o mini 2024-07-18</td>\n",
       "      <td>Open AI</td>\n",
       "      <td>GPT-4o Mini: Cost-efficient, advanced model fo...</td>\n",
       "      <td>128000.0</td>\n",
       "      <td>16384.0</td>\n",
       "      <td>https://aimlapi.com/models/chat-gpt-4o-mini</td>\n",
       "      <td>https://docs.aimlapi.com/api-references/text-m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>chatgpt-4o-latest</td>\n",
       "      <td>chat-completion</td>\n",
       "      <td>[openai/chat-completion, openai/response-api, ...</td>\n",
       "      <td>[/v1/chat/completions, /v1/responses]</td>\n",
       "      <td>ChatGPT 4o latest</td>\n",
       "      <td>Open AI</td>\n",
       "      <td>Multimodal AI model by OpenAI enhancing human-...</td>\n",
       "      <td>128000.0</td>\n",
       "      <td>16384.0</td>\n",
       "      <td>https://aimlapi.com/models/gpt-4o-2024-08-06-api</td>\n",
       "      <td>https://docs.aimlapi.com/api-references/text-m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>gpt-4-turbo</td>\n",
       "      <td>chat-completion</td>\n",
       "      <td>[openai/chat-completion, openai/response-api, ...</td>\n",
       "      <td>[/v1/chat/completions, /v1/responses]</td>\n",
       "      <td>GPT 4 turbo</td>\n",
       "      <td>Open AI</td>\n",
       "      <td>High-speed AI model for instant language proce...</td>\n",
       "      <td>128000.0</td>\n",
       "      <td>4096.0</td>\n",
       "      <td>https://aimlapi.com/models/chat-gpt-4-turbo</td>\n",
       "      <td>https://docs.aimlapi.com/api-references/text-m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>gpt-4-turbo-2024-04-09</td>\n",
       "      <td>chat-completion</td>\n",
       "      <td>[openai/chat-completion, openai/response-api, ...</td>\n",
       "      <td>[/v1/chat/completions, /v1/responses]</td>\n",
       "      <td>GPT 4 turbo 2024-04-09</td>\n",
       "      <td>Open AI</td>\n",
       "      <td></td>\n",
       "      <td>128000.0</td>\n",
       "      <td>4096.0</td>\n",
       "      <td></td>\n",
       "      <td>https://docs.aimlapi.com/api-references/text-m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>gpt-4</td>\n",
       "      <td>chat-completion</td>\n",
       "      <td>[openai/chat-completion, openai/response-api, ...</td>\n",
       "      <td>[/v1/chat/completions, /v1/responses]</td>\n",
       "      <td>GPT 4</td>\n",
       "      <td>Open AI</td>\n",
       "      <td>Revolutionary AI model for unparalleled natura...</td>\n",
       "      <td>8000.0</td>\n",
       "      <td>8192.0</td>\n",
       "      <td>https://aimlapi.com/models/chat-gpt-4</td>\n",
       "      <td>https://docs.aimlapi.com/api-references/text-m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>gpt-4-0125-preview</td>\n",
       "      <td>chat-completion</td>\n",
       "      <td>[openai/chat-completion, openai/response-api, ...</td>\n",
       "      <td>[/v1/chat/completions, /v1/responses]</td>\n",
       "      <td>GPT 4 0125 preview</td>\n",
       "      <td>Open AI</td>\n",
       "      <td></td>\n",
       "      <td>8000.0</td>\n",
       "      <td>4096.0</td>\n",
       "      <td></td>\n",
       "      <td>https://docs.aimlapi.com/api-references/text-m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>gpt-4-1106-preview</td>\n",
       "      <td>chat-completion</td>\n",
       "      <td>[openai/chat-completion, openai/response-api, ...</td>\n",
       "      <td>[/v1/chat/completions, /v1/responses]</td>\n",
       "      <td>GPT 4 1106 preview</td>\n",
       "      <td>Open AI</td>\n",
       "      <td></td>\n",
       "      <td>8000.0</td>\n",
       "      <td>4096.0</td>\n",
       "      <td></td>\n",
       "      <td>https://docs.aimlapi.com/api-references/text-m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>gpt-3.5-turbo</td>\n",
       "      <td>chat-completion</td>\n",
       "      <td>[openai/chat-completion, openai/response-api, ...</td>\n",
       "      <td>[/v1/chat/completions, /v1/responses]</td>\n",
       "      <td>GPT 3.5 turbo</td>\n",
       "      <td>Open AI</td>\n",
       "      <td></td>\n",
       "      <td>16000.0</td>\n",
       "      <td>4096.0</td>\n",
       "      <td></td>\n",
       "      <td>https://docs.aimlapi.com/api-references/text-m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>gpt-3.5-turbo-0125</td>\n",
       "      <td>chat-completion</td>\n",
       "      <td>[openai/chat-completion, openai/response-api, ...</td>\n",
       "      <td>[/v1/chat/completions, /v1/responses]</td>\n",
       "      <td>GPT 3.5 turbo 0125</td>\n",
       "      <td>Open AI</td>\n",
       "      <td>The newest GPT-3.5 Turbo with improved accurac...</td>\n",
       "      <td>16000.0</td>\n",
       "      <td>4096.0</td>\n",
       "      <td>https://aimlapi.com/models/chat-gpt-3-5-turbo-...</td>\n",
       "      <td>https://docs.aimlapi.com/api-references/text-m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>gpt-3.5-turbo-1106</td>\n",
       "      <td>chat-completion</td>\n",
       "      <td>[openai/chat-completion, openai/response-api, ...</td>\n",
       "      <td>[/v1/chat/completions, /v1/responses]</td>\n",
       "      <td>GPT 3.5 turbo 1106</td>\n",
       "      <td>Open AI</td>\n",
       "      <td>The GPT-3.5 Turbo 1106 model features enhanced...</td>\n",
       "      <td>16000.0</td>\n",
       "      <td>4096.0</td>\n",
       "      <td>https://aimlapi.com/models/chat-gpt-3-5-turbo-...</td>\n",
       "      <td>https://docs.aimlapi.com/api-references/text-m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>o3-mini</td>\n",
       "      <td>chat-completion</td>\n",
       "      <td>[openai/chat-completion, openai/response-api, ...</td>\n",
       "      <td>[/v1/chat/completions, /v1/responses]</td>\n",
       "      <td>o3 mini</td>\n",
       "      <td>Open AI</td>\n",
       "      <td>OpenAI o3-mini excels in reasoning tasks with ...</td>\n",
       "      <td>200000.0</td>\n",
       "      <td>100000.0</td>\n",
       "      <td>https://aimlapi.com/models/openai-o3-mini-api</td>\n",
       "      <td>https://docs.aimlapi.com/api-references/text-m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>gpt-4o-audio-preview</td>\n",
       "      <td>chat-completion</td>\n",
       "      <td>[openai/chat-completion, openai/chat-completio...</td>\n",
       "      <td>[/v1/chat/completions]</td>\n",
       "      <td>Chat GPT 4o audio preview</td>\n",
       "      <td>Open AI</td>\n",
       "      <td>GPT-4o Audio Preview is OpenAI's latest flagsh...</td>\n",
       "      <td>128000.0</td>\n",
       "      <td>16384.0</td>\n",
       "      <td>https://aimlapi.com/models/gpt-4o-audio-previe...</td>\n",
       "      <td>https://docs.aimlapi.com/api-references/text-m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>gpt-4o-mini-audio-preview</td>\n",
       "      <td>chat-completion</td>\n",
       "      <td>[openai/chat-completion, openai/chat-completio...</td>\n",
       "      <td>[/v1/chat/completions]</td>\n",
       "      <td>Chat GPT 4o mini audio preview</td>\n",
       "      <td>Open AI</td>\n",
       "      <td>GPT-4o Mini Audio adds speech-to-text and text...</td>\n",
       "      <td>128000.0</td>\n",
       "      <td>16384.0</td>\n",
       "      <td>https://aimlapi.com/models/gpt-4o-mini-audio-api</td>\n",
       "      <td>https://docs.aimlapi.com/api-references/text-m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>openai/gpt-audio</td>\n",
       "      <td>chat-completion</td>\n",
       "      <td>[openai/chat-completion, openai/chat-completio...</td>\n",
       "      <td>[/v1/chat/completions]</td>\n",
       "      <td>Chat GPT audio</td>\n",
       "      <td>Open AI</td>\n",
       "      <td>GPT Audio is OpenAI's latest flagship model ca...</td>\n",
       "      <td>128000.0</td>\n",
       "      <td>16384.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>openai/gpt-audio-mini</td>\n",
       "      <td>chat-completion</td>\n",
       "      <td>[openai/chat-completion, openai/chat-completio...</td>\n",
       "      <td>[/v1/chat/completions]</td>\n",
       "      <td>Chat GPT mini audio</td>\n",
       "      <td>Open AI</td>\n",
       "      <td>GPT Mini Audio adds speech-to-text and text-to...</td>\n",
       "      <td>128000.0</td>\n",
       "      <td>16384.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>gpt-4o-search-preview</td>\n",
       "      <td>chat-completion</td>\n",
       "      <td>[openai/chat-completion, openai/chat-completio...</td>\n",
       "      <td>[/v1/chat/completions]</td>\n",
       "      <td>Chat GPT 4o search preview</td>\n",
       "      <td>Open AI</td>\n",
       "      <td>GPT-4o Search Preview blends OpenAI's GPT-4o c...</td>\n",
       "      <td>128000.0</td>\n",
       "      <td>16384.0</td>\n",
       "      <td>https://aimlapi.com/models/gpt-4o-search-previ...</td>\n",
       "      <td>https://docs.aimlapi.com/api-references/text-m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>gpt-4o-mini-search-preview</td>\n",
       "      <td>chat-completion</td>\n",
       "      <td>[openai/chat-completion, openai/chat-completio...</td>\n",
       "      <td>[/v1/chat/completions]</td>\n",
       "      <td>Chat GPT 4o mini search preview</td>\n",
       "      <td>Open AI</td>\n",
       "      <td>GPT-4o Mini Search Preview is a more efficient...</td>\n",
       "      <td>128000.0</td>\n",
       "      <td>16384.0</td>\n",
       "      <td>https://aimlapi.com/models/gpt-4o-mini-search-...</td>\n",
       "      <td>https://docs.aimlapi.com/api-references/text-m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>openai/gpt-4.1-2025-04-14</td>\n",
       "      <td>chat-completion</td>\n",
       "      <td>[openai/chat-completion, openai/response-api, ...</td>\n",
       "      <td>[/v1/chat/completions, /v1/responses]</td>\n",
       "      <td>Chat GPT 4.1</td>\n",
       "      <td>Open AI</td>\n",
       "      <td></td>\n",
       "      <td>1000000.0</td>\n",
       "      <td>32768.0</td>\n",
       "      <td></td>\n",
       "      <td>https://docs.aimlapi.com/api-references/text-m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>openai/gpt-4.1-mini-2025-04-14</td>\n",
       "      <td>chat-completion</td>\n",
       "      <td>[openai/chat-completion, openai/response-api, ...</td>\n",
       "      <td>[/v1/chat/completions, /v1/responses]</td>\n",
       "      <td>Chat GPT 4.1 mini</td>\n",
       "      <td>Open AI</td>\n",
       "      <td></td>\n",
       "      <td>1000000.0</td>\n",
       "      <td>32768.0</td>\n",
       "      <td></td>\n",
       "      <td>https://docs.aimlapi.com/api-references/text-m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>openai/gpt-4.1-nano-2025-04-14</td>\n",
       "      <td>chat-completion</td>\n",
       "      <td>[openai/chat-completion, openai/response-api, ...</td>\n",
       "      <td>[/v1/chat/completions, /v1/responses]</td>\n",
       "      <td>Chat GPT 4.1 nano</td>\n",
       "      <td>Open AI</td>\n",
       "      <td></td>\n",
       "      <td>1000000.0</td>\n",
       "      <td>32768.0</td>\n",
       "      <td></td>\n",
       "      <td>https://docs.aimlapi.com/api-references/text-m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>openai/o4-mini-2025-04-16</td>\n",
       "      <td>chat-completion</td>\n",
       "      <td>[openai/chat-completion, openai/response-api, ...</td>\n",
       "      <td>[/v1/chat/completions, /v1/responses]</td>\n",
       "      <td>o4-mini</td>\n",
       "      <td>Open AI</td>\n",
       "      <td></td>\n",
       "      <td>200000.0</td>\n",
       "      <td>100000.0</td>\n",
       "      <td></td>\n",
       "      <td>https://docs.aimlapi.com/api-references/text-m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>openai/o3-2025-04-16</td>\n",
       "      <td>chat-completion</td>\n",
       "      <td>[openai/chat-completion, openai/response-api, ...</td>\n",
       "      <td>[/v1/chat/completions, /v1/responses]</td>\n",
       "      <td>o3</td>\n",
       "      <td>Open AI</td>\n",
       "      <td></td>\n",
       "      <td>200000.0</td>\n",
       "      <td>100000.0</td>\n",
       "      <td></td>\n",
       "      <td>https://docs.aimlapi.com/api-references/text-m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>o1</td>\n",
       "      <td>chat-completion</td>\n",
       "      <td>[openai/chat-completion, openai/response-api, ...</td>\n",
       "      <td>[/v1/chat/completions, /v1/responses]</td>\n",
       "      <td>o1</td>\n",
       "      <td>Open AI</td>\n",
       "      <td>OpenAI o1 excels in complex reasoning tasks wi...</td>\n",
       "      <td>200000.0</td>\n",
       "      <td>100000.0</td>\n",
       "      <td>https://aimlapi.com/models/openai-o1-api</td>\n",
       "      <td>https://docs.aimlapi.com/api-references/text-m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>openai/gpt-5-2025-08-07</td>\n",
       "      <td>chat-completion</td>\n",
       "      <td>[openai/chat-completion, openai/response-api, ...</td>\n",
       "      <td>[/v1/chat/completions, /v1/responses]</td>\n",
       "      <td>GPT-5</td>\n",
       "      <td>Open AI</td>\n",
       "      <td></td>\n",
       "      <td>400000.0</td>\n",
       "      <td>128000.0</td>\n",
       "      <td></td>\n",
       "      <td>https://docs.aimlapi.com/api-references/text-m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>openai/gpt-5-mini-2025-08-07</td>\n",
       "      <td>chat-completion</td>\n",
       "      <td>[openai/chat-completion, openai/response-api, ...</td>\n",
       "      <td>[/v1/chat/completions, /v1/responses]</td>\n",
       "      <td>GPT-5 mini</td>\n",
       "      <td>Open AI</td>\n",
       "      <td></td>\n",
       "      <td>400000.0</td>\n",
       "      <td>128000.0</td>\n",
       "      <td></td>\n",
       "      <td>https://docs.aimlapi.com/api-references/text-m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>openai/gpt-5-nano-2025-08-07</td>\n",
       "      <td>chat-completion</td>\n",
       "      <td>[openai/chat-completion, openai/response-api, ...</td>\n",
       "      <td>[/v1/chat/completions, /v1/responses]</td>\n",
       "      <td>GPT-5 nano</td>\n",
       "      <td>Open AI</td>\n",
       "      <td></td>\n",
       "      <td>400000.0</td>\n",
       "      <td>128000.0</td>\n",
       "      <td></td>\n",
       "      <td>https://docs.aimlapi.com/api-references/text-m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>openai/gpt-5-chat-latest</td>\n",
       "      <td>chat-completion</td>\n",
       "      <td>[openai/chat-completion, openai/response-api, ...</td>\n",
       "      <td>[/v1/chat/completions, /v1/responses]</td>\n",
       "      <td>GPT-5 Chat</td>\n",
       "      <td>Open AI</td>\n",
       "      <td></td>\n",
       "      <td>400000.0</td>\n",
       "      <td>16384.0</td>\n",
       "      <td></td>\n",
       "      <td>https://docs.aimlapi.com/api-references/text-m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>openai/gpt-5-1</td>\n",
       "      <td>chat-completion</td>\n",
       "      <td>[openai/chat-completion, openai/response-api, ...</td>\n",
       "      <td>[/v1/chat/completions, /v1/responses]</td>\n",
       "      <td>GPT-5.1</td>\n",
       "      <td>Open AI</td>\n",
       "      <td></td>\n",
       "      <td>400000.0</td>\n",
       "      <td>128000.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>openai/gpt-5-1-chat-latest</td>\n",
       "      <td>chat-completion</td>\n",
       "      <td>[openai/chat-completion, openai/response-api, ...</td>\n",
       "      <td>[/v1/chat/completions, /v1/responses]</td>\n",
       "      <td>GPT-5.1 Chat Latest</td>\n",
       "      <td>Open AI</td>\n",
       "      <td></td>\n",
       "      <td>400000.0</td>\n",
       "      <td>128000.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>openai/o3-pro</td>\n",
       "      <td>responses</td>\n",
       "      <td>[openai/response-api, openai/chat-completion.v...</td>\n",
       "      <td>[/v1/responses]</td>\n",
       "      <td>o3-pro</td>\n",
       "      <td>Open AI</td>\n",
       "      <td></td>\n",
       "      <td>200000.0</td>\n",
       "      <td>100000.0</td>\n",
       "      <td></td>\n",
       "      <td>https://docs.aimlapi.com/api-references/text-m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>openai/gpt-5-pro</td>\n",
       "      <td>responses</td>\n",
       "      <td>[openai/response-api]</td>\n",
       "      <td>[/v1/responses]</td>\n",
       "      <td>gpt 5 pro</td>\n",
       "      <td>Open AI</td>\n",
       "      <td></td>\n",
       "      <td>400000.0</td>\n",
       "      <td>272000.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>openai/gpt-5-1-codex</td>\n",
       "      <td>responses</td>\n",
       "      <td>[openai/response-api]</td>\n",
       "      <td>[/v1/responses]</td>\n",
       "      <td>GPT-5.1 Codex</td>\n",
       "      <td>Open AI</td>\n",
       "      <td></td>\n",
       "      <td>400000.0</td>\n",
       "      <td>128000.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>openai/gpt-5-1-codex-mini</td>\n",
       "      <td>responses</td>\n",
       "      <td>[openai/response-api]</td>\n",
       "      <td>[/v1/responses]</td>\n",
       "      <td>GPT-5.1 Codex Mini</td>\n",
       "      <td>Open AI</td>\n",
       "      <td></td>\n",
       "      <td>400000.0</td>\n",
       "      <td>128000.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>mistralai/Mixtral-8x7B-Instruct-v0.1</td>\n",
       "      <td>chat-completion</td>\n",
       "      <td>[openai/chat-completion.max-completion-tokens,...</td>\n",
       "      <td>[/v1/chat/completions]</td>\n",
       "      <td>Mixtral 8x7B Instruct v0.1</td>\n",
       "      <td>Mistral AI</td>\n",
       "      <td>Harness the power of tailored AI with Mixtral-...</td>\n",
       "      <td>64000.0</td>\n",
       "      <td>32768.0</td>\n",
       "      <td>https://aimlapi.com/models/mixtral-8x7b-instru...</td>\n",
       "      <td>https://docs.aimlapi.com/api-references/text-m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>meta-llama/Llama-3.3-70B-Instruct-Turbo</td>\n",
       "      <td>chat-completion</td>\n",
       "      <td>[openai/chat-completion.function, openai/chat-...</td>\n",
       "      <td>[/v1/chat/completions]</td>\n",
       "      <td>Llama 3.3 70B Instruct Turbo</td>\n",
       "      <td>Meta</td>\n",
       "      <td>Meta Llama 3.3 70B Instruct Turbo is an advanc...</td>\n",
       "      <td>128000.0</td>\n",
       "      <td>127000.0</td>\n",
       "      <td>https://aimlapi.com/models/meta-llama-3-3-70b-...</td>\n",
       "      <td>https://docs.aimlapi.com/api-references/text-m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>meta-llama/Llama-3.2-3B-Instruct-Turbo</td>\n",
       "      <td>chat-completion</td>\n",
       "      <td>[openai/chat-completion.function, openai/chat-...</td>\n",
       "      <td>[/v1/chat/completions]</td>\n",
       "      <td>Llama 3.2 3B Instruct Turbo</td>\n",
       "      <td>Meta</td>\n",
       "      <td>Efficient multilingual LLM for diverse NLP tas...</td>\n",
       "      <td>131000.0</td>\n",
       "      <td>130000.0</td>\n",
       "      <td>https://aimlapi.com/models/llama-3-2-3b-instru...</td>\n",
       "      <td>https://docs.aimlapi.com/api-references/text-m...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         id             type  \\\n",
       "0                             openai/gpt-4o  chat-completion   \n",
       "1                         gpt-4o-2024-08-06  chat-completion   \n",
       "2                         gpt-4o-2024-05-13  chat-completion   \n",
       "3                               gpt-4o-mini  chat-completion   \n",
       "4                    gpt-4o-mini-2024-07-18  chat-completion   \n",
       "5                         chatgpt-4o-latest  chat-completion   \n",
       "6                               gpt-4-turbo  chat-completion   \n",
       "7                    gpt-4-turbo-2024-04-09  chat-completion   \n",
       "8                                     gpt-4  chat-completion   \n",
       "9                        gpt-4-0125-preview  chat-completion   \n",
       "10                       gpt-4-1106-preview  chat-completion   \n",
       "11                            gpt-3.5-turbo  chat-completion   \n",
       "12                       gpt-3.5-turbo-0125  chat-completion   \n",
       "13                       gpt-3.5-turbo-1106  chat-completion   \n",
       "14                                  o3-mini  chat-completion   \n",
       "15                     gpt-4o-audio-preview  chat-completion   \n",
       "16                gpt-4o-mini-audio-preview  chat-completion   \n",
       "17                         openai/gpt-audio  chat-completion   \n",
       "18                    openai/gpt-audio-mini  chat-completion   \n",
       "19                    gpt-4o-search-preview  chat-completion   \n",
       "20               gpt-4o-mini-search-preview  chat-completion   \n",
       "21                openai/gpt-4.1-2025-04-14  chat-completion   \n",
       "22           openai/gpt-4.1-mini-2025-04-14  chat-completion   \n",
       "23           openai/gpt-4.1-nano-2025-04-14  chat-completion   \n",
       "24                openai/o4-mini-2025-04-16  chat-completion   \n",
       "25                     openai/o3-2025-04-16  chat-completion   \n",
       "26                                       o1  chat-completion   \n",
       "27                  openai/gpt-5-2025-08-07  chat-completion   \n",
       "28             openai/gpt-5-mini-2025-08-07  chat-completion   \n",
       "29             openai/gpt-5-nano-2025-08-07  chat-completion   \n",
       "30                 openai/gpt-5-chat-latest  chat-completion   \n",
       "31                           openai/gpt-5-1  chat-completion   \n",
       "32               openai/gpt-5-1-chat-latest  chat-completion   \n",
       "33                            openai/o3-pro        responses   \n",
       "34                         openai/gpt-5-pro        responses   \n",
       "35                     openai/gpt-5-1-codex        responses   \n",
       "36                openai/gpt-5-1-codex-mini        responses   \n",
       "37     mistralai/Mixtral-8x7B-Instruct-v0.1  chat-completion   \n",
       "38  meta-llama/Llama-3.3-70B-Instruct-Turbo  chat-completion   \n",
       "39   meta-llama/Llama-3.2-3B-Instruct-Turbo  chat-completion   \n",
       "\n",
       "                                             features  \\\n",
       "0   [openai/chat-completion, openai/response-api, ...   \n",
       "1   [openai/chat-completion, openai/response-api, ...   \n",
       "2   [openai/chat-completion, openai/response-api, ...   \n",
       "3   [openai/chat-completion, openai/response-api, ...   \n",
       "4   [openai/chat-completion, openai/response-api, ...   \n",
       "5   [openai/chat-completion, openai/response-api, ...   \n",
       "6   [openai/chat-completion, openai/response-api, ...   \n",
       "7   [openai/chat-completion, openai/response-api, ...   \n",
       "8   [openai/chat-completion, openai/response-api, ...   \n",
       "9   [openai/chat-completion, openai/response-api, ...   \n",
       "10  [openai/chat-completion, openai/response-api, ...   \n",
       "11  [openai/chat-completion, openai/response-api, ...   \n",
       "12  [openai/chat-completion, openai/response-api, ...   \n",
       "13  [openai/chat-completion, openai/response-api, ...   \n",
       "14  [openai/chat-completion, openai/response-api, ...   \n",
       "15  [openai/chat-completion, openai/chat-completio...   \n",
       "16  [openai/chat-completion, openai/chat-completio...   \n",
       "17  [openai/chat-completion, openai/chat-completio...   \n",
       "18  [openai/chat-completion, openai/chat-completio...   \n",
       "19  [openai/chat-completion, openai/chat-completio...   \n",
       "20  [openai/chat-completion, openai/chat-completio...   \n",
       "21  [openai/chat-completion, openai/response-api, ...   \n",
       "22  [openai/chat-completion, openai/response-api, ...   \n",
       "23  [openai/chat-completion, openai/response-api, ...   \n",
       "24  [openai/chat-completion, openai/response-api, ...   \n",
       "25  [openai/chat-completion, openai/response-api, ...   \n",
       "26  [openai/chat-completion, openai/response-api, ...   \n",
       "27  [openai/chat-completion, openai/response-api, ...   \n",
       "28  [openai/chat-completion, openai/response-api, ...   \n",
       "29  [openai/chat-completion, openai/response-api, ...   \n",
       "30  [openai/chat-completion, openai/response-api, ...   \n",
       "31  [openai/chat-completion, openai/response-api, ...   \n",
       "32  [openai/chat-completion, openai/response-api, ...   \n",
       "33  [openai/response-api, openai/chat-completion.v...   \n",
       "34                              [openai/response-api]   \n",
       "35                              [openai/response-api]   \n",
       "36                              [openai/response-api]   \n",
       "37  [openai/chat-completion.max-completion-tokens,...   \n",
       "38  [openai/chat-completion.function, openai/chat-...   \n",
       "39  [openai/chat-completion.function, openai/chat-...   \n",
       "\n",
       "                                endpoints                             name  \\\n",
       "0   [/v1/chat/completions, /v1/responses]                           GPT 4o   \n",
       "1   [/v1/chat/completions, /v1/responses]                GPT 4o 2024-08-06   \n",
       "2   [/v1/chat/completions, /v1/responses]                GPT 4o 2024-05-13   \n",
       "3   [/v1/chat/completions, /v1/responses]                      GPT 4o mini   \n",
       "4   [/v1/chat/completions, /v1/responses]           GPT 4o mini 2024-07-18   \n",
       "5   [/v1/chat/completions, /v1/responses]                ChatGPT 4o latest   \n",
       "6   [/v1/chat/completions, /v1/responses]                      GPT 4 turbo   \n",
       "7   [/v1/chat/completions, /v1/responses]           GPT 4 turbo 2024-04-09   \n",
       "8   [/v1/chat/completions, /v1/responses]                            GPT 4   \n",
       "9   [/v1/chat/completions, /v1/responses]               GPT 4 0125 preview   \n",
       "10  [/v1/chat/completions, /v1/responses]               GPT 4 1106 preview   \n",
       "11  [/v1/chat/completions, /v1/responses]                    GPT 3.5 turbo   \n",
       "12  [/v1/chat/completions, /v1/responses]               GPT 3.5 turbo 0125   \n",
       "13  [/v1/chat/completions, /v1/responses]               GPT 3.5 turbo 1106   \n",
       "14  [/v1/chat/completions, /v1/responses]                          o3 mini   \n",
       "15                 [/v1/chat/completions]        Chat GPT 4o audio preview   \n",
       "16                 [/v1/chat/completions]   Chat GPT 4o mini audio preview   \n",
       "17                 [/v1/chat/completions]                   Chat GPT audio   \n",
       "18                 [/v1/chat/completions]              Chat GPT mini audio   \n",
       "19                 [/v1/chat/completions]       Chat GPT 4o search preview   \n",
       "20                 [/v1/chat/completions]  Chat GPT 4o mini search preview   \n",
       "21  [/v1/chat/completions, /v1/responses]                     Chat GPT 4.1   \n",
       "22  [/v1/chat/completions, /v1/responses]                Chat GPT 4.1 mini   \n",
       "23  [/v1/chat/completions, /v1/responses]                Chat GPT 4.1 nano   \n",
       "24  [/v1/chat/completions, /v1/responses]                          o4-mini   \n",
       "25  [/v1/chat/completions, /v1/responses]                               o3   \n",
       "26  [/v1/chat/completions, /v1/responses]                               o1   \n",
       "27  [/v1/chat/completions, /v1/responses]                            GPT-5   \n",
       "28  [/v1/chat/completions, /v1/responses]                       GPT-5 mini   \n",
       "29  [/v1/chat/completions, /v1/responses]                       GPT-5 nano   \n",
       "30  [/v1/chat/completions, /v1/responses]                       GPT-5 Chat   \n",
       "31  [/v1/chat/completions, /v1/responses]                          GPT-5.1   \n",
       "32  [/v1/chat/completions, /v1/responses]              GPT-5.1 Chat Latest   \n",
       "33                        [/v1/responses]                           o3-pro   \n",
       "34                        [/v1/responses]                        gpt 5 pro   \n",
       "35                        [/v1/responses]                    GPT-5.1 Codex   \n",
       "36                        [/v1/responses]               GPT-5.1 Codex Mini   \n",
       "37                 [/v1/chat/completions]       Mixtral 8x7B Instruct v0.1   \n",
       "38                 [/v1/chat/completions]     Llama 3.3 70B Instruct Turbo   \n",
       "39                 [/v1/chat/completions]      Llama 3.2 3B Instruct Turbo   \n",
       "\n",
       "     developer                                        description  \\\n",
       "0      Open AI  GPT-4o integrates text, vision, and audio for ...   \n",
       "1      Open AI  Multimodal AI model by OpenAI enhancing human-...   \n",
       "2      Open AI  GPT-4o-2024-05-13 is the initial release versi...   \n",
       "3      Open AI  GPT-4o Mini: Cost-efficient, advanced model fo...   \n",
       "4      Open AI  GPT-4o Mini: Cost-efficient, advanced model fo...   \n",
       "5      Open AI  Multimodal AI model by OpenAI enhancing human-...   \n",
       "6      Open AI  High-speed AI model for instant language proce...   \n",
       "7      Open AI                                                      \n",
       "8      Open AI  Revolutionary AI model for unparalleled natura...   \n",
       "9      Open AI                                                      \n",
       "10     Open AI                                                      \n",
       "11     Open AI                                                      \n",
       "12     Open AI  The newest GPT-3.5 Turbo with improved accurac...   \n",
       "13     Open AI  The GPT-3.5 Turbo 1106 model features enhanced...   \n",
       "14     Open AI  OpenAI o3-mini excels in reasoning tasks with ...   \n",
       "15     Open AI  GPT-4o Audio Preview is OpenAI's latest flagsh...   \n",
       "16     Open AI  GPT-4o Mini Audio adds speech-to-text and text...   \n",
       "17     Open AI  GPT Audio is OpenAI's latest flagship model ca...   \n",
       "18     Open AI  GPT Mini Audio adds speech-to-text and text-to...   \n",
       "19     Open AI  GPT-4o Search Preview blends OpenAI's GPT-4o c...   \n",
       "20     Open AI  GPT-4o Mini Search Preview is a more efficient...   \n",
       "21     Open AI                                                      \n",
       "22     Open AI                                                      \n",
       "23     Open AI                                                      \n",
       "24     Open AI                                                      \n",
       "25     Open AI                                                      \n",
       "26     Open AI  OpenAI o1 excels in complex reasoning tasks wi...   \n",
       "27     Open AI                                                      \n",
       "28     Open AI                                                      \n",
       "29     Open AI                                                      \n",
       "30     Open AI                                                      \n",
       "31     Open AI                                                      \n",
       "32     Open AI                                                      \n",
       "33     Open AI                                                      \n",
       "34     Open AI                                                      \n",
       "35     Open AI                                                      \n",
       "36     Open AI                                                      \n",
       "37  Mistral AI  Harness the power of tailored AI with Mixtral-...   \n",
       "38        Meta  Meta Llama 3.3 70B Instruct Turbo is an advanc...   \n",
       "39        Meta  Efficient multilingual LLM for diverse NLP tas...   \n",
       "\n",
       "    contextLength  maxTokens  \\\n",
       "0        128000.0    16384.0   \n",
       "1        128000.0    16384.0   \n",
       "2        128000.0     4096.0   \n",
       "3        128000.0    16384.0   \n",
       "4        128000.0    16384.0   \n",
       "5        128000.0    16384.0   \n",
       "6        128000.0     4096.0   \n",
       "7        128000.0     4096.0   \n",
       "8          8000.0     8192.0   \n",
       "9          8000.0     4096.0   \n",
       "10         8000.0     4096.0   \n",
       "11        16000.0     4096.0   \n",
       "12        16000.0     4096.0   \n",
       "13        16000.0     4096.0   \n",
       "14       200000.0   100000.0   \n",
       "15       128000.0    16384.0   \n",
       "16       128000.0    16384.0   \n",
       "17       128000.0    16384.0   \n",
       "18       128000.0    16384.0   \n",
       "19       128000.0    16384.0   \n",
       "20       128000.0    16384.0   \n",
       "21      1000000.0    32768.0   \n",
       "22      1000000.0    32768.0   \n",
       "23      1000000.0    32768.0   \n",
       "24       200000.0   100000.0   \n",
       "25       200000.0   100000.0   \n",
       "26       200000.0   100000.0   \n",
       "27       400000.0   128000.0   \n",
       "28       400000.0   128000.0   \n",
       "29       400000.0   128000.0   \n",
       "30       400000.0    16384.0   \n",
       "31       400000.0   128000.0   \n",
       "32       400000.0   128000.0   \n",
       "33       200000.0   100000.0   \n",
       "34       400000.0   272000.0   \n",
       "35       400000.0   128000.0   \n",
       "36       400000.0   128000.0   \n",
       "37        64000.0    32768.0   \n",
       "38       128000.0   127000.0   \n",
       "39       131000.0   130000.0   \n",
       "\n",
       "                                                  url  \\\n",
       "0          https://aimlapi.com/models/chat-gpt-4-omni   \n",
       "1    https://aimlapi.com/models/gpt-4o-2024-08-06-api   \n",
       "2    https://aimlapi.com/models/gpt-4o-2024-05-13-api   \n",
       "3         https://aimlapi.com/models/chat-gpt-4o-mini   \n",
       "4         https://aimlapi.com/models/chat-gpt-4o-mini   \n",
       "5    https://aimlapi.com/models/gpt-4o-2024-08-06-api   \n",
       "6         https://aimlapi.com/models/chat-gpt-4-turbo   \n",
       "7                                                       \n",
       "8               https://aimlapi.com/models/chat-gpt-4   \n",
       "9                                                       \n",
       "10                                                      \n",
       "11                                                      \n",
       "12  https://aimlapi.com/models/chat-gpt-3-5-turbo-...   \n",
       "13  https://aimlapi.com/models/chat-gpt-3-5-turbo-...   \n",
       "14      https://aimlapi.com/models/openai-o3-mini-api   \n",
       "15  https://aimlapi.com/models/gpt-4o-audio-previe...   \n",
       "16   https://aimlapi.com/models/gpt-4o-mini-audio-api   \n",
       "17                                                      \n",
       "18                                                      \n",
       "19  https://aimlapi.com/models/gpt-4o-search-previ...   \n",
       "20  https://aimlapi.com/models/gpt-4o-mini-search-...   \n",
       "21                                                      \n",
       "22                                                      \n",
       "23                                                      \n",
       "24                                                      \n",
       "25                                                      \n",
       "26           https://aimlapi.com/models/openai-o1-api   \n",
       "27                                                      \n",
       "28                                                      \n",
       "29                                                      \n",
       "30                                                      \n",
       "31                                                      \n",
       "32                                                      \n",
       "33                                                      \n",
       "34                                                      \n",
       "35                                                      \n",
       "36                                                      \n",
       "37  https://aimlapi.com/models/mixtral-8x7b-instru...   \n",
       "38  https://aimlapi.com/models/meta-llama-3-3-70b-...   \n",
       "39  https://aimlapi.com/models/llama-3-2-3b-instru...   \n",
       "\n",
       "                                             docs_url  \n",
       "0   https://docs.aimlapi.com/api-references/text-m...  \n",
       "1   https://docs.aimlapi.com/api-references/text-m...  \n",
       "2   https://docs.aimlapi.com/api-references/text-m...  \n",
       "3   https://docs.aimlapi.com/api-references/text-m...  \n",
       "4   https://docs.aimlapi.com/api-references/text-m...  \n",
       "5   https://docs.aimlapi.com/api-references/text-m...  \n",
       "6   https://docs.aimlapi.com/api-references/text-m...  \n",
       "7   https://docs.aimlapi.com/api-references/text-m...  \n",
       "8   https://docs.aimlapi.com/api-references/text-m...  \n",
       "9   https://docs.aimlapi.com/api-references/text-m...  \n",
       "10  https://docs.aimlapi.com/api-references/text-m...  \n",
       "11  https://docs.aimlapi.com/api-references/text-m...  \n",
       "12  https://docs.aimlapi.com/api-references/text-m...  \n",
       "13  https://docs.aimlapi.com/api-references/text-m...  \n",
       "14  https://docs.aimlapi.com/api-references/text-m...  \n",
       "15  https://docs.aimlapi.com/api-references/text-m...  \n",
       "16  https://docs.aimlapi.com/api-references/text-m...  \n",
       "17                                                     \n",
       "18                                                     \n",
       "19  https://docs.aimlapi.com/api-references/text-m...  \n",
       "20  https://docs.aimlapi.com/api-references/text-m...  \n",
       "21  https://docs.aimlapi.com/api-references/text-m...  \n",
       "22  https://docs.aimlapi.com/api-references/text-m...  \n",
       "23  https://docs.aimlapi.com/api-references/text-m...  \n",
       "24  https://docs.aimlapi.com/api-references/text-m...  \n",
       "25  https://docs.aimlapi.com/api-references/text-m...  \n",
       "26  https://docs.aimlapi.com/api-references/text-m...  \n",
       "27  https://docs.aimlapi.com/api-references/text-m...  \n",
       "28  https://docs.aimlapi.com/api-references/text-m...  \n",
       "29  https://docs.aimlapi.com/api-references/text-m...  \n",
       "30  https://docs.aimlapi.com/api-references/text-m...  \n",
       "31                                                     \n",
       "32                                                     \n",
       "33  https://docs.aimlapi.com/api-references/text-m...  \n",
       "34                                                     \n",
       "35                                                     \n",
       "36                                                     \n",
       "37  https://docs.aimlapi.com/api-references/text-m...  \n",
       "38  https://docs.aimlapi.com/api-references/text-m...  \n",
       "39  https://docs.aimlapi.com/api-references/text-m...  "
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_models[:40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "fff20982-12ed-4644-8a28-1160134a57e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Aura is a real-time TTS model with human-like voices for conversational AI applications.'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_models.iloc[220][\"description\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85971702-7b39-47ce-9a77-7737a039e351",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
