{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7e407ea-076c-414c-8576-618a6e778854",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: semanticscholar in /opt/conda/lib/python3.13/site-packages (0.11.0)\n",
      "Requirement already satisfied: tenacity in /opt/conda/lib/python3.13/site-packages (from semanticscholar) (9.1.2)\n",
      "Requirement already satisfied: httpx in /opt/conda/lib/python3.13/site-packages (from semanticscholar) (0.28.1)\n",
      "Requirement already satisfied: nest_asyncio in /opt/conda/lib/python3.13/site-packages (from semanticscholar) (1.6.0)\n",
      "Requirement already satisfied: anyio in /opt/conda/lib/python3.13/site-packages (from httpx->semanticscholar) (4.11.0)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.13/site-packages (from httpx->semanticscholar) (2025.10.5)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.13/site-packages (from httpx->semanticscholar) (1.0.9)\n",
      "Requirement already satisfied: idna in /opt/conda/lib/python3.13/site-packages (from httpx->semanticscholar) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in /opt/conda/lib/python3.13/site-packages (from httpcore==1.*->httpx->semanticscholar) (0.16.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/conda/lib/python3.13/site-packages (from anyio->httpx->semanticscholar) (1.3.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install semanticscholar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ee4ca6ab-350d-404e-b369-43ff9d95aa17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen_agentchat.agents import AssistantAgent\n",
    "from autogen_agentchat.conditions import TextMentionTermination\n",
    "from autogen_agentchat.teams import MagenticOneGroupChat\n",
    "from autogen_agentchat.ui import Console\n",
    "from autogen_core.tools import FunctionTool\n",
    "from autogen_ext.models.openai import OpenAIChatCompletionClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "49d1107e-c0dc-41ce-84d1-064a16460e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from semanticscholar import SemanticScholar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "855de688-6621-4266-a8c5-b95173952a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def s2_search(query: str, max_results: int = 2) -> list:  # type: ignore[type-arg]\n",
    "    \"\"\"\n",
    "    Search Semantic scholar by keywords and return the results including abstracts.\n",
    "    \"\"\"\n",
    "    from semanticscholar import SemanticScholar\n",
    "\n",
    "    sch = SemanticScholar()\n",
    "    \n",
    "    search = sch.search_paper(query=query, limit = max_results)\n",
    "\n",
    "    results = []\n",
    "    for paper in search.items:\n",
    "        try:\n",
    "            results.append(\n",
    "                {\n",
    "                    \"title\": paper.title,\n",
    "                    \"authors\": [author.name for author in paper.authors],\n",
    "                    \"published\": paper.publicationDate.strftime(\"%Y-%m-%d\"),\n",
    "                    \"abstract\": paper.abstract,\n",
    "                    \"pdf_url\": paper.openAccessPdf[\"url\"]\n",
    "                }\n",
    "            )\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    # # Write results to a file\n",
    "    # with open('s2_search_results.json', 'w') as f:\n",
    "    #     json.dump(results, f, indent=2)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bde4ae38-a9cb-46fd-85a6-e7e7b4a3b3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def arxiv_search(query: str, max_results: int = 2) -> list:  # type: ignore[type-arg]\n",
    "    \"\"\"\n",
    "    Search Arxiv for papers and return the results including abstracts.\n",
    "    \"\"\"\n",
    "    import arxiv\n",
    "\n",
    "    client = arxiv.Client(\n",
    "        page_size=max_results,\n",
    "        delay_seconds=5,\n",
    "        num_retries=3\n",
    "    )\n",
    "    search = arxiv.Search(query=query, max_results=max_results, sort_by=arxiv.SortCriterion.Relevance)\n",
    "\n",
    "    results = []\n",
    "    for paper in client.results(search):\n",
    "        results.append(\n",
    "            {\n",
    "                \"title\": paper.title,\n",
    "                \"authors\": [author.name for author in paper.authors],\n",
    "                \"published\": paper.published.strftime(\"%Y-%m-%d\"),\n",
    "                \"abstract\": paper.summary,\n",
    "                \"pdf_url\": paper.pdf_url,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    # # Write results to a file\n",
    "    # with open('arxiv_search_results.json', 'w') as f:\n",
    "    #     json.dump(results, f, indent=2)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "11ec59f6-6e9f-40c2-8df0-84bc08c463f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "s2_search_tool = FunctionTool(\n",
    "    s2_search, description=\"Search Semantic scholar for papers by keywords, returns found papers including abstracts\"\n",
    ")\n",
    "arxiv_search_tool = FunctionTool(\n",
    "    arxiv_search, description=\"Search Arxiv for papers related to a given topic, including abstracts\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7d9b9d65-0a11-45bc-89b8-2aefe06967ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_client = OpenAIChatCompletionClient(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    api_key = \"<YOUR_API_KEY>\"\n",
    ")\n",
    "\n",
    "s2_search_agent = AssistantAgent(\n",
    "    name=\"Semantic_Scholar_Search_Agent\",\n",
    "    tools=[s2_search_tool],\n",
    "    model_client=model_client,\n",
    "    description=\"An agent that can search Semantic scholar paper database using keywords related to given topic\",\n",
    "    system_message=\"You are a helpful AI assistant. Solve tasks using your tools.\",\n",
    ")\n",
    "\n",
    "arxiv_search_agent = AssistantAgent(\n",
    "    name=\"Arxiv_Search_Agent\",\n",
    "    tools=[arxiv_search_tool],\n",
    "    model_client=model_client,\n",
    "    description=\"An agent that can search Arxiv for papers related to a given topic, including abstracts\",\n",
    "    system_message=\"You are a helpful AI assistant. Solve tasks using your tools. Specifically, you can take into consideration the user's request and craft a search query that is most likely to return relevant academi papers.\",\n",
    ")\n",
    "\n",
    "\n",
    "report_agent = AssistantAgent(\n",
    "    name=\"Report_Agent\",\n",
    "    model_client=model_client,\n",
    "    description=\"Generate a report based on a given topic\",\n",
    "    system_message=\"You are a helpful assistant. Your task is to synthesize data extracted into a high quality literature review including CORRECT references. You MUST write a final report that is formatted as a literature review with CORRECT references.  Your response should end with the word 'TERMINATE'\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6295cea2-e498-463e-af43-390cd2234c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "termination = TextMentionTermination(\"TERMINATE\")\n",
    "team = MagenticOneGroupChat(\n",
    "    participants=[s2_search_agent, arxiv_search_agent, report_agent], \n",
    "    termination_condition=termination,\n",
    "    model_client = model_client\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0507dad8-07c6-48e8-9653-b9166a5cf33d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- TextMessage (user) ----------\n",
      "Make a search of 20 recent publications and write a literature review on recent advances in deep learning optimizers with an accent on novel optimization algorythms\n",
      "---------- TextMessage (MagenticOneOrchestrator) ----------\n",
      "\n",
      "We are working to address the following user request:\n",
      "\n",
      "Make a search of 20 recent publications and write a literature review on recent advances in deep learning optimizers with an accent on novel optimization algorythms\n",
      "\n",
      "\n",
      "To answer this request we have assembled the following team:\n",
      "\n",
      "Semantic_Scholar_Search_Agent: An agent that can search Semantic scholar paper database using keywords related to given topic\n",
      "Arxiv_Search_Agent: An agent that can search Arxiv for papers related to a given topic, including abstracts\n",
      "Report_Agent: Generate a report based on a given topic\n",
      "\n",
      "\n",
      "Here is an initial fact sheet to consider:\n",
      "\n",
      "1. **GIVEN OR VERIFIED FACTS**\n",
      "   - The request involves a literature review.\n",
      "   - The focus is on recent advances in deep learning optimizers.\n",
      "   - There is an emphasis on novel optimization algorithms.\n",
      "   - The request specifies searching for 20 recent publications.\n",
      "\n",
      "2. **FACTS TO LOOK UP**\n",
      "   - A list of 20 recent publications on deep learning optimizers, which can typically be found in academic databases such as IEEE Xplore, arXiv, or Google Scholar.\n",
      "   - The specific novel optimization algorithms that have been proposed in recent years, potentially found in research papers or surveys published in academic journals like \"NeurIPS,\" “ICML,” “CVPR,” or “Journal of Machine Learning Research.”\n",
      "\n",
      "3. **FACTS TO DERIVE**\n",
      "   - The effectiveness and performance comparisons of the novel optimizers with existing algorithms, which may require synthesizing data from various publications and experiments.\n",
      "   - General trends in deep learning optimizers over recent years, possibly inferred from the patterns in the literature.\n",
      "\n",
      "4. **EDUCATED GUESSES**\n",
      "   - There will likely be an increase in algorithms that incorporate concepts from meta-learning or reinforcement learning, as these areas have seen significant growth in research.\n",
      "   - Some recent optimizers may integrate ideas from evolutionary algorithms or swarm intelligence, given the push towards hybrid approaches in optimization.\n",
      "   - Novel optimizers may focus on improving convergence times and reducing the need for manual tuning of hyperparameters, reflecting ongoing challenges in the field.\n",
      "\n",
      "\n",
      "Here is the plan to follow as best as possible:\n",
      "\n",
      "- **Step 1: Define Keywords and Search Criteria**\n",
      "  - Determine specific keywords related to deep learning optimizers, focusing on recent advances and novel algorithms. \n",
      "\n",
      "- **Step 2: Conduct Literature Searches**\n",
      "  - **Semantic_Scholar_Search_Agent**: Utilize this agent to search for recent publications on deep learning optimizers using the defined keywords. Gather relevant papers along with their citation counts to identify influential works.\n",
      "  - **Arxiv_Search_Agent**: Use this agent to search for preprint articles and conference papers that discuss recent advances in optimization algorithms for deep learning. Collect abstracts for further evaluation.\n",
      "\n",
      "- **Step 3: Compile List of Publications**\n",
      "  - Compile a list of at least 20 relevant publications from both sources, ensuring a mix of journal articles and conference papers.\n",
      "\n",
      "- **Step 4: Extract Key Information**\n",
      "  - Extract key findings, methodologies, and insights from the collected papers, focusing specifically on novel optimization algorithms and their performance.\n",
      "\n",
      "- **Step 5: Synthesize a Literature Review**\n",
      "  - **Report_Agent**: Use the gathered information to generate a structured literature review, summarizing recent advances in deep learning optimizers and highlighting the significance of novel algorithms.\n",
      "\n",
      "- **Step 6: Edit and Revise**\n",
      "  - Review the generated report for clarity, coherence, and completeness, ensuring it addresses the request comprehensively. \n",
      "\n",
      "- **Step 7: Finalize and Present Findings**\n",
      "  - Compile the final literature review along with references and present the findings in a polished document format.\n",
      "\n",
      "---------- TextMessage (MagenticOneOrchestrator) ----------\n",
      "Please conduct a search for recent publications on deep learning optimizers using relevant keywords and gather a list of at least 20 papers along with their abstracts.\n",
      "---------- ToolCallRequestEvent (Semantic_Scholar_Search_Agent) ----------\n",
      "[FunctionCall(id='call_42WxKbHh3uruuTSBl0GD737Y', arguments='{\"query\": \"deep learning optimizers novel algorithms\", \"max_results\": 10}', name='s2_search'), FunctionCall(id='call_77XQ6y30eJBbAjFKOzKz26v4', arguments='{\"query\": \"novel optimization algorithms deep learning\", \"max_results\": 10}', name='s2_search')]\n",
      "---------- ToolCallExecutionEvent (Semantic_Scholar_Search_Agent) ----------\n",
      "[FunctionExecutionResult(content='[{\\'title\\': \\'Adaptive Friction in Deep Learning: Enhancing Optimizers with Sigmoid and Tanh Function\\', \\'authors\\': [\\'Hongye Zheng\\', \\'Bingxing Wang\\', \\'Minheng Xiao\\', \\'Honglin Qin\\', \\'Zhizhong Wu\\', \\'Lianghao Tan\\'], \\'published\\': \\'2024-07-26\\', \\'abstract\\': \\'Adaptive optimizers are pivotal in guiding the weight updates of deep neural networks, yet they often face challenges such as poor generalization and oscillation issues. To counter these, we introduce sigSignGrad and tanhSignGrad, two novel optimizers that integrate adaptive friction coefficients based on the Sigmoid and Tanh functions, respectively. These algorithms leverage short-term gradient information, a feature overlooked in traditional Adam variants like diffGrad and AngularGrad, to enhance parameter updates and convergence. Our theoretical analysis demonstrates the wide-ranging adjustment capability of the friction coefficient S, which aligns with targeted parameter update strategies and outperforms existing methods in both optimization trajectory smoothness and convergence rate. Extensive experiments on CIFAR-10, CIFAR-100, and Mini-ImageNet datasets using ResNet50 and ViT architectures confirm the superior performance of our proposed optimizers, showcasing improved accuracy and reduced training time. The innovative approach of integrating adaptive friction coefficients as plug-ins into existing optimizers, exemplified by the sigSignAdamW and sigSignAdamP variants, presents a promising strategy for boosting the optimization performance of established algorithms. The findings of this study contribute to the advancement of optimizer design in deep learning.\\', \\'pdf_url\\': \\'\\'}, {\\'title\\': \\'Anomalib: A Deep Learning Library for Anomaly Detection\\', \\'authors\\': [\\'S. Akçay\\', \\'Dick Ameln\\', \\'Ashwin Vaidya\\', \\'B. Lakshmanan\\', \\'Nilesh A. Ahuja\\', \\'Ergin Utku Genc\\'], \\'published\\': \\'2022-02-16\\', \\'abstract\\': \\'This paper introduces anomalib1, a novel library for unsupervised anomaly detection and localization. With reproducibility and modularity in mind, this open-source library provides algorithms from the literature and a set of tools to design custom anomaly detection algorithms via a plug-and-play approach. Anomalib comprises state-of-the-art anomaly detection algorithms that achieve top performance on the benchmarks and that can be used off-the-shelf. In addition, the library provides components to design custom algorithms that could be tailored towards specific needs. Additional tools, including experiment trackers, visualizers, and hyper-parameter optimizers, make it simple to design and implement anomaly detection models. The library also supports OpenVINO model-optimization and quantization for real-time deployment. Overall, anomalib is an extensive library for the design, implementation, and deployment of unsupervised anomaly detection models from data to the edge.\\', \\'pdf_url\\': \\'http://arxiv.org/pdf/2202.08341\\'}, {\\'title\\': \\'Discovering General Reinforcement Learning Algorithms with Adversarial Environment Design\\', \\'authors\\': [\\'Matthew Jackson\\', \\'Minqi Jiang\\', \\'Jack Parker-Holder\\', \\'Risto Vuorio\\', \\'Chris Lu\\', \\'Gregory Farquhar\\', \\'S. Whiteson\\', \\'J. Foerster\\'], \\'published\\': \\'2023-10-04\\', \\'abstract\\': \\'The past decade has seen vast progress in deep reinforcement learning (RL) on the back of algorithms manually designed by human researchers. Recently, it has been shown that it is possible to meta-learn update rules, with the hope of discovering algorithms that can perform well on a wide range of RL tasks. Despite impressive initial results from algorithms such as Learned Policy Gradient (LPG), there remains a generalization gap when these algorithms are applied to unseen environments. In this work, we examine how characteristics of the meta-training distribution impact the generalization performance of these algorithms. Motivated by this analysis and building on ideas from Unsupervised Environment Design (UED), we propose a novel approach for automatically generating curricula to maximize the regret of a meta-learned optimizer, in addition to a novel approximation of regret, which we name algorithmic regret (AR). The result is our method, General RL Optimizers Obtained Via Environment Design (GROOVE). In a series of experiments, we show that GROOVE achieves superior generalization to LPG, and evaluate AR against baseline metrics from UED, identifying it as a critical component of environment design in this setting. We believe this approach is a step towards the discovery of truly general RL algorithms, capable of solving a wide range of real-world environments.\\', \\'pdf_url\\': \\'https://arxiv.org/pdf/2310.02782\\'}, {\\'title\\': \\'A Novel Multistage Transfer Learning for Ultrasound Breast Cancer Image Classification\\', \\'authors\\': [\\'Gelan Ayana\\', \\'Jinhyung Park\\', \\'Jin-Woo Jeong\\', \\'S. Choe\\'], \\'published\\': \\'2022-01-01\\', \\'abstract\\': \\'Breast cancer diagnosis is one of the many areas that has taken advantage of artificial intelligence to achieve better performance, despite the fact that the availability of a large medical image dataset remains a challenge. Transfer learning (TL) is a phenomenon that enables deep learning algorithms to overcome the issue of shortage of training data in constructing an efficient model by transferring knowledge from a given source task to a target task. However, in most cases, ImageNet (natural images) pre-trained models that do not include medical images, are utilized for transfer learning to medical images. Considering the utilization of microscopic cancer cell line images that can be acquired in large amount, we argue that learning from both natural and medical datasets improves performance in ultrasound breast cancer image classification. The proposed multistage transfer learning (MSTL) algorithm was implemented using three pre-trained models: EfficientNetB2, InceptionV3, and ResNet50 with three optimizers: Adam, Adagrad, and stochastic gradient de-scent (SGD). Dataset sizes of 20,400 cancer cell images, 200 ultrasound images from Mendeley and 400 ultrasound images from the MT-Small-Dataset were used. ResNet50-Adagrad-based MSTL achieved a test accuracy of 99 ± 0.612% on the Mendeley dataset and 98.7 ± 1.1% on the MT-Small-Dataset, averaging over 5-fold cross validation. A p-value of 0.01191 was achieved when comparing MSTL against ImageNet based TL for the Mendeley dataset. The result is a significant improvement in the performance of artificial intelligence methods for ultrasound breast cancer classification compared to state-of-the-art methods and could remarkably improve the early diagnosis of breast cancer in young women.\\', \\'pdf_url\\': \\'https://www.mdpi.com/2075-4418/12/1/135/pdf?version=1641807757\\'}, {\\'title\\': \\'Polygonal Unadjusted Langevin Algorithms: Creating stable and efficient adaptive algorithms for neural networks\\', \\'authors\\': [\\'Dong-Young Lim\\', \\'S. Sabanis\\'], \\'published\\': \\'2021-05-28\\', \\'abstract\\': \"We present a new class of Langevin based algorithms, which overcomes many of the known shortcomings of popular adaptive optimizers that are currently used for the fine tuning of deep learning models. Its underpinning theory relies on recent advances of Euler\\'s polygonal approximations for stochastic differential equations (SDEs) with monotone coefficients. As a result, it inherits the stability properties of tamed algorithms, while it addresses other known issues, e.g. vanishing gradients in neural networks. In particular, we provide a nonasymptotic analysis and full theoretical guarantees for the convergence properties of an algorithm of this novel class, which we named TH$\\\\\\\\varepsilon$O POULA (or, simply, TheoPouLa). Finally, several experiments are presented with different types of deep learning models, which show the superior performance of TheoPouLa over many popular adaptive optimization algorithms.\", \\'pdf_url\\': \\'\\'}, {\\'title\\': \\'A Proximal Block Coordinate Descent Algorithm for Deep Neural Network Training\\', \\'authors\\': [\\'Tim Tsz-Kit Lau\\', \\'Jinshan Zeng\\', \\'Baoyuan Wu\\', \\'Y. Yao\\'], \\'published\\': \\'2018-02-12\\', \\'abstract\\': \\'Training deep neural networks (DNNs) efficiently is a challenge due to the associated highly nonconvex optimization. The backpropagation (backprop) algorithm has long been the most widely used algorithm for gradient computation of parameters of DNNs and is used along with gradient descent-type algorithms for this optimization task. Recent work have shown the efficiency of block coordinate descent (BCD) type methods empirically for training DNNs. In view of this, we propose a novel algorithm based on the BCD method for training DNNs and provide its global convergence results built upon the powerful framework of the Kurdyka-Lojasiewicz (KL) property. Numerical experiments on standard datasets demonstrate its competitive efficiency against standard optimizers with backprop.\\', \\'pdf_url\\': \\'\\'}, {\\'title\\': \\'Differentially Private Optimization on Large Model at Small Cost\\', \\'authors\\': [\\'Zhiqi Bu\\', \\'Yu-Xiang Wang\\', \\'Sheng Zha\\', \\'G. Karypis\\'], \\'published\\': \\'2022-09-30\\', \\'abstract\\': \\'Differentially private (DP) optimization is the standard paradigm to learn large neural networks that are accurate and privacy-preserving. The computational cost for DP deep learning, however, is notoriously heavy due to the per-sample gradient clipping. Existing DP implementations are 2-1000X more costly in time and space complexity than the standard (non-private) training. In this work, we develop a novel Book-Keeping (BK) technique that implements existing DP optimizers (thus achieving the same accuracy), with a substantial improvement on the computational cost. Specifically, BK enables DP training on large models and high dimensional data to be roughly as fast and memory-saving as the standard training, whereas previous DP algorithms can be inefficient or incapable of training due to memory error. The computational advantage of BK is supported by the complexity analysis as well as extensive experiments on vision and language tasks. Our implementation achieves state-of-the-art (SOTA) accuracy with very small extra cost: on GPT2 and at almost the same memory cost (<1% overhead), BK has 1.03X the time complexity of the standard training (0.83X training speed in practice), and 0.61X the time complexity of the most efficient DP implementation (1.36X training speed in practice). We open-source the codebase for the BK algorithm at the FastDP library (https://github.com/awslabs/fast-differential-privacy).\\', \\'pdf_url\\': \\'https://arxiv.org/pdf/2210.00038\\'}, {\\'title\\': \\'Enhancing deep learning-based slope stability classification using a novel metaheuristic optimization algorithm for feature selection\\', \\'authors\\': [\\'Bilel Zerouali\\', \\'N. Bailek\\', \\'Aqil Tariq\\', \\'Alban Kuriqi\\', \\'M. Guermoui\\', \\'Amal H. Alharbi\\', \\'D. S. Khafaga\\', \\'El-sayed M. El-kenawy\\'], \\'published\\': \\'2024-09-18\\', \\'abstract\\': \\'The evaluation of slope stability is of crucial importance in geotechnical engineering and has significant implications for infrastructure safety, natural hazard mitigation, and environmental protection. This study aimed to identify the most influential factors affecting slope stability and evaluate the performance of various machine learning models for classifying slope stability. Through correlation analysis and feature importance evaluation using a random forest regressor, cohesion, unit weight, slope height, and friction angle were identified as the most critical parameters influencing slope stability. This research assessed the effectiveness of machine learning techniques combined with modern feature selection algorithms and conventional feature analysis methods. The performance of deep learning models, including recurrent neural networks (RNNs), long short-term memory (LSTM) networks, and generative adversarial networks (GANs), in slope stability classification was evaluated. The GAN model demonstrated superior performance, achieving the highest overall accuracy of 0.913 and the highest area under the ROC curve (AUC) of 0.9285. Integration of the binary bGGO technique for feature selection with the GAN model led to significant improvements in classification performance, with the bGGO-GAN model showing enhanced sensitivity, positive predictive value, negative predictive value, and F1 score compared to the classical GAN model. The bGGO-GAN model achieved 95% accuracy on a substantial dataset of 627 samples, demonstrating competitive performance against other models in the literature while offering strong generalizability. This study highlights the potential of advanced machine learning techniques and feature selection methods for improving slope stability classification and provides valuable insights for geotechnical engineering applications.\\', \\'pdf_url\\': \\'https://doi.org/10.1038/s41598-024-72588-5\\'}, {\\'title\\': \\'A two-phase cuckoo search based approach for gene selection and deep learning classification of cancer disease using gene expression data with a novel fitness function\\', \\'authors\\': [\\'Amol Avinash Joshi\\', \\'R. Aziz\\'], \\'published\\': \\'2024-02-06\\', \\'abstract\\': None, \\'pdf_url\\': \\'\\'}]', name='s2_search', call_id='call_42WxKbHh3uruuTSBl0GD737Y', is_error=False), FunctionExecutionResult(content=\"[{'title': 'Cognizable crime rate prediction and analysis under Indian penal code using deep learning with novel optimization approach', 'authors': ['R. Aziz', 'Aftab Hussain', 'Prajwal Sharma'], 'published': '2023-08-07', 'abstract': None, 'pdf_url': ''}, {'title': 'Design of Metaheuristic Optimization Algorithms for Deep Learning Model for Secure IoT Environment', 'authors': ['Amit Sagu', 'N. S. Gill', 'Preeti Gulia', 'P. Singh', 'Wei‐Chiang Hong'], 'published': '2023-01-25', 'abstract': 'Because of the rise in the number of cyberattacks, the devices that make up the Internet of Things (IoT) environment are experiencing increased levels of security risks. In recent years, a significant number of centralized systems have been developed to identify intrusions into the IoT environment. However, due to diverse requirements of IoT devices such as dispersion, scalability, resource restrictions, and decreased latency, these strategies were unable to achieve notable outcomes. The present paper introduces two novel metaheuristic optimization algorithms for optimizing the weights of deep learning (DL) models, use of DL may help in the detection and prevention of cyberattacks of this nature. Furthermore, two hybrid DL classifiers, i.e., convolutional neural network (CNN) + deep belief network (DBN) and bidirectional long short-term memory (Bi-LSTM) + gated recurrent network (GRU), were designed and tuned using the already proposed optimization algorithms, which results in ads to improved model accuracy. The results are evaluated against the recent approaches in the relevant field along with the hybrid DL classifier. Model performance metrics such as accuracy, rand index, f-measure, and MCC are used to draw conclusions about the model’s validity by employing two distinct datasets. Regarding all performance metrics, the proposed approach outperforms both conventional and cutting-edge methods.', 'pdf_url': 'https://www.mdpi.com/2071-1050/15/3/2204/pdf?version=1675690970'}, {'title': 'Enhancing deep learning-based slope stability classification using a novel metaheuristic optimization algorithm for feature selection', 'authors': ['Bilel Zerouali', 'N. Bailek', 'Aqil Tariq', 'Alban Kuriqi', 'M. Guermoui', 'Amal H. Alharbi', 'D. S. Khafaga', 'El-sayed M. El-kenawy'], 'published': '2024-09-18', 'abstract': 'The evaluation of slope stability is of crucial importance in geotechnical engineering and has significant implications for infrastructure safety, natural hazard mitigation, and environmental protection. This study aimed to identify the most influential factors affecting slope stability and evaluate the performance of various machine learning models for classifying slope stability. Through correlation analysis and feature importance evaluation using a random forest regressor, cohesion, unit weight, slope height, and friction angle were identified as the most critical parameters influencing slope stability. This research assessed the effectiveness of machine learning techniques combined with modern feature selection algorithms and conventional feature analysis methods. The performance of deep learning models, including recurrent neural networks (RNNs), long short-term memory (LSTM) networks, and generative adversarial networks (GANs), in slope stability classification was evaluated. The GAN model demonstrated superior performance, achieving the highest overall accuracy of 0.913 and the highest area under the ROC curve (AUC) of 0.9285. Integration of the binary bGGO technique for feature selection with the GAN model led to significant improvements in classification performance, with the bGGO-GAN model showing enhanced sensitivity, positive predictive value, negative predictive value, and F1 score compared to the classical GAN model. The bGGO-GAN model achieved 95% accuracy on a substantial dataset of 627 samples, demonstrating competitive performance against other models in the literature while offering strong generalizability. This study highlights the potential of advanced machine learning techniques and feature selection methods for improving slope stability classification and provides valuable insights for geotechnical engineering applications.', 'pdf_url': 'https://doi.org/10.1038/s41598-024-72588-5'}, {'title': 'Novel hybrid classifier based on fuzzy type-III decision maker and ensemble deep learning model and improved chaos game optimization', 'authors': ['Nastaran Mehrabi Hashjin', 'Mohammad Hussein Amiri', 'A. Mohammadzadeh', 'S. Mirjalili', 'Nima Khodadadi'], 'published': '2024-05-05', 'abstract': 'This paper presents a unique hybrid classifier that combines deep neural networks with a type-III fuzzy system for decision-making. The ensemble incorporates ResNet-18, Efficient Capsule neural network, ResNet-50, the Histogram of Oriented Gradients (HOG) for feature extraction, neighborhood component analysis (NCA) for feature selection, and Support Vector Machine (SVM) for classification. The innovative inputs fed into the type-III fuzzy system come from the outputs of the mentioned neural networks. The system’s rule parameters are fine-tuned using the Improved Chaos Game Optimization algorithm (ICGO). The conventional CGO’s simple random mutation is substituted with wavelet mutation to enhance the CGO algorithm while preserving non-parametricity and computational complexity. The ICGO was evaluated using 126 benchmark functions and 5 engineering problems, comparing its performance with well-known algorithms. It achieved the best results across all functions except for 2 benchmark functions. The introduced classifier is applied to seven malware datasets and consistently outperforms notable networks like AlexNet, ResNet-18, GoogleNet, and Efficient Capsule neural network in 35 separate runs, achieving over 96% accuracy. Additionally, the classifier’s performance is tested on the MNIST and Fashion-MNIST in 10 separate runs. The results show that the new classifier excels in accuracy, precision, sensitivity, specificity, and F1-score compared to other recent classifiers. Based on the statistical analysis, it has been concluded that the ICGO and propose method exhibit significant superiority compared to the examined algorithms and methods. The source code for ICGO is available publicly at https://nimakhodadadi.com/algorithms-%2B-codes.', 'pdf_url': 'https://link.springer.com/content/pdf/10.1007/s10586-024-04475-7.pdf'}, {'title': 'RNA-Seq analysis for breast cancer detection: a study on paired tissue samples using hybrid optimization and deep learning techniques', 'authors': ['Abrar Yaqoob', 'N. verma', 'R. Aziz', 'Mohd Asif Shah'], 'published': '2024-10-01', 'abstract': 'Breast cancer is a leading global health issue, contributing to high mortality rates among women. The challenge of early detection is exacerbated by the high dimensionality and complexity of gene expression data, which complicates the classification process. This study aims to develop an advanced deep learning model that can accurately detect breast cancer using RNA-Seq gene expression data, while effectively addressing the challenges posed by the data’s high dimensionality and complexity. We introduce a novel hybrid gene selection approach that combines the Harris Hawk Optimization (HHO) and Whale Optimization (WO) algorithms with deep learning to improve feature selection and classification accuracy. The model’s performance was compared to five conventional optimization algorithms integrated with deep learning: Genetic Algorithm (GA), Artificial Bee Colony (ABC), Cuckoo Search (CS), and Particle Swarm Optimization (PSO). RNA-Seq data was collected from 66 paired samples of normal and cancerous tissues from breast cancer patients at the Jawaharlal Nehru Cancer Hospital & Research Centre, Bhopal, India. Sequencing was performed by Biokart Genomics Lab, Bengaluru, India. The proposed model achieved a mean classification accuracy of 99.0%, consistently outperforming the GA, ABC, CS, and PSO methods. The dataset comprised 55 female breast cancer patients, including both early and advanced stages, along with age-matched healthy controls. Our findings demonstrate that the hybrid gene selection approach using HHO and WO, combined with deep learning, is a powerful and accurate tool for breast cancer detection. This approach shows promise for early detection and could facilitate personalized treatment strategies, ultimately improving patient outcomes.', 'pdf_url': 'https://doi.org/10.1007/s00432-024-05968-z'}, {'title': 'A Novel Multi-Objective Binary Chimp Optimization Algorithm for Optimal Feature Selection: Application of Deep-Learning-Based Approaches for SAR Image Classification', 'authors': ['Fatemeh Sadeghi', 'A. Larijani', 'Omid Rostami', 'Diego Martín', 'Parisa Hajirahimi'], 'published': '2023-01-19', 'abstract': 'Removing redundant features and improving classifier performance necessitates the use of meta-heuristic and deep learning (DL) algorithms in feature selection and classification problems. With the maturity of DL tools, many data-driven polarimetric synthetic aperture radar (POLSAR) representation models have been suggested, most of which are based on deep convolutional neural networks (DCNNs). In this paper, we propose a hybrid approach of a new multi-objective binary chimp optimization algorithm (MOBChOA) and DCNN for optimal feature selection. We implemented the proposed method to classify POLSAR images from San Francisco, USA. To do so, we first performed the necessary preprocessing, including speckle reduction, radiometric calibration, and feature extraction. After that, we implemented the proposed MOBChOA for optimal feature selection. Finally, we trained the fully connected DCNN to classify the pixels into specific land-cover labels. We evaluated the performance of the proposed MOBChOA-DCNN in comparison with nine competitive methods. Our experimental results with the POLSAR image datasets show that the proposed architecture had a great performance for different important optimization parameters. The proposed MOBChOA-DCNN provided fewer features (27) and the highest overall accuracy. The overall accuracy values of MOBChOA-DCNN on the training and validation datasets were 96.89% and 96.13%, respectively, which were the best results. The overall accuracy of SVM was 89.30%, which was the worst result. The results of the proposed MOBChOA on two real-world benchmark problems were also better than the results with the other methods. Furthermore, it was shown that the MOBChOA-DCNN performed better than methods from previous studies.', 'pdf_url': 'https://www.mdpi.com/1424-8220/23/3/1180/pdf?version=1675245862'}, {'title': 'A two-phase cuckoo search based approach for gene selection and deep learning classification of cancer disease using gene expression data with a novel fitness function', 'authors': ['Amol Avinash Joshi', 'R. Aziz'], 'published': '2024-02-06', 'abstract': None, 'pdf_url': ''}, {'title': 'Prediction of gestational diabetes using deep learning and Bayesian optimization and traditional machine learning techniques', 'authors': ['B. Kurt', 'Beril Gürlek', 'S. Keskin', 'Sinem Özdemir', 'Özlem Karadeniz', 'İlknur Buçan Kırkbir', 'Tuğba Kurt', 'Serbülent Ünsal', 'C. Kart', 'Neslihan Baki', 'K. Turhan'], 'published': '2023-02-27', 'abstract': 'The study aimed to develop a clinical diagnosis system to identify patients in the GD risk group and reduce unnecessary oral glucose tolerance test (OGTT) applications for pregnant women who are not in the GD risk group using deep learning algorithms. With this aim, a prospective study was designed and the data was taken from 489 patients between the years 2019 and 2021, and informed consent was obtained. The clinical decision support system for the diagnosis of GD was developed using the generated dataset with deep learning algorithms and Bayesian optimization. As a result, a novel successful decision support model was developed using RNN-LSTM with Bayesian optimization that gave 95% sensitivity and 99% specificity on the dataset for the diagnosis of patients in the GD risk group by obtaining 98% AUC (95% CI (0.95–1.00) and p \\\\u2009<\\\\u2009 0.001 ). Thus, with the clinical diagnosis system developed to assist physicians, it is planned to save both cost and time, and reduce possible adverse effects by preventing unnecessary OGTT for patients who are not in the GD risk group. Graphical Abstract', 'pdf_url': 'https://link.springer.com/content/pdf/10.1007/s11517-023-02800-7.pdf'}, {'title': 'Deep Learning for Logic Optimization Algorithms', 'authors': ['Winston Haaswijk', 'Edo Collins', 'Benoit Seguin', 'Mathias Soeken', 'F. Kaplan', 'S. Süsstrunk', 'G. Micheli'], 'published': '2018-05-27', 'abstract': None, 'pdf_url': ''}]\", name='s2_search', call_id='call_77XQ6y30eJBbAjFKOzKz26v4', is_error=False)]\n",
      "---------- ToolCallSummaryMessage (Semantic_Scholar_Search_Agent) ----------\n",
      "[{'title': 'Adaptive Friction in Deep Learning: Enhancing Optimizers with Sigmoid and Tanh Function', 'authors': ['Hongye Zheng', 'Bingxing Wang', 'Minheng Xiao', 'Honglin Qin', 'Zhizhong Wu', 'Lianghao Tan'], 'published': '2024-07-26', 'abstract': 'Adaptive optimizers are pivotal in guiding the weight updates of deep neural networks, yet they often face challenges such as poor generalization and oscillation issues. To counter these, we introduce sigSignGrad and tanhSignGrad, two novel optimizers that integrate adaptive friction coefficients based on the Sigmoid and Tanh functions, respectively. These algorithms leverage short-term gradient information, a feature overlooked in traditional Adam variants like diffGrad and AngularGrad, to enhance parameter updates and convergence. Our theoretical analysis demonstrates the wide-ranging adjustment capability of the friction coefficient S, which aligns with targeted parameter update strategies and outperforms existing methods in both optimization trajectory smoothness and convergence rate. Extensive experiments on CIFAR-10, CIFAR-100, and Mini-ImageNet datasets using ResNet50 and ViT architectures confirm the superior performance of our proposed optimizers, showcasing improved accuracy and reduced training time. The innovative approach of integrating adaptive friction coefficients as plug-ins into existing optimizers, exemplified by the sigSignAdamW and sigSignAdamP variants, presents a promising strategy for boosting the optimization performance of established algorithms. The findings of this study contribute to the advancement of optimizer design in deep learning.', 'pdf_url': ''}, {'title': 'Anomalib: A Deep Learning Library for Anomaly Detection', 'authors': ['S. Akçay', 'Dick Ameln', 'Ashwin Vaidya', 'B. Lakshmanan', 'Nilesh A. Ahuja', 'Ergin Utku Genc'], 'published': '2022-02-16', 'abstract': 'This paper introduces anomalib1, a novel library for unsupervised anomaly detection and localization. With reproducibility and modularity in mind, this open-source library provides algorithms from the literature and a set of tools to design custom anomaly detection algorithms via a plug-and-play approach. Anomalib comprises state-of-the-art anomaly detection algorithms that achieve top performance on the benchmarks and that can be used off-the-shelf. In addition, the library provides components to design custom algorithms that could be tailored towards specific needs. Additional tools, including experiment trackers, visualizers, and hyper-parameter optimizers, make it simple to design and implement anomaly detection models. The library also supports OpenVINO model-optimization and quantization for real-time deployment. Overall, anomalib is an extensive library for the design, implementation, and deployment of unsupervised anomaly detection models from data to the edge.', 'pdf_url': 'http://arxiv.org/pdf/2202.08341'}, {'title': 'Discovering General Reinforcement Learning Algorithms with Adversarial Environment Design', 'authors': ['Matthew Jackson', 'Minqi Jiang', 'Jack Parker-Holder', 'Risto Vuorio', 'Chris Lu', 'Gregory Farquhar', 'S. Whiteson', 'J. Foerster'], 'published': '2023-10-04', 'abstract': 'The past decade has seen vast progress in deep reinforcement learning (RL) on the back of algorithms manually designed by human researchers. Recently, it has been shown that it is possible to meta-learn update rules, with the hope of discovering algorithms that can perform well on a wide range of RL tasks. Despite impressive initial results from algorithms such as Learned Policy Gradient (LPG), there remains a generalization gap when these algorithms are applied to unseen environments. In this work, we examine how characteristics of the meta-training distribution impact the generalization performance of these algorithms. Motivated by this analysis and building on ideas from Unsupervised Environment Design (UED), we propose a novel approach for automatically generating curricula to maximize the regret of a meta-learned optimizer, in addition to a novel approximation of regret, which we name algorithmic regret (AR). The result is our method, General RL Optimizers Obtained Via Environment Design (GROOVE). In a series of experiments, we show that GROOVE achieves superior generalization to LPG, and evaluate AR against baseline metrics from UED, identifying it as a critical component of environment design in this setting. We believe this approach is a step towards the discovery of truly general RL algorithms, capable of solving a wide range of real-world environments.', 'pdf_url': 'https://arxiv.org/pdf/2310.02782'}, {'title': 'A Novel Multistage Transfer Learning for Ultrasound Breast Cancer Image Classification', 'authors': ['Gelan Ayana', 'Jinhyung Park', 'Jin-Woo Jeong', 'S. Choe'], 'published': '2022-01-01', 'abstract': 'Breast cancer diagnosis is one of the many areas that has taken advantage of artificial intelligence to achieve better performance, despite the fact that the availability of a large medical image dataset remains a challenge. Transfer learning (TL) is a phenomenon that enables deep learning algorithms to overcome the issue of shortage of training data in constructing an efficient model by transferring knowledge from a given source task to a target task. However, in most cases, ImageNet (natural images) pre-trained models that do not include medical images, are utilized for transfer learning to medical images. Considering the utilization of microscopic cancer cell line images that can be acquired in large amount, we argue that learning from both natural and medical datasets improves performance in ultrasound breast cancer image classification. The proposed multistage transfer learning (MSTL) algorithm was implemented using three pre-trained models: EfficientNetB2, InceptionV3, and ResNet50 with three optimizers: Adam, Adagrad, and stochastic gradient de-scent (SGD). Dataset sizes of 20,400 cancer cell images, 200 ultrasound images from Mendeley and 400 ultrasound images from the MT-Small-Dataset were used. ResNet50-Adagrad-based MSTL achieved a test accuracy of 99 ± 0.612% on the Mendeley dataset and 98.7 ± 1.1% on the MT-Small-Dataset, averaging over 5-fold cross validation. A p-value of 0.01191 was achieved when comparing MSTL against ImageNet based TL for the Mendeley dataset. The result is a significant improvement in the performance of artificial intelligence methods for ultrasound breast cancer classification compared to state-of-the-art methods and could remarkably improve the early diagnosis of breast cancer in young women.', 'pdf_url': 'https://www.mdpi.com/2075-4418/12/1/135/pdf?version=1641807757'}, {'title': 'Polygonal Unadjusted Langevin Algorithms: Creating stable and efficient adaptive algorithms for neural networks', 'authors': ['Dong-Young Lim', 'S. Sabanis'], 'published': '2021-05-28', 'abstract': \"We present a new class of Langevin based algorithms, which overcomes many of the known shortcomings of popular adaptive optimizers that are currently used for the fine tuning of deep learning models. Its underpinning theory relies on recent advances of Euler's polygonal approximations for stochastic differential equations (SDEs) with monotone coefficients. As a result, it inherits the stability properties of tamed algorithms, while it addresses other known issues, e.g. vanishing gradients in neural networks. In particular, we provide a nonasymptotic analysis and full theoretical guarantees for the convergence properties of an algorithm of this novel class, which we named TH$\\\\varepsilon$O POULA (or, simply, TheoPouLa). Finally, several experiments are presented with different types of deep learning models, which show the superior performance of TheoPouLa over many popular adaptive optimization algorithms.\", 'pdf_url': ''}, {'title': 'A Proximal Block Coordinate Descent Algorithm for Deep Neural Network Training', 'authors': ['Tim Tsz-Kit Lau', 'Jinshan Zeng', 'Baoyuan Wu', 'Y. Yao'], 'published': '2018-02-12', 'abstract': 'Training deep neural networks (DNNs) efficiently is a challenge due to the associated highly nonconvex optimization. The backpropagation (backprop) algorithm has long been the most widely used algorithm for gradient computation of parameters of DNNs and is used along with gradient descent-type algorithms for this optimization task. Recent work have shown the efficiency of block coordinate descent (BCD) type methods empirically for training DNNs. In view of this, we propose a novel algorithm based on the BCD method for training DNNs and provide its global convergence results built upon the powerful framework of the Kurdyka-Lojasiewicz (KL) property. Numerical experiments on standard datasets demonstrate its competitive efficiency against standard optimizers with backprop.', 'pdf_url': ''}, {'title': 'Differentially Private Optimization on Large Model at Small Cost', 'authors': ['Zhiqi Bu', 'Yu-Xiang Wang', 'Sheng Zha', 'G. Karypis'], 'published': '2022-09-30', 'abstract': 'Differentially private (DP) optimization is the standard paradigm to learn large neural networks that are accurate and privacy-preserving. The computational cost for DP deep learning, however, is notoriously heavy due to the per-sample gradient clipping. Existing DP implementations are 2-1000X more costly in time and space complexity than the standard (non-private) training. In this work, we develop a novel Book-Keeping (BK) technique that implements existing DP optimizers (thus achieving the same accuracy), with a substantial improvement on the computational cost. Specifically, BK enables DP training on large models and high dimensional data to be roughly as fast and memory-saving as the standard training, whereas previous DP algorithms can be inefficient or incapable of training due to memory error. The computational advantage of BK is supported by the complexity analysis as well as extensive experiments on vision and language tasks. Our implementation achieves state-of-the-art (SOTA) accuracy with very small extra cost: on GPT2 and at almost the same memory cost (<1% overhead), BK has 1.03X the time complexity of the standard training (0.83X training speed in practice), and 0.61X the time complexity of the most efficient DP implementation (1.36X training speed in practice). We open-source the codebase for the BK algorithm at the FastDP library (https://github.com/awslabs/fast-differential-privacy).', 'pdf_url': 'https://arxiv.org/pdf/2210.00038'}, {'title': 'Enhancing deep learning-based slope stability classification using a novel metaheuristic optimization algorithm for feature selection', 'authors': ['Bilel Zerouali', 'N. Bailek', 'Aqil Tariq', 'Alban Kuriqi', 'M. Guermoui', 'Amal H. Alharbi', 'D. S. Khafaga', 'El-sayed M. El-kenawy'], 'published': '2024-09-18', 'abstract': 'The evaluation of slope stability is of crucial importance in geotechnical engineering and has significant implications for infrastructure safety, natural hazard mitigation, and environmental protection. This study aimed to identify the most influential factors affecting slope stability and evaluate the performance of various machine learning models for classifying slope stability. Through correlation analysis and feature importance evaluation using a random forest regressor, cohesion, unit weight, slope height, and friction angle were identified as the most critical parameters influencing slope stability. This research assessed the effectiveness of machine learning techniques combined with modern feature selection algorithms and conventional feature analysis methods. The performance of deep learning models, including recurrent neural networks (RNNs), long short-term memory (LSTM) networks, and generative adversarial networks (GANs), in slope stability classification was evaluated. The GAN model demonstrated superior performance, achieving the highest overall accuracy of 0.913 and the highest area under the ROC curve (AUC) of 0.9285. Integration of the binary bGGO technique for feature selection with the GAN model led to significant improvements in classification performance, with the bGGO-GAN model showing enhanced sensitivity, positive predictive value, negative predictive value, and F1 score compared to the classical GAN model. The bGGO-GAN model achieved 95% accuracy on a substantial dataset of 627 samples, demonstrating competitive performance against other models in the literature while offering strong generalizability. This study highlights the potential of advanced machine learning techniques and feature selection methods for improving slope stability classification and provides valuable insights for geotechnical engineering applications.', 'pdf_url': 'https://doi.org/10.1038/s41598-024-72588-5'}, {'title': 'A two-phase cuckoo search based approach for gene selection and deep learning classification of cancer disease using gene expression data with a novel fitness function', 'authors': ['Amol Avinash Joshi', 'R. Aziz'], 'published': '2024-02-06', 'abstract': None, 'pdf_url': ''}]\n",
      "[{'title': 'Cognizable crime rate prediction and analysis under Indian penal code using deep learning with novel optimization approach', 'authors': ['R. Aziz', 'Aftab Hussain', 'Prajwal Sharma'], 'published': '2023-08-07', 'abstract': None, 'pdf_url': ''}, {'title': 'Design of Metaheuristic Optimization Algorithms for Deep Learning Model for Secure IoT Environment', 'authors': ['Amit Sagu', 'N. S. Gill', 'Preeti Gulia', 'P. Singh', 'Wei‐Chiang Hong'], 'published': '2023-01-25', 'abstract': 'Because of the rise in the number of cyberattacks, the devices that make up the Internet of Things (IoT) environment are experiencing increased levels of security risks. In recent years, a significant number of centralized systems have been developed to identify intrusions into the IoT environment. However, due to diverse requirements of IoT devices such as dispersion, scalability, resource restrictions, and decreased latency, these strategies were unable to achieve notable outcomes. The present paper introduces two novel metaheuristic optimization algorithms for optimizing the weights of deep learning (DL) models, use of DL may help in the detection and prevention of cyberattacks of this nature. Furthermore, two hybrid DL classifiers, i.e., convolutional neural network (CNN) + deep belief network (DBN) and bidirectional long short-term memory (Bi-LSTM) + gated recurrent network (GRU), were designed and tuned using the already proposed optimization algorithms, which results in ads to improved model accuracy. The results are evaluated against the recent approaches in the relevant field along with the hybrid DL classifier. Model performance metrics such as accuracy, rand index, f-measure, and MCC are used to draw conclusions about the model’s validity by employing two distinct datasets. Regarding all performance metrics, the proposed approach outperforms both conventional and cutting-edge methods.', 'pdf_url': 'https://www.mdpi.com/2071-1050/15/3/2204/pdf?version=1675690970'}, {'title': 'Enhancing deep learning-based slope stability classification using a novel metaheuristic optimization algorithm for feature selection', 'authors': ['Bilel Zerouali', 'N. Bailek', 'Aqil Tariq', 'Alban Kuriqi', 'M. Guermoui', 'Amal H. Alharbi', 'D. S. Khafaga', 'El-sayed M. El-kenawy'], 'published': '2024-09-18', 'abstract': 'The evaluation of slope stability is of crucial importance in geotechnical engineering and has significant implications for infrastructure safety, natural hazard mitigation, and environmental protection. This study aimed to identify the most influential factors affecting slope stability and evaluate the performance of various machine learning models for classifying slope stability. Through correlation analysis and feature importance evaluation using a random forest regressor, cohesion, unit weight, slope height, and friction angle were identified as the most critical parameters influencing slope stability. This research assessed the effectiveness of machine learning techniques combined with modern feature selection algorithms and conventional feature analysis methods. The performance of deep learning models, including recurrent neural networks (RNNs), long short-term memory (LSTM) networks, and generative adversarial networks (GANs), in slope stability classification was evaluated. The GAN model demonstrated superior performance, achieving the highest overall accuracy of 0.913 and the highest area under the ROC curve (AUC) of 0.9285. Integration of the binary bGGO technique for feature selection with the GAN model led to significant improvements in classification performance, with the bGGO-GAN model showing enhanced sensitivity, positive predictive value, negative predictive value, and F1 score compared to the classical GAN model. The bGGO-GAN model achieved 95% accuracy on a substantial dataset of 627 samples, demonstrating competitive performance against other models in the literature while offering strong generalizability. This study highlights the potential of advanced machine learning techniques and feature selection methods for improving slope stability classification and provides valuable insights for geotechnical engineering applications.', 'pdf_url': 'https://doi.org/10.1038/s41598-024-72588-5'}, {'title': 'Novel hybrid classifier based on fuzzy type-III decision maker and ensemble deep learning model and improved chaos game optimization', 'authors': ['Nastaran Mehrabi Hashjin', 'Mohammad Hussein Amiri', 'A. Mohammadzadeh', 'S. Mirjalili', 'Nima Khodadadi'], 'published': '2024-05-05', 'abstract': 'This paper presents a unique hybrid classifier that combines deep neural networks with a type-III fuzzy system for decision-making. The ensemble incorporates ResNet-18, Efficient Capsule neural network, ResNet-50, the Histogram of Oriented Gradients (HOG) for feature extraction, neighborhood component analysis (NCA) for feature selection, and Support Vector Machine (SVM) for classification. The innovative inputs fed into the type-III fuzzy system come from the outputs of the mentioned neural networks. The system’s rule parameters are fine-tuned using the Improved Chaos Game Optimization algorithm (ICGO). The conventional CGO’s simple random mutation is substituted with wavelet mutation to enhance the CGO algorithm while preserving non-parametricity and computational complexity. The ICGO was evaluated using 126 benchmark functions and 5 engineering problems, comparing its performance with well-known algorithms. It achieved the best results across all functions except for 2 benchmark functions. The introduced classifier is applied to seven malware datasets and consistently outperforms notable networks like AlexNet, ResNet-18, GoogleNet, and Efficient Capsule neural network in 35 separate runs, achieving over 96% accuracy. Additionally, the classifier’s performance is tested on the MNIST and Fashion-MNIST in 10 separate runs. The results show that the new classifier excels in accuracy, precision, sensitivity, specificity, and F1-score compared to other recent classifiers. Based on the statistical analysis, it has been concluded that the ICGO and propose method exhibit significant superiority compared to the examined algorithms and methods. The source code for ICGO is available publicly at https://nimakhodadadi.com/algorithms-%2B-codes.', 'pdf_url': 'https://link.springer.com/content/pdf/10.1007/s10586-024-04475-7.pdf'}, {'title': 'RNA-Seq analysis for breast cancer detection: a study on paired tissue samples using hybrid optimization and deep learning techniques', 'authors': ['Abrar Yaqoob', 'N. verma', 'R. Aziz', 'Mohd Asif Shah'], 'published': '2024-10-01', 'abstract': 'Breast cancer is a leading global health issue, contributing to high mortality rates among women. The challenge of early detection is exacerbated by the high dimensionality and complexity of gene expression data, which complicates the classification process. This study aims to develop an advanced deep learning model that can accurately detect breast cancer using RNA-Seq gene expression data, while effectively addressing the challenges posed by the data’s high dimensionality and complexity. We introduce a novel hybrid gene selection approach that combines the Harris Hawk Optimization (HHO) and Whale Optimization (WO) algorithms with deep learning to improve feature selection and classification accuracy. The model’s performance was compared to five conventional optimization algorithms integrated with deep learning: Genetic Algorithm (GA), Artificial Bee Colony (ABC), Cuckoo Search (CS), and Particle Swarm Optimization (PSO). RNA-Seq data was collected from 66 paired samples of normal and cancerous tissues from breast cancer patients at the Jawaharlal Nehru Cancer Hospital & Research Centre, Bhopal, India. Sequencing was performed by Biokart Genomics Lab, Bengaluru, India. The proposed model achieved a mean classification accuracy of 99.0%, consistently outperforming the GA, ABC, CS, and PSO methods. The dataset comprised 55 female breast cancer patients, including both early and advanced stages, along with age-matched healthy controls. Our findings demonstrate that the hybrid gene selection approach using HHO and WO, combined with deep learning, is a powerful and accurate tool for breast cancer detection. This approach shows promise for early detection and could facilitate personalized treatment strategies, ultimately improving patient outcomes.', 'pdf_url': 'https://doi.org/10.1007/s00432-024-05968-z'}, {'title': 'A Novel Multi-Objective Binary Chimp Optimization Algorithm for Optimal Feature Selection: Application of Deep-Learning-Based Approaches for SAR Image Classification', 'authors': ['Fatemeh Sadeghi', 'A. Larijani', 'Omid Rostami', 'Diego Martín', 'Parisa Hajirahimi'], 'published': '2023-01-19', 'abstract': 'Removing redundant features and improving classifier performance necessitates the use of meta-heuristic and deep learning (DL) algorithms in feature selection and classification problems. With the maturity of DL tools, many data-driven polarimetric synthetic aperture radar (POLSAR) representation models have been suggested, most of which are based on deep convolutional neural networks (DCNNs). In this paper, we propose a hybrid approach of a new multi-objective binary chimp optimization algorithm (MOBChOA) and DCNN for optimal feature selection. We implemented the proposed method to classify POLSAR images from San Francisco, USA. To do so, we first performed the necessary preprocessing, including speckle reduction, radiometric calibration, and feature extraction. After that, we implemented the proposed MOBChOA for optimal feature selection. Finally, we trained the fully connected DCNN to classify the pixels into specific land-cover labels. We evaluated the performance of the proposed MOBChOA-DCNN in comparison with nine competitive methods. Our experimental results with the POLSAR image datasets show that the proposed architecture had a great performance for different important optimization parameters. The proposed MOBChOA-DCNN provided fewer features (27) and the highest overall accuracy. The overall accuracy values of MOBChOA-DCNN on the training and validation datasets were 96.89% and 96.13%, respectively, which were the best results. The overall accuracy of SVM was 89.30%, which was the worst result. The results of the proposed MOBChOA on two real-world benchmark problems were also better than the results with the other methods. Furthermore, it was shown that the MOBChOA-DCNN performed better than methods from previous studies.', 'pdf_url': 'https://www.mdpi.com/1424-8220/23/3/1180/pdf?version=1675245862'}, {'title': 'A two-phase cuckoo search based approach for gene selection and deep learning classification of cancer disease using gene expression data with a novel fitness function', 'authors': ['Amol Avinash Joshi', 'R. Aziz'], 'published': '2024-02-06', 'abstract': None, 'pdf_url': ''}, {'title': 'Prediction of gestational diabetes using deep learning and Bayesian optimization and traditional machine learning techniques', 'authors': ['B. Kurt', 'Beril Gürlek', 'S. Keskin', 'Sinem Özdemir', 'Özlem Karadeniz', 'İlknur Buçan Kırkbir', 'Tuğba Kurt', 'Serbülent Ünsal', 'C. Kart', 'Neslihan Baki', 'K. Turhan'], 'published': '2023-02-27', 'abstract': 'The study aimed to develop a clinical diagnosis system to identify patients in the GD risk group and reduce unnecessary oral glucose tolerance test (OGTT) applications for pregnant women who are not in the GD risk group using deep learning algorithms. With this aim, a prospective study was designed and the data was taken from 489 patients between the years 2019 and 2021, and informed consent was obtained. The clinical decision support system for the diagnosis of GD was developed using the generated dataset with deep learning algorithms and Bayesian optimization. As a result, a novel successful decision support model was developed using RNN-LSTM with Bayesian optimization that gave 95% sensitivity and 99% specificity on the dataset for the diagnosis of patients in the GD risk group by obtaining 98% AUC (95% CI (0.95–1.00) and p \\u2009<\\u2009 0.001 ). Thus, with the clinical diagnosis system developed to assist physicians, it is planned to save both cost and time, and reduce possible adverse effects by preventing unnecessary OGTT for patients who are not in the GD risk group. Graphical Abstract', 'pdf_url': 'https://link.springer.com/content/pdf/10.1007/s11517-023-02800-7.pdf'}, {'title': 'Deep Learning for Logic Optimization Algorithms', 'authors': ['Winston Haaswijk', 'Edo Collins', 'Benoit Seguin', 'Mathias Soeken', 'F. Kaplan', 'S. Süsstrunk', 'G. Micheli'], 'published': '2018-05-27', 'abstract': None, 'pdf_url': ''}]\n",
      "---------- TextMessage (MagenticOneOrchestrator) ----------\n",
      "Please search Arxiv for more recent publications on deep learning optimizers, especially focusing on novel optimization algorithms.\n",
      "---------- ToolCallRequestEvent (Arxiv_Search_Agent) ----------\n",
      "[FunctionCall(id='call_QsLs991S6DO9EpyWE8bVuCLz', arguments='{\"query\":\"deep learning optimizers novel optimization algorithms\",\"max_results\":20}', name='arxiv_search')]\n",
      "---------- ToolCallExecutionEvent (Arxiv_Search_Agent) ----------\n",
      "[FunctionExecutionResult(content='[{\\'title\\': \\'Iterative Surrogate Model Optimization (ISMO): An active learning algorithm for PDE constrained optimization with deep neural networks\\', \\'authors\\': [\\'Kjetil O. Lye\\', \\'Siddhartha Mishra\\', \\'Deep Ray\\', \\'Praveen Chandrasekhar\\'], \\'published\\': \\'2020-08-13\\', \\'abstract\\': \\'We present a novel active learning algorithm, termed as iterative surrogate model optimization (ISMO), for robust and efficient numerical approximation of PDE constrained optimization problems. This algorithm is based on deep neural networks and its key feature is the iterative selection of training data through a feedback loop between deep neural networks and any underlying standard optimization algorithm. Under suitable hypotheses, we show that the resulting optimizers converge exponentially fast (and with exponentially decaying variance), with respect to increasing number of training samples. Numerical examples for optimal control, parameter identification and shape optimization problems for PDEs are provided to validate the proposed theory and to illustrate that ISMO significantly outperforms a standard deep neural network based surrogate optimization algorithm.\\', \\'pdf_url\\': None}, {\\'title\\': \\'The Modern Mathematics of Deep Learning\\', \\'authors\\': [\\'Julius Berner\\', \\'Philipp Grohs\\', \\'Gitta Kutyniok\\', \\'Philipp Petersen\\'], \\'published\\': \\'2021-05-09\\', \\'abstract\\': \\'We describe the new field of mathematical analysis of deep learning. This field emerged around a list of research questions that were not answered within the classical framework of learning theory. These questions concern: the outstanding generalization power of overparametrized neural networks, the role of depth in deep architectures, the apparent absence of the curse of dimensionality, the surprisingly successful optimization performance despite the non-convexity of the problem, understanding what features are learned, why deep architectures perform exceptionally well in physical problems, and which fine aspects of an architecture affect the behavior of a learning task in which way. We present an overview of modern approaches that yield partial answers to these questions. For selected approaches, we describe the main ideas in more detail.\\', \\'pdf_url\\': None}, {\\'title\\': \\'Learn to Accumulate Evidence from All Training Samples: Theory and Practice\\', \\'authors\\': [\\'Deep Pandey\\', \\'Qi Yu\\'], \\'published\\': \\'2023-06-19\\', \\'abstract\\': \\'Evidential deep learning, built upon belief theory and subjective logic, offers a principled and computationally efficient way to turn a deterministic neural network uncertainty-aware. The resultant evidential models can quantify fine-grained uncertainty using the learned evidence. To ensure theoretically sound evidential models, the evidence needs to be non-negative, which requires special activation functions for model training and inference. This constraint often leads to inferior predictive performance compared to standard softmax models, making it challenging to extend them to many large-scale datasets. To unveil the real cause of this undesired behavior, we theoretically investigate evidential models and identify a fundamental limitation that explains the inferior performance: existing evidential activation functions create zero evidence regions, which prevent the model to learn from training samples falling into such regions. A deeper analysis of evidential activation functions based on our theoretical underpinning inspires the design of a novel regularizer that effectively alleviates this fundamental limitation. Extensive experiments over many challenging real-world datasets and settings confirm our theoretical findings and demonstrate the effectiveness of our proposed approach.\\', \\'pdf_url\\': None}, {\\'title\\': \\'Towards Practical Second-Order Optimizers in Deep Learning: Insights from Fisher Information Analysis\\', \\'authors\\': [\\'Damien Martins Gomes\\'], \\'published\\': \\'2025-04-26\\', \\'abstract\\': \\'First-order optimization methods remain the standard for training deep neural networks (DNNs). Optimizers like Adam incorporate limited curvature information by preconditioning the stochastic gradient with a diagonal matrix. Despite the widespread adoption of first-order methods, second-order optimization algorithms often exhibit superior convergence compared to methods like Adam and SGD. However, their practicality in training DNNs is still limited by a significantly higher per-iteration computational cost compared to first-order methods. In this thesis, we present AdaFisher, a novel adaptive second-order optimizer that leverages a diagonal block-Kronecker approximation of the Fisher information matrix to adaptively precondition gradients. AdaFisher aims to bridge the gap between the improved convergence and generalization of second-order methods and the computational efficiency needed for training DNNs. Despite the traditionally slower speed of second-order optimizers, AdaFisher is effective for tasks such as image classification and language modeling, exhibiting remarkable stability and robustness during hyperparameter tuning. We demonstrate that AdaFisher outperforms state-of-the-art optimizers in both accuracy and convergence speed. The code is available from https://github.com/AtlasAnalyticsLab/AdaFisher.\\', \\'pdf_url\\': None}, {\\'title\\': \\'Deep Learning and Computational Physics (Lecture Notes)\\', \\'authors\\': [\\'Deep Ray\\', \\'Orazio Pinti\\', \\'Assad A. Oberai\\'], \\'published\\': \\'2023-01-03\\', \\'abstract\\': \\'These notes were compiled as lecture notes for a course developed and taught at the University of the Southern California. They should be accessible to a typical engineering graduate student with a strong background in Applied Mathematics.\\\\n  The main objective of these notes is to introduce a student who is familiar with concepts in linear algebra and partial differential equations to select topics in deep learning. These lecture notes exploit the strong connections between deep learning algorithms and the more conventional techniques of computational physics to achieve two goals. First, they use concepts from computational physics to develop an understanding of deep learning algorithms. Not surprisingly, many concepts in deep learning can be connected to similar concepts in computational physics, and one can utilize this connection to better understand these algorithms. Second, several novel deep learning algorithms can be used to solve challenging problems in computational physics. Thus, they offer someone who is interested in modeling a physical phenomena with a complementary set of tools.\\', \\'pdf_url\\': None}, {\\'title\\': \\'On Hyperparameter Optimization of Machine Learning Algorithms: Theory and Practice\\', \\'authors\\': [\\'Li Yang\\', \\'Abdallah Shami\\'], \\'published\\': \\'2020-07-30\\', \\'abstract\\': \"Machine learning algorithms have been used widely in various applications and areas. To fit a machine learning model into different problems, its hyper-parameters must be tuned. Selecting the best hyper-parameter configuration for machine learning models has a direct impact on the model\\'s performance. It often requires deep knowledge of machine learning algorithms and appropriate hyper-parameter optimization techniques. Although several automatic optimization techniques exist, they have different strengths and drawbacks when applied to different types of problems. In this paper, optimizing the hyper-parameters of common machine learning models is studied. We introduce several state-of-the-art optimization techniques and discuss how to apply them to machine learning algorithms. Many available libraries and frameworks developed for hyper-parameter optimization problems are provided, and some open challenges of hyper-parameter optimization research are also discussed in this paper. Moreover, experiments are conducted on benchmark datasets to compare the performance of different optimization methods and provide practical examples of hyper-parameter optimization. This survey paper will help industrial users, data analysts, and researchers to better develop machine learning models by identifying the proper hyper-parameter configurations effectively.\", \\'pdf_url\\': None}, {\\'title\\': \\'Optimization Methods in Deep Learning: A Comprehensive Overview\\', \\'authors\\': [\\'David Shulman\\'], \\'published\\': \\'2023-02-19\\', \\'abstract\\': \\'In recent years, deep learning has achieved remarkable success in various fields such as image recognition, natural language processing, and speech recognition. The effectiveness of deep learning largely depends on the optimization methods used to train deep neural networks. In this paper, we provide an overview of first-order optimization methods such as Stochastic Gradient Descent, Adagrad, Adadelta, and RMSprop, as well as recent momentum-based and adaptive gradient methods such as Nesterov accelerated gradient, Adam, Nadam, AdaMax, and AMSGrad. We also discuss the challenges associated with optimization in deep learning and explore techniques for addressing these challenges, including weight initialization, batch normalization, and layer normalization. Finally, we provide recommendations for selecting optimization methods for different deep learning tasks and datasets. This paper serves as a comprehensive guide to optimization methods in deep learning and can be used as a reference for researchers and practitioners in the field.\\', \\'pdf_url\\': None}, {\\'title\\': \\'Deep learning observables in computational fluid dynamics\\', \\'authors\\': [\\'Kjetil O. Lye\\', \\'Siddhartha Mishra\\', \\'Deep Ray\\'], \\'published\\': \\'2019-03-07\\', \\'abstract\\': \\'Many large scale problems in computational fluid dynamics such as uncertainty quantification, Bayesian inversion, data assimilation and PDE constrained optimization are considered very challenging computationally as they require a large number of expensive (forward) numerical solutions of the corresponding PDEs. We propose a machine learning algorithm, based on deep artificial neural networks, that predicts the underlying \\\\\\\\emph{input parameters to observable} map from a few training samples (computed realizations of this map). By a judicious combination of theoretical arguments and empirical observations, we find suitable network architectures and training hyperparameters that result in robust and efficient neural network approximations of the parameters to observable map. Numerical experiments are presented to demonstrate low prediction errors for the trained network networks, even when the network has been trained with a few samples, at a computational cost which is several orders of magnitude lower than the underlying PDE solver.\\\\n  Moreover, we combine the proposed deep learning algorithm with Monte Carlo (MC) and Quasi-Monte Carlo (QMC) methods to efficiently compute uncertainty propagation for nonlinear PDEs. Under the assumption that the underlying neural networks generalize well, we prove that the deep learning MC and QMC algorithms are guaranteed to be faster than the baseline (quasi-) Monte Carlo methods. Numerical experiments demonstrating one to two orders of magnitude speed up over baseline QMC and MC algorithms, for the intricate problem of computing probability distributions of the observable, are also presented.\\', \\'pdf_url\\': None}, {\\'title\\': \\'Quantum circuit optimization with deep reinforcement learning\\', \\'authors\\': [\\'Thomas Fösel\\', \\'Murphy Yuezhen Niu\\', \\'Florian Marquardt\\', \\'Li Li\\'], \\'published\\': \\'2021-03-13\\', \\'abstract\\': \\'A central aspect for operating future quantum computers is quantum circuit optimization, i.e., the search for efficient realizations of quantum algorithms given the device capabilities. In recent years, powerful approaches have been developed which focus on optimizing the high-level circuit structure. However, these approaches do not consider and thus cannot optimize for the hardware details of the quantum architecture, which is especially important for near-term devices. To address this point, we present an approach to quantum circuit optimization based on reinforcement learning. We demonstrate how an agent, realized by a deep convolutional neural network, can autonomously learn generic strategies to optimize arbitrary circuits on a specific architecture, where the optimization target can be chosen freely by the user. We demonstrate the feasibility of this approach by training agents on 12-qubit random circuits, where we find on average a depth reduction by 27% and a gate count reduction by 15%. We examine the extrapolation to larger circuits than used for training, and envision how this approach can be utilized for near-term quantum devices.\\', \\'pdf_url\\': None}, {\\'title\\': \\'On Connections between Constrained Optimization and Reinforcement Learning\\', \\'authors\\': [\\'Nino Vieillard\\', \\'Olivier Pietquin\\', \\'Matthieu Geist\\'], \\'published\\': \\'2019-10-18\\', \\'abstract\\': \\'Dynamic Programming (DP) provides standard algorithms to solve Markov Decision Processes. However, these algorithms generally do not optimize a scalar objective function. In this paper, we draw connections between DP and (constrained) convex optimization. Specifically, we show clear links in the algorithmic structure between three DP schemes and optimization algorithms. We link Conservative Policy Iteration to Frank-Wolfe, Mirror-Descent Modified Policy Iteration to Mirror Descent, and Politex (Policy Iteration Using Expert Prediction) to Dual Averaging. These abstract DP schemes are representative of a number of (deep) Reinforcement Learning (RL) algorithms. By highlighting these connections (most of which have been noticed earlier, but in a scattered way), we would like to encourage further studies linking RL and convex optimization, that could lead to the design of new, more efficient, and better understood RL algorithms.\\', \\'pdf_url\\': None}, {\\'title\\': \\'NCVX: A General-Purpose Optimization Solver for Constrained Machine and Deep Learning\\', \\'authors\\': [\\'Buyun Liang\\', \\'Tim Mitchell\\', \\'Ju Sun\\'], \\'published\\': \\'2022-10-03\\', \\'abstract\\': \\'Imposing explicit constraints is relatively new but increasingly pressing in deep learning, stimulated by, e.g., trustworthy AI that performs robust optimization over complicated perturbation sets and scientific applications that need to respect physical laws and constraints. However, it can be hard to reliably solve constrained deep learning problems without optimization expertise. The existing deep learning frameworks do not admit constraints. General-purpose optimization packages can handle constraints but do not perform auto-differentiation and have trouble dealing with nonsmoothness. In this paper, we introduce a new software package called NCVX, whose initial release contains the solver PyGRANSO, a PyTorch-enabled general-purpose optimization package for constrained machine/deep learning problems, the first of its kind. NCVX inherits auto-differentiation, GPU acceleration, and tensor variables from PyTorch, and is built on freely available and widely used open-source frameworks. NCVX is available at https://ncvx.org, with detailed documentation and numerous examples from machine/deep learning and other fields.\\', \\'pdf_url\\': None}, {\\'title\\': \\'Gradient-based Bi-level Optimization for Deep Learning: A Survey\\', \\'authors\\': [\\'Can Chen\\', \\'Xi Chen\\', \\'Chen Ma\\', \\'Zixuan Liu\\', \\'Xue Liu\\'], \\'published\\': \\'2022-07-24\\', \\'abstract\\': \\'Bi-level optimization, especially the gradient-based category, has been widely used in the deep learning community including hyperparameter optimization and meta-knowledge extraction. Bi-level optimization embeds one problem within another and the gradient-based category solves the outer-level task by computing the hypergradient, which is much more efficient than classical methods such as the evolutionary algorithm. In this survey, we first give a formal definition of the gradient-based bi-level optimization. Next, we delineate criteria to determine if a research problem is apt for bi-level optimization and provide a practical guide on structuring such problems into a bi-level optimization framework, a feature particularly beneficial for those new to this domain. More specifically, there are two formulations: the single-task formulation to optimize hyperparameters such as regularization parameters and the distilled data, and the multi-task formulation to extract meta-knowledge such as the model initialization. With a bi-level formulation, we then discuss four bi-level optimization solvers to update the outer variable including explicit gradient update, proxy update, implicit function update, and closed-form update. Finally, we wrap up the survey by highlighting two prospective future directions: (1) Effective Data Optimization for Science examined through the lens of task formulation. (2) Accurate Explicit Proxy Update analyzed from an optimization standpoint.\\', \\'pdf_url\\': None}, {\\'title\\': \\'Convex Optimization: Algorithms and Complexity\\', \\'authors\\': [\\'Sébastien Bubeck\\'], \\'published\\': \\'2014-05-20\\', \\'abstract\\': \"This monograph presents the main complexity theorems in convex optimization and their corresponding algorithms. Starting from the fundamental theory of black-box optimization, the material progresses towards recent advances in structural optimization and stochastic optimization. Our presentation of black-box optimization, strongly influenced by Nesterov\\'s seminal book and Nemirovski\\'s lecture notes, includes the analysis of cutting plane methods, as well as (accelerated) gradient descent schemes. We also pay special attention to non-Euclidean settings (relevant algorithms include Frank-Wolfe, mirror descent, and dual averaging) and discuss their relevance in machine learning. We provide a gentle introduction to structural optimization with FISTA (to optimize a sum of a smooth and a simple non-smooth term), saddle-point mirror prox (Nemirovski\\'s alternative to Nesterov\\'s smoothing), and a concise description of interior point methods. In stochastic optimization we discuss stochastic gradient descent, mini-batches, random coordinate descent, and sublinear algorithms. We also briefly touch upon convex relaxation of combinatorial problems and the use of randomness to round solutions, as well as random walks based methods.\", \\'pdf_url\\': None}, {\\'title\\': \\'A multitask deep learning model for real-time deployment in embedded systems\\', \\'authors\\': [\\'Miquel Martí\\', \\'Atsuto Maki\\'], \\'published\\': \\'2017-10-31\\', \\'abstract\\': \\'We propose an approach to Multitask Learning (MTL) to make deep learning models faster and lighter for applications in which multiple tasks need to be solved simultaneously, which is particularly useful in embedded, real-time systems. We develop a multitask model for both Object Detection and Semantic Segmentation and analyze the challenges that appear during its training. Our multitask network is 1.6x faster, lighter and uses less memory than deploying the single-task models in parallel. We conclude that MTL has the potential to give superior performance in exchange of a more complex training process that introduces challenges not present in single-task models.\\', \\'pdf_url\\': None}, {\\'title\\': \\'BenLOC: A Benchmark for Learning to Configure MIP Optimizers\\', \\'authors\\': [\\'Hongpei Li\\', \\'Ziyan He\\', \\'Yufei Wang\\', \\'Wenting Tu\\', \\'Shanwen Pu\\', \\'Qi Deng\\', \\'Dongdong Ge\\'], \\'published\\': \\'2025-06-03\\', \\'abstract\\': \\'The automatic configuration of Mixed-Integer Programming (MIP) optimizers has become increasingly critical as the large number of configurations can significantly affect solver performance. Yet the lack of standardized evaluation frameworks has led to data leakage and over-optimistic claims, as prior studies often rely on homogeneous datasets and inconsistent experimental setups. To promote a fair evaluation process, we present BenLOC, a comprehensive benchmark and open-source toolkit, which not only offers an end-to-end pipeline for learning instance-wise MIP optimizer configurations, but also standardizes dataset selection, train-test splits, feature engineering and baseline choice for unbiased and comprehensive evaluations. Leveraging this framework, we conduct an empirical analysis on five well-established MIP datasets and compare classical machine learning models with handcrafted features against state-of-the-art deep-learning techniques. The results demonstrate the importance of datasets, features and baseline criteria proposed by BenLOC and the effectiveness of BenLOC in providing unbiased and comprehensive evaluations.\\', \\'pdf_url\\': None}, {\\'title\\': \\'DILIE: Deep Internal Learning for Image Enhancement\\', \\'authors\\': [\\'Indra Deep Mastan\\', \\'Shanmuganathan Raman\\'], \\'published\\': \\'2020-12-11\\', \\'abstract\\': \\'We consider the generic deep image enhancement problem where an input image is transformed into a perceptually better-looking image. Recent methods for image enhancement consider the problem by performing style transfer and image restoration. The methods mostly fall into two categories: training data-based and training data-independent (deep internal learning methods). We perform image enhancement in the deep internal learning framework. Our Deep Internal Learning for Image Enhancement framework enhances content features and style features and uses contextual content loss for preserving image context in the enhanced image. We show results on both hazy and noisy image enhancement. To validate the results, we use structure similarity and perceptual error, which is efficient in measuring the unrealistic deformation present in the images. We show that the proposed framework outperforms the relevant state-of-the-art works for image enhancement.\\', \\'pdf_url\\': None}, {\\'title\\': \\'Activation Analysis of a Byte-Based Deep Neural Network for Malware Classification\\', \\'authors\\': [\\'Scott E. Coull\\', \\'Christopher Gardner\\'], \\'published\\': \\'2019-03-12\\', \\'abstract\\': \\'Feature engineering is one of the most costly aspects of developing effective machine learning models, and that cost is even greater in specialized problem domains, like malware classification, where expert skills are necessary to identify useful features. Recent work, however, has shown that deep learning models can be used to automatically learn feature representations directly from the raw, unstructured bytes of the binaries themselves. In this paper, we explore what these models are learning about malware. To do so, we examine the learned features at multiple levels of resolution, from individual byte embeddings to end-to-end analysis of the model. At each step, we connect these byte-oriented activations to their original semantics through parsing and disassembly of the binary to arrive at human-understandable features. Through our results, we identify several interesting features learned by the model and their connection to manually-derived features typically used by traditional machine learning models. Additionally, we explore the impact of training data volume and regularization on the quality of the learned features and the efficacy of the classifiers, revealing the somewhat paradoxical insight that better generalization does not necessarily result in better performance for byte-based malware classifiers.\\', \\'pdf_url\\': None}, {\\'title\\': \\'Predicting Thrombectomy Recanalization from CT Imaging Using Deep Learning Models\\', \\'authors\\': [\\'Haoyue Zhang\\', \\'Jennifer S. Polson\\', \\'Eric J. Yang\\', \\'Kambiz Nael\\', \\'William Speier\\', \\'Corey W. Arnold\\'], \\'published\\': \\'2023-02-08\\', \\'abstract\\': \"For acute ischemic stroke (AIS) patients with large vessel occlusions, clinicians must decide if the benefit of mechanical thrombectomy (MTB) outweighs the risks and potential complications following an invasive procedure. Pre-treatment computed tomography (CT) and angiography (CTA) are widely used to characterize occlusions in the brain vasculature. If a patient is deemed eligible, a modified treatment in cerebral ischemia (mTICI) score will be used to grade how well blood flow is reestablished throughout and following the MTB procedure. An estimation of the likelihood of successful recanalization can support treatment decision-making. In this study, we proposed a fully automated prediction of a patient\\'s recanalization score using pre-treatment CT and CTA imaging. We designed a spatial cross attention network (SCANet) that utilizes vision transformers to localize to pertinent slices and brain regions. Our top model achieved an average cross-validated ROC-AUC of 77.33 $\\\\\\\\pm$ 3.9\\\\\\\\%. This is a promising result that supports future applications of deep learning on CT and CTA for the identification of eligible AIS patients for MTB.\", \\'pdf_url\\': None}, {\\'title\\': \\'Learning Objective Boundaries for Constraint Optimization Problems\\', \\'authors\\': [\\'Helge Spieker\\', \\'Arnaud Gotlieb\\'], \\'published\\': \\'2020-06-20\\', \\'abstract\\': \"Constraint Optimization Problems (COP) are often considered without sufficient knowledge on the boundaries of the objective variable to optimize. When available, tight boundaries are helpful to prune the search space or estimate problem characteristics. Finding close boundaries, that correctly under- and overestimate the optimum, is almost impossible without actually solving the COP. This paper introduces Bion, a novel approach for boundary estimation by learning from previously solved instances of the COP. Based on supervised machine learning, Bion is problem-specific and solver-independent and can be applied to any COP which is repeatedly solved with different data inputs. An experimental evaluation over seven realistic COPs shows that an estimation model can be trained to prune the objective variables\\' domains by over 80%. By evaluating the estimated boundaries with various COP solvers, we find that Bion improves the solving process for some problems, although the effect of closer bounds is generally problem-dependent.\", \\'pdf_url\\': None}, {\\'title\\': \\'PePR: Performance Per Resource Unit as a Metric to Promote Small-Scale Deep Learning in Medical Image Analysis\\', \\'authors\\': [\\'Raghavendra Selvan\\', \\'Bob Pepin\\', \\'Christian Igel\\', \\'Gabrielle Samuel\\', \\'Erik B Dam\\'], \\'published\\': \\'2024-03-19\\', \\'abstract\\': \\'The recent advances in deep learning (DL) have been accelerated by access to large-scale data and compute. These large-scale resources have been used to train progressively larger models which are resource intensive in terms of compute, data, energy, and carbon emissions. These costs are becoming a new type of entry barrier to researchers and practitioners with limited access to resources at such scale, particularly in the Global South. In this work, we take a comprehensive look at the landscape of existing DL models for medical image analysis tasks and demonstrate their usefulness in settings where resources are limited. To account for the resource consumption of DL models, we introduce a novel measure to estimate the performance per resource unit, which we call the PePR score. Using a diverse family of 131 unique DL architectures (spanning 1M to 130M trainable parameters) and three medical image datasets, we capture trends about the performance-resource trade-offs. In applications like medical image analysis, we argue that small-scale, specialized models are better than striving for large-scale models. Furthermore, we show that using existing pretrained models that are fine-tuned on new data can significantly reduce the computational resources and data required compared to training models from scratch. We hope this work will encourage the community to focus on improving AI equity by developing methods and models with smaller resource footprints.\\', \\'pdf_url\\': None}]', name='arxiv_search', call_id='call_QsLs991S6DO9EpyWE8bVuCLz', is_error=False)]\n",
      "---------- ToolCallSummaryMessage (Arxiv_Search_Agent) ----------\n",
      "[{'title': 'Iterative Surrogate Model Optimization (ISMO): An active learning algorithm for PDE constrained optimization with deep neural networks', 'authors': ['Kjetil O. Lye', 'Siddhartha Mishra', 'Deep Ray', 'Praveen Chandrasekhar'], 'published': '2020-08-13', 'abstract': 'We present a novel active learning algorithm, termed as iterative surrogate model optimization (ISMO), for robust and efficient numerical approximation of PDE constrained optimization problems. This algorithm is based on deep neural networks and its key feature is the iterative selection of training data through a feedback loop between deep neural networks and any underlying standard optimization algorithm. Under suitable hypotheses, we show that the resulting optimizers converge exponentially fast (and with exponentially decaying variance), with respect to increasing number of training samples. Numerical examples for optimal control, parameter identification and shape optimization problems for PDEs are provided to validate the proposed theory and to illustrate that ISMO significantly outperforms a standard deep neural network based surrogate optimization algorithm.', 'pdf_url': None}, {'title': 'The Modern Mathematics of Deep Learning', 'authors': ['Julius Berner', 'Philipp Grohs', 'Gitta Kutyniok', 'Philipp Petersen'], 'published': '2021-05-09', 'abstract': 'We describe the new field of mathematical analysis of deep learning. This field emerged around a list of research questions that were not answered within the classical framework of learning theory. These questions concern: the outstanding generalization power of overparametrized neural networks, the role of depth in deep architectures, the apparent absence of the curse of dimensionality, the surprisingly successful optimization performance despite the non-convexity of the problem, understanding what features are learned, why deep architectures perform exceptionally well in physical problems, and which fine aspects of an architecture affect the behavior of a learning task in which way. We present an overview of modern approaches that yield partial answers to these questions. For selected approaches, we describe the main ideas in more detail.', 'pdf_url': None}, {'title': 'Learn to Accumulate Evidence from All Training Samples: Theory and Practice', 'authors': ['Deep Pandey', 'Qi Yu'], 'published': '2023-06-19', 'abstract': 'Evidential deep learning, built upon belief theory and subjective logic, offers a principled and computationally efficient way to turn a deterministic neural network uncertainty-aware. The resultant evidential models can quantify fine-grained uncertainty using the learned evidence. To ensure theoretically sound evidential models, the evidence needs to be non-negative, which requires special activation functions for model training and inference. This constraint often leads to inferior predictive performance compared to standard softmax models, making it challenging to extend them to many large-scale datasets. To unveil the real cause of this undesired behavior, we theoretically investigate evidential models and identify a fundamental limitation that explains the inferior performance: existing evidential activation functions create zero evidence regions, which prevent the model to learn from training samples falling into such regions. A deeper analysis of evidential activation functions based on our theoretical underpinning inspires the design of a novel regularizer that effectively alleviates this fundamental limitation. Extensive experiments over many challenging real-world datasets and settings confirm our theoretical findings and demonstrate the effectiveness of our proposed approach.', 'pdf_url': None}, {'title': 'Towards Practical Second-Order Optimizers in Deep Learning: Insights from Fisher Information Analysis', 'authors': ['Damien Martins Gomes'], 'published': '2025-04-26', 'abstract': 'First-order optimization methods remain the standard for training deep neural networks (DNNs). Optimizers like Adam incorporate limited curvature information by preconditioning the stochastic gradient with a diagonal matrix. Despite the widespread adoption of first-order methods, second-order optimization algorithms often exhibit superior convergence compared to methods like Adam and SGD. However, their practicality in training DNNs is still limited by a significantly higher per-iteration computational cost compared to first-order methods. In this thesis, we present AdaFisher, a novel adaptive second-order optimizer that leverages a diagonal block-Kronecker approximation of the Fisher information matrix to adaptively precondition gradients. AdaFisher aims to bridge the gap between the improved convergence and generalization of second-order methods and the computational efficiency needed for training DNNs. Despite the traditionally slower speed of second-order optimizers, AdaFisher is effective for tasks such as image classification and language modeling, exhibiting remarkable stability and robustness during hyperparameter tuning. We demonstrate that AdaFisher outperforms state-of-the-art optimizers in both accuracy and convergence speed. The code is available from https://github.com/AtlasAnalyticsLab/AdaFisher.', 'pdf_url': None}, {'title': 'Deep Learning and Computational Physics (Lecture Notes)', 'authors': ['Deep Ray', 'Orazio Pinti', 'Assad A. Oberai'], 'published': '2023-01-03', 'abstract': 'These notes were compiled as lecture notes for a course developed and taught at the University of the Southern California. They should be accessible to a typical engineering graduate student with a strong background in Applied Mathematics.\\n  The main objective of these notes is to introduce a student who is familiar with concepts in linear algebra and partial differential equations to select topics in deep learning. These lecture notes exploit the strong connections between deep learning algorithms and the more conventional techniques of computational physics to achieve two goals. First, they use concepts from computational physics to develop an understanding of deep learning algorithms. Not surprisingly, many concepts in deep learning can be connected to similar concepts in computational physics, and one can utilize this connection to better understand these algorithms. Second, several novel deep learning algorithms can be used to solve challenging problems in computational physics. Thus, they offer someone who is interested in modeling a physical phenomena with a complementary set of tools.', 'pdf_url': None}, {'title': 'On Hyperparameter Optimization of Machine Learning Algorithms: Theory and Practice', 'authors': ['Li Yang', 'Abdallah Shami'], 'published': '2020-07-30', 'abstract': \"Machine learning algorithms have been used widely in various applications and areas. To fit a machine learning model into different problems, its hyper-parameters must be tuned. Selecting the best hyper-parameter configuration for machine learning models has a direct impact on the model's performance. It often requires deep knowledge of machine learning algorithms and appropriate hyper-parameter optimization techniques. Although several automatic optimization techniques exist, they have different strengths and drawbacks when applied to different types of problems. In this paper, optimizing the hyper-parameters of common machine learning models is studied. We introduce several state-of-the-art optimization techniques and discuss how to apply them to machine learning algorithms. Many available libraries and frameworks developed for hyper-parameter optimization problems are provided, and some open challenges of hyper-parameter optimization research are also discussed in this paper. Moreover, experiments are conducted on benchmark datasets to compare the performance of different optimization methods and provide practical examples of hyper-parameter optimization. This survey paper will help industrial users, data analysts, and researchers to better develop machine learning models by identifying the proper hyper-parameter configurations effectively.\", 'pdf_url': None}, {'title': 'Optimization Methods in Deep Learning: A Comprehensive Overview', 'authors': ['David Shulman'], 'published': '2023-02-19', 'abstract': 'In recent years, deep learning has achieved remarkable success in various fields such as image recognition, natural language processing, and speech recognition. The effectiveness of deep learning largely depends on the optimization methods used to train deep neural networks. In this paper, we provide an overview of first-order optimization methods such as Stochastic Gradient Descent, Adagrad, Adadelta, and RMSprop, as well as recent momentum-based and adaptive gradient methods such as Nesterov accelerated gradient, Adam, Nadam, AdaMax, and AMSGrad. We also discuss the challenges associated with optimization in deep learning and explore techniques for addressing these challenges, including weight initialization, batch normalization, and layer normalization. Finally, we provide recommendations for selecting optimization methods for different deep learning tasks and datasets. This paper serves as a comprehensive guide to optimization methods in deep learning and can be used as a reference for researchers and practitioners in the field.', 'pdf_url': None}, {'title': 'Deep learning observables in computational fluid dynamics', 'authors': ['Kjetil O. Lye', 'Siddhartha Mishra', 'Deep Ray'], 'published': '2019-03-07', 'abstract': 'Many large scale problems in computational fluid dynamics such as uncertainty quantification, Bayesian inversion, data assimilation and PDE constrained optimization are considered very challenging computationally as they require a large number of expensive (forward) numerical solutions of the corresponding PDEs. We propose a machine learning algorithm, based on deep artificial neural networks, that predicts the underlying \\\\emph{input parameters to observable} map from a few training samples (computed realizations of this map). By a judicious combination of theoretical arguments and empirical observations, we find suitable network architectures and training hyperparameters that result in robust and efficient neural network approximations of the parameters to observable map. Numerical experiments are presented to demonstrate low prediction errors for the trained network networks, even when the network has been trained with a few samples, at a computational cost which is several orders of magnitude lower than the underlying PDE solver.\\n  Moreover, we combine the proposed deep learning algorithm with Monte Carlo (MC) and Quasi-Monte Carlo (QMC) methods to efficiently compute uncertainty propagation for nonlinear PDEs. Under the assumption that the underlying neural networks generalize well, we prove that the deep learning MC and QMC algorithms are guaranteed to be faster than the baseline (quasi-) Monte Carlo methods. Numerical experiments demonstrating one to two orders of magnitude speed up over baseline QMC and MC algorithms, for the intricate problem of computing probability distributions of the observable, are also presented.', 'pdf_url': None}, {'title': 'Quantum circuit optimization with deep reinforcement learning', 'authors': ['Thomas Fösel', 'Murphy Yuezhen Niu', 'Florian Marquardt', 'Li Li'], 'published': '2021-03-13', 'abstract': 'A central aspect for operating future quantum computers is quantum circuit optimization, i.e., the search for efficient realizations of quantum algorithms given the device capabilities. In recent years, powerful approaches have been developed which focus on optimizing the high-level circuit structure. However, these approaches do not consider and thus cannot optimize for the hardware details of the quantum architecture, which is especially important for near-term devices. To address this point, we present an approach to quantum circuit optimization based on reinforcement learning. We demonstrate how an agent, realized by a deep convolutional neural network, can autonomously learn generic strategies to optimize arbitrary circuits on a specific architecture, where the optimization target can be chosen freely by the user. We demonstrate the feasibility of this approach by training agents on 12-qubit random circuits, where we find on average a depth reduction by 27% and a gate count reduction by 15%. We examine the extrapolation to larger circuits than used for training, and envision how this approach can be utilized for near-term quantum devices.', 'pdf_url': None}, {'title': 'On Connections between Constrained Optimization and Reinforcement Learning', 'authors': ['Nino Vieillard', 'Olivier Pietquin', 'Matthieu Geist'], 'published': '2019-10-18', 'abstract': 'Dynamic Programming (DP) provides standard algorithms to solve Markov Decision Processes. However, these algorithms generally do not optimize a scalar objective function. In this paper, we draw connections between DP and (constrained) convex optimization. Specifically, we show clear links in the algorithmic structure between three DP schemes and optimization algorithms. We link Conservative Policy Iteration to Frank-Wolfe, Mirror-Descent Modified Policy Iteration to Mirror Descent, and Politex (Policy Iteration Using Expert Prediction) to Dual Averaging. These abstract DP schemes are representative of a number of (deep) Reinforcement Learning (RL) algorithms. By highlighting these connections (most of which have been noticed earlier, but in a scattered way), we would like to encourage further studies linking RL and convex optimization, that could lead to the design of new, more efficient, and better understood RL algorithms.', 'pdf_url': None}, {'title': 'NCVX: A General-Purpose Optimization Solver for Constrained Machine and Deep Learning', 'authors': ['Buyun Liang', 'Tim Mitchell', 'Ju Sun'], 'published': '2022-10-03', 'abstract': 'Imposing explicit constraints is relatively new but increasingly pressing in deep learning, stimulated by, e.g., trustworthy AI that performs robust optimization over complicated perturbation sets and scientific applications that need to respect physical laws and constraints. However, it can be hard to reliably solve constrained deep learning problems without optimization expertise. The existing deep learning frameworks do not admit constraints. General-purpose optimization packages can handle constraints but do not perform auto-differentiation and have trouble dealing with nonsmoothness. In this paper, we introduce a new software package called NCVX, whose initial release contains the solver PyGRANSO, a PyTorch-enabled general-purpose optimization package for constrained machine/deep learning problems, the first of its kind. NCVX inherits auto-differentiation, GPU acceleration, and tensor variables from PyTorch, and is built on freely available and widely used open-source frameworks. NCVX is available at https://ncvx.org, with detailed documentation and numerous examples from machine/deep learning and other fields.', 'pdf_url': None}, {'title': 'Gradient-based Bi-level Optimization for Deep Learning: A Survey', 'authors': ['Can Chen', 'Xi Chen', 'Chen Ma', 'Zixuan Liu', 'Xue Liu'], 'published': '2022-07-24', 'abstract': 'Bi-level optimization, especially the gradient-based category, has been widely used in the deep learning community including hyperparameter optimization and meta-knowledge extraction. Bi-level optimization embeds one problem within another and the gradient-based category solves the outer-level task by computing the hypergradient, which is much more efficient than classical methods such as the evolutionary algorithm. In this survey, we first give a formal definition of the gradient-based bi-level optimization. Next, we delineate criteria to determine if a research problem is apt for bi-level optimization and provide a practical guide on structuring such problems into a bi-level optimization framework, a feature particularly beneficial for those new to this domain. More specifically, there are two formulations: the single-task formulation to optimize hyperparameters such as regularization parameters and the distilled data, and the multi-task formulation to extract meta-knowledge such as the model initialization. With a bi-level formulation, we then discuss four bi-level optimization solvers to update the outer variable including explicit gradient update, proxy update, implicit function update, and closed-form update. Finally, we wrap up the survey by highlighting two prospective future directions: (1) Effective Data Optimization for Science examined through the lens of task formulation. (2) Accurate Explicit Proxy Update analyzed from an optimization standpoint.', 'pdf_url': None}, {'title': 'Convex Optimization: Algorithms and Complexity', 'authors': ['Sébastien Bubeck'], 'published': '2014-05-20', 'abstract': \"This monograph presents the main complexity theorems in convex optimization and their corresponding algorithms. Starting from the fundamental theory of black-box optimization, the material progresses towards recent advances in structural optimization and stochastic optimization. Our presentation of black-box optimization, strongly influenced by Nesterov's seminal book and Nemirovski's lecture notes, includes the analysis of cutting plane methods, as well as (accelerated) gradient descent schemes. We also pay special attention to non-Euclidean settings (relevant algorithms include Frank-Wolfe, mirror descent, and dual averaging) and discuss their relevance in machine learning. We provide a gentle introduction to structural optimization with FISTA (to optimize a sum of a smooth and a simple non-smooth term), saddle-point mirror prox (Nemirovski's alternative to Nesterov's smoothing), and a concise description of interior point methods. In stochastic optimization we discuss stochastic gradient descent, mini-batches, random coordinate descent, and sublinear algorithms. We also briefly touch upon convex relaxation of combinatorial problems and the use of randomness to round solutions, as well as random walks based methods.\", 'pdf_url': None}, {'title': 'A multitask deep learning model for real-time deployment in embedded systems', 'authors': ['Miquel Martí', 'Atsuto Maki'], 'published': '2017-10-31', 'abstract': 'We propose an approach to Multitask Learning (MTL) to make deep learning models faster and lighter for applications in which multiple tasks need to be solved simultaneously, which is particularly useful in embedded, real-time systems. We develop a multitask model for both Object Detection and Semantic Segmentation and analyze the challenges that appear during its training. Our multitask network is 1.6x faster, lighter and uses less memory than deploying the single-task models in parallel. We conclude that MTL has the potential to give superior performance in exchange of a more complex training process that introduces challenges not present in single-task models.', 'pdf_url': None}, {'title': 'BenLOC: A Benchmark for Learning to Configure MIP Optimizers', 'authors': ['Hongpei Li', 'Ziyan He', 'Yufei Wang', 'Wenting Tu', 'Shanwen Pu', 'Qi Deng', 'Dongdong Ge'], 'published': '2025-06-03', 'abstract': 'The automatic configuration of Mixed-Integer Programming (MIP) optimizers has become increasingly critical as the large number of configurations can significantly affect solver performance. Yet the lack of standardized evaluation frameworks has led to data leakage and over-optimistic claims, as prior studies often rely on homogeneous datasets and inconsistent experimental setups. To promote a fair evaluation process, we present BenLOC, a comprehensive benchmark and open-source toolkit, which not only offers an end-to-end pipeline for learning instance-wise MIP optimizer configurations, but also standardizes dataset selection, train-test splits, feature engineering and baseline choice for unbiased and comprehensive evaluations. Leveraging this framework, we conduct an empirical analysis on five well-established MIP datasets and compare classical machine learning models with handcrafted features against state-of-the-art deep-learning techniques. The results demonstrate the importance of datasets, features and baseline criteria proposed by BenLOC and the effectiveness of BenLOC in providing unbiased and comprehensive evaluations.', 'pdf_url': None}, {'title': 'DILIE: Deep Internal Learning for Image Enhancement', 'authors': ['Indra Deep Mastan', 'Shanmuganathan Raman'], 'published': '2020-12-11', 'abstract': 'We consider the generic deep image enhancement problem where an input image is transformed into a perceptually better-looking image. Recent methods for image enhancement consider the problem by performing style transfer and image restoration. The methods mostly fall into two categories: training data-based and training data-independent (deep internal learning methods). We perform image enhancement in the deep internal learning framework. Our Deep Internal Learning for Image Enhancement framework enhances content features and style features and uses contextual content loss for preserving image context in the enhanced image. We show results on both hazy and noisy image enhancement. To validate the results, we use structure similarity and perceptual error, which is efficient in measuring the unrealistic deformation present in the images. We show that the proposed framework outperforms the relevant state-of-the-art works for image enhancement.', 'pdf_url': None}, {'title': 'Activation Analysis of a Byte-Based Deep Neural Network for Malware Classification', 'authors': ['Scott E. Coull', 'Christopher Gardner'], 'published': '2019-03-12', 'abstract': 'Feature engineering is one of the most costly aspects of developing effective machine learning models, and that cost is even greater in specialized problem domains, like malware classification, where expert skills are necessary to identify useful features. Recent work, however, has shown that deep learning models can be used to automatically learn feature representations directly from the raw, unstructured bytes of the binaries themselves. In this paper, we explore what these models are learning about malware. To do so, we examine the learned features at multiple levels of resolution, from individual byte embeddings to end-to-end analysis of the model. At each step, we connect these byte-oriented activations to their original semantics through parsing and disassembly of the binary to arrive at human-understandable features. Through our results, we identify several interesting features learned by the model and their connection to manually-derived features typically used by traditional machine learning models. Additionally, we explore the impact of training data volume and regularization on the quality of the learned features and the efficacy of the classifiers, revealing the somewhat paradoxical insight that better generalization does not necessarily result in better performance for byte-based malware classifiers.', 'pdf_url': None}, {'title': 'Predicting Thrombectomy Recanalization from CT Imaging Using Deep Learning Models', 'authors': ['Haoyue Zhang', 'Jennifer S. Polson', 'Eric J. Yang', 'Kambiz Nael', 'William Speier', 'Corey W. Arnold'], 'published': '2023-02-08', 'abstract': \"For acute ischemic stroke (AIS) patients with large vessel occlusions, clinicians must decide if the benefit of mechanical thrombectomy (MTB) outweighs the risks and potential complications following an invasive procedure. Pre-treatment computed tomography (CT) and angiography (CTA) are widely used to characterize occlusions in the brain vasculature. If a patient is deemed eligible, a modified treatment in cerebral ischemia (mTICI) score will be used to grade how well blood flow is reestablished throughout and following the MTB procedure. An estimation of the likelihood of successful recanalization can support treatment decision-making. In this study, we proposed a fully automated prediction of a patient's recanalization score using pre-treatment CT and CTA imaging. We designed a spatial cross attention network (SCANet) that utilizes vision transformers to localize to pertinent slices and brain regions. Our top model achieved an average cross-validated ROC-AUC of 77.33 $\\\\pm$ 3.9\\\\%. This is a promising result that supports future applications of deep learning on CT and CTA for the identification of eligible AIS patients for MTB.\", 'pdf_url': None}, {'title': 'Learning Objective Boundaries for Constraint Optimization Problems', 'authors': ['Helge Spieker', 'Arnaud Gotlieb'], 'published': '2020-06-20', 'abstract': \"Constraint Optimization Problems (COP) are often considered without sufficient knowledge on the boundaries of the objective variable to optimize. When available, tight boundaries are helpful to prune the search space or estimate problem characteristics. Finding close boundaries, that correctly under- and overestimate the optimum, is almost impossible without actually solving the COP. This paper introduces Bion, a novel approach for boundary estimation by learning from previously solved instances of the COP. Based on supervised machine learning, Bion is problem-specific and solver-independent and can be applied to any COP which is repeatedly solved with different data inputs. An experimental evaluation over seven realistic COPs shows that an estimation model can be trained to prune the objective variables' domains by over 80%. By evaluating the estimated boundaries with various COP solvers, we find that Bion improves the solving process for some problems, although the effect of closer bounds is generally problem-dependent.\", 'pdf_url': None}, {'title': 'PePR: Performance Per Resource Unit as a Metric to Promote Small-Scale Deep Learning in Medical Image Analysis', 'authors': ['Raghavendra Selvan', 'Bob Pepin', 'Christian Igel', 'Gabrielle Samuel', 'Erik B Dam'], 'published': '2024-03-19', 'abstract': 'The recent advances in deep learning (DL) have been accelerated by access to large-scale data and compute. These large-scale resources have been used to train progressively larger models which are resource intensive in terms of compute, data, energy, and carbon emissions. These costs are becoming a new type of entry barrier to researchers and practitioners with limited access to resources at such scale, particularly in the Global South. In this work, we take a comprehensive look at the landscape of existing DL models for medical image analysis tasks and demonstrate their usefulness in settings where resources are limited. To account for the resource consumption of DL models, we introduce a novel measure to estimate the performance per resource unit, which we call the PePR score. Using a diverse family of 131 unique DL architectures (spanning 1M to 130M trainable parameters) and three medical image datasets, we capture trends about the performance-resource trade-offs. In applications like medical image analysis, we argue that small-scale, specialized models are better than striving for large-scale models. Furthermore, we show that using existing pretrained models that are fine-tuned on new data can significantly reduce the computational resources and data required compared to training models from scratch. We hope this work will encourage the community to focus on improving AI equity by developing methods and models with smaller resource footprints.', 'pdf_url': None}]\n",
      "---------- TextMessage (MagenticOneOrchestrator) ----------\n",
      "Please compile the gathered publications into a structured literature review focusing on novel optimization algorithms in deep learning.\n",
      "---------- TextMessage (Report_Agent) ----------\n",
      "# Literature Review on Recent Advances in Novel Optimization Algorithms for Deep Learning\n",
      "\n",
      "## Introduction\n",
      "\n",
      "Deep learning has transformed various fields such as computer vision, natural language processing, and more. The optimization of deep neural networks (DNNs) is a critical aspect that significantly influences learning efficiency and model performance. Traditionally, first-order optimizers like Stochastic Gradient Descent (SGD) and its variants such as Adam have been predominant. However, recent research has focused on novel optimization algorithms aimed at enhancing convergence rates, improving generalization, and integrating adaptive techniques. This review highlights recent advances in optimization algorithms for deep learning, particularly focusing on novel methods developed in the past few years.\n",
      "\n",
      "## Novel Optimization Algorithms\n",
      "\n",
      "1. **Adaptive Friction Optimizers**\n",
      "   Zheng et al. (2024) introduce two novel optimizers, sigSignGrad and tanhSignGrad, which incorporate adaptive friction coefficients based on Sigmoid and Tanh functions, respectively. The proposed optimizers leverage short-term gradient information to enhance parameter updates, resulting in improved accuracy and reduced training time across datasets such as CIFAR-10 and Mini-ImageNet.\n",
      "\n",
      "2. **AdaFisher: Adaptive Second-Order Optimizer**\n",
      "   Gomes (2025) presents AdaFisher, an adaptive second-order optimizer that uses a diagonal block-Kronecker approximation of the Fisher information matrix. This technique aims to bridge the gap between second-order methods and the computational efficiency required for large DNNs. AdaFisher shows superior accuracy and convergence speed in comparative analyses.\n",
      "\n",
      "3. **Metaheuristic Algorithms for Security in IoT**\n",
      "   Sagu et al. (2023) developed novel metaheuristic optimization algorithms formulated to tune deep learning models for intrusion detection in IoT environments. The proposed techniques demonstrate improved performance against conventional approaches, showcasing the application of novel optimization strategies in securing deep learning models from cyberattacks.\n",
      "\n",
      "4. **Hybrid Optimization in Feature Selection**\n",
      "   Mehrabi Hashjin et al. (2024) propose a hybrid classifier utilizing the Improved Chaos Game Optimization algorithm to improve decision-making in malware detection. This work merges deep learning with optimization to refine the feature selection process and enhance classification accuracy beyond traditional methods.\n",
      "\n",
      "5. **ISMO: Iterative Surrogate Model Optimization**\n",
      "   Lye et al. (2020) introduced a novel active learning algorithm, Iterative Surrogate Model Optimization (ISMO), designed to enhance PDE constrained optimization tasks through deep neural networks. Their results illustrate a robust performance, particularly for optimal control and shape optimization scenarios.\n",
      "\n",
      "6. **Hybrid Gene Selection with Deep Learning**\n",
      "   Yaqoob et al. (2024) focus on a hybrid genetic selection approach that combines Harris Hawk Optimization and Whale Optimization algorithms with deep learning for breast cancer detection. This model achieves high classification accuracy, demonstrating the effectiveness of combining multiple optimization techniques to handle complex datasets.\n",
      "\n",
      "7. **Gradient-based Bi-level Optimization**\n",
      "   Chen et al. (2022) survey gradient-based bi-level optimization methods widely employed in hyperparameter tuning and meta-learning. They provide a framework for applying these methods effectively, contributing to both theoretical understanding and practical implementation of bi-level optimization in deep learning.\n",
      "\n",
      "8. **Performance per Resource Unit (PePR) Metric**\n",
      "   Selvan et al. (2024) propose the PePR score, a novel metric aimed at encouraging the development of deep learning models with smaller resource footprints. This work emphasizes the importance of efficiency in optimization, especially concerning medical image analysis tasks, paving the way for equitable AI solutions.\n",
      "\n",
      "## Performance Comparisons and Effectiveness\n",
      "\n",
      "Recent studies have systematically compared these novel optimizers against traditional methods. For example, Zheng et al. demonstrated the improved performance of their adaptive friction models not only in terms of convergence rate but also in terms of model generalization to unseen data. Similarly, AdaFisher achieved performance metrics surpassing state-of-the-art first-order optimizers in both robustness and accuracy on key tasks like image classification.\n",
      "\n",
      "The integration of metaheuristic algorithms, as discussed by Sagu et al. and Yaqoob et al., highlights the growing trend of hybrid approaches that combine classical optimization techniques with modern machine learning strategies, allowing for better adaptability and performance in complex environments.\n",
      "\n",
      "## Conclusion\n",
      "\n",
      "The advancement of novel optimization algorithms in deep learning offers promising directions for improving model training and performance. As the field continues to evolve, ongoing research exploring hybridization, second-order methods, and adaptive techniques will likely enhance the capabilities of deep learning in various applications. The exploration of novel optimization strategies remains crucial for addressing the computational challenges and performance limitations inherent in modern deep learning practices.\n",
      "\n",
      "## References\n",
      "\n",
      "1. Zheng, H., Wang, B., Xiao, M., Qin, H., Wu, Z., & Tan, L. (2024). Adaptive Friction in Deep Learning: Enhancing Optimizers with Sigmoid and Tanh Function.\n",
      "   \n",
      "2. Sagu, A., Gill, N. S., Gulia, P., Singh, P., & Hong, W. C. (2023). Design of Metaheuristic Optimization Algorithms for Deep Learning Model for Secure IoT Environment.\n",
      "\n",
      "3. Gomes, D. M. (2025). Towards Practical Second-Order Optimizers in Deep Learning: Insights from Fisher Information Analysis.\n",
      "\n",
      "4. Mehrabi Hashjin, N., Amiri, M. H., Mohammadzadeh, A., & Mirjalili, S. (2024). Novel hybrid classifier based on fuzzy type-III decision maker and ensemble deep learning model and improved chaos game optimization.\n",
      "\n",
      "5. Lye, K. O., Mishra, S., Ray, D., & Chandrasekhar, P. (2020). Iterative Surrogate Model Optimization (ISMO): An active learning algorithm for PDE constrained optimization with deep neural networks.\n",
      "\n",
      "6. Yaqoob, A., Verma, N., Aziz, R., & Shah, M. A. (2024). RNA-Seq analysis for breast cancer detection: a study on paired tissue samples using hybrid optimization and deep learning techniques.\n",
      "\n",
      "7. Chen, C., Chen, X., Ma, C., Liu, Z., & Liu, X. (2022). Gradient-based Bi-level Optimization for Deep Learning: A Survey.\n",
      "\n",
      "8. Selvan, R., Pepin, B., Igel, C., Samuel, G., & Dam, E. B. (2024). PePR: Performance Per Resource Unit as a Metric to Promote Small-Scale Deep Learning in Medical Image Analysis.\n",
      "\n",
      "TERMINATE\n"
     ]
    }
   ],
   "source": [
    "await Console(\n",
    "    team.run_stream(\n",
    "        task=\"Make a search of 20 recent publications and write a literature review on recent advances in deep learning optimizers with an accent on novel optimization algorythms\",\n",
    "    )\n",
    ");\n",
    "\n",
    "#await model_client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9c14da-d642-4126-a98c-fdf3c3c3945a",
   "metadata": {},
   "outputs": [],
   "source": [
    "team."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e4df26-cb41-4aef-a990-e34c7007306b",
   "metadata": {},
   "source": [
    "# Literature Review on Recent Advances in Machine Learning Optimizers\n",
    "\n",
    "The field of optimization in machine learning has seen significant advancements in recent years. These developments include novel algorithms, techniques for integrating machine learning with traditional optimization methods, and applications across various domains. This literature review synthesizes the key findings from recent studies highlighting innovative approaches to optimization problems in machine learning.\n",
    "\n",
    "## 1. Novel Optimization Techniques\n",
    "\n",
    "One notable advancement is the integration of Optimal Transport (OT) theory into machine learning optimization methodologies. OT provides a probabilistic framework for comparing probability distributions, which has proven useful for data-driven tasks such as generative modeling and transfer learning. Montesuma et al. (2023) discussed significant contributions in this area, emphasizing recent innovations in computational OT methods and their implications for various aspects of machine learning, including supervised and unsupervised learning paradigms (Montesuma, E. F., Mboula, F. N., & Souloumiac, A. (2023). *Recent Advances in Optimal Transport for Machine Learning*. arXiv:2306.16156).\n",
    "\n",
    "Further, recent studies have harnessed machine learning to enhance conventional optimization techniques, specifically in areas like model predictive control (MPC). E et al. (2022) highlighted how machine learning can empower control solvers, addressing challenges inherent in tackling complex optimal control problems (E, W., Han, J., & Long, J. (2022). *Empowering Optimal Control with Machine Learning: A Perspective from Model Predictive Control*. arXiv:2205.07990).\n",
    "\n",
    "## 2. Constrained Optimization\n",
    "\n",
    "The challenge of integrating machine learning with combinatorial optimization is another area garnering attention. Kotary et al. (2021) provided a comprehensive survey on leveraging machine learning to solve constrained optimization problems. They emphasized the potential for hybrid models that can generate fast, approximate solutions through the combination of machine learning architectures and combinatorial optimizers (Kotary, J., Fioretto, F., Van Hentenryck, P., & Wilder, B. (2021). *End-to-End Constrained Optimization Learning: A Survey*. arXiv:2103.16378).\n",
    "\n",
    "## 3. Process Optimization\n",
    "\n",
    "The application of machine learning in process optimization, especially within chemical engineering, has also seen notable advances. Mitrai and Daoutidis (2024) reviewed strategies for automating the selection and tuning of optimization algorithms based on learned behaviors from numerical solvers. They discussed the importance of representing decision-making problems effectively for machine learning tasks (Mitrai, I., & Daoutidis, P. (2024). *Accelerating Process Control and Optimization via Machine Learning: A Review*. arXiv:2412.18529).\n",
    "\n",
    "## 4. The Interplay Between Machine Learning and Mathematical Programming\n",
    "\n",
    "The integration of machine learning and mathematical programming has opened up new avenues for optimization under uncertainty. Ning and You (2019) explored how data-driven optimization can synergistically link machine learning with traditional optimization frameworks, facilitating better decision-making under uncertain conditions. Their review covered various data-driven approaches, identifying critical research opportunities in this emerging inter-disciplinary field (Ning, C., & You, F. (2019). *Optimization under Uncertainty in the Era of Big Data and Deep Learning: When Machine Learning Meets Mathematical Programming*. arXiv:1904.01934).\n",
    "\n",
    "## 5. Hybrid Models and Future Directions\n",
    "\n",
    "The emergent trend toward hybrid models that combine classical optimization techniques with machine learning is becoming increasingly prominent. These models have demonstrated their capability in tackling intricate systems where conventional methods fall short. As research continues to evolve, scholars advocate for more adaptive algorithms that can learn from their performance dynamically, which has significant implications for real-time optimization scenarios.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "In conclusion, recent advancements in machine learning optimizers reveal a clear trajectory toward the integration of traditional optimization strategies with machine learning innovations. This synthesis of techniques not only enhances the efficiency of optimization processes but also broadens their applicability across diverse fields. Future research is expected to delve deeper into hybrid models, the interactions between learning and optimization, and uncertainty handling mechanisms in complex environments.\n",
    "\n",
    "### References\n",
    "\n",
    "E, W., Han, J., & Long, J. (2022). Empowering Optimal Control with Machine Learning: A Perspective from Model Predictive Control. *arXiv:2205.07990*.  \n",
    "\n",
    "Mitrai, I., & Daoutidis, P. (2024). Accelerating Process Control and Optimization via Machine Learning: A Review. *arXiv:2412.18529*.  \n",
    "\n",
    "Montesuma, E. F., Mboula, F. N., & Souloumiac, A. (2023). Recent Advances in Optimal Transport for Machine Learning. *arXiv:2306.16156*.  \n",
    "\n",
    "Ning, C., & You, F. (2019). Optimization under Uncertainty in the Era of Big Data and Deep Learning: When Machine Learning Meets Mathematical Programming. *arXiv:1904.01934*.\n",
    "\n",
    "Kotary, J., Fioretto, F., Van Hentenryck, P., & Wilder, B. (2021). End-to-End Constrained Optimization Learning: A Survey. *arXiv:2103.16378*.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "66f166d1-5fe1-43f7-b3c7-c72feba4eae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "response = requests.get(\n",
    "    \"https://api.aimlapi.com/models\",\n",
    "    headers={\"Accept\":\"*/*\"},\n",
    ")\n",
    "\n",
    "data = response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "216664b3-4464-4cdc-86ca-3dcfa93217b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "df_models = pd.DataFrame(json.loads(response.content)[\"data\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "503c391b-cf9d-4268-bdf1-fc5fe9fd21ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_models = pd.concat([df_models,pd.json_normalize(df_models[\"info\"])], axis = 1).drop(\"info\", axis = \"columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "8e1cd405-f90e-4b28-a461-d73c27d5037e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>type</th>\n",
       "      <th>features</th>\n",
       "      <th>endpoints</th>\n",
       "      <th>name</th>\n",
       "      <th>developer</th>\n",
       "      <th>description</th>\n",
       "      <th>contextLength</th>\n",
       "      <th>maxTokens</th>\n",
       "      <th>url</th>\n",
       "      <th>docs_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>380</th>\n",
       "      <td>stable-audio</td>\n",
       "      <td>audio</td>\n",
       "      <td>[]</td>\n",
       "      <td>[/v2/generate/audio]</td>\n",
       "      <td>Stable audio</td>\n",
       "      <td>Stability AI</td>\n",
       "      <td>Stable Audio generates high-quality audio from...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://aimlapi.com/models/stable-audio</td>\n",
       "      <td>https://docs.aimlapi.com/api-references/music-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>381</th>\n",
       "      <td>minimax-music</td>\n",
       "      <td>audio</td>\n",
       "      <td>[]</td>\n",
       "      <td>[/v2/generate/audio]</td>\n",
       "      <td>Minimax music</td>\n",
       "      <td>Minimax AI</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>https://docs.aimlapi.com/api-references/music-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>382</th>\n",
       "      <td>google/lyria2</td>\n",
       "      <td>audio</td>\n",
       "      <td>[]</td>\n",
       "      <td>[/v2/generate/audio]</td>\n",
       "      <td>Lyria 2</td>\n",
       "      <td>Google</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>https://docs.aimlapi.com/api-references/music-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>383</th>\n",
       "      <td>music-01</td>\n",
       "      <td>audio</td>\n",
       "      <td>[]</td>\n",
       "      <td>[v2/generate/audio/minimax/upload, v2/generate...</td>\n",
       "      <td>Music 01</td>\n",
       "      <td>Minimax AI</td>\n",
       "      <td>MiniMax Music generates high-quality music fro...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://aimlapi.com/models/minimax-music-api</td>\n",
       "      <td>https://docs.aimlapi.com/api-references/music-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>384</th>\n",
       "      <td>minimax/music-1.5</td>\n",
       "      <td>audio</td>\n",
       "      <td>[]</td>\n",
       "      <td>[v2/generate/audio/minimax/generate]</td>\n",
       "      <td>MiniMax Music 1.5</td>\n",
       "      <td>Minimax AI</td>\n",
       "      <td>MiniMax Music 1.5 provides enhanced music gene...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>385</th>\n",
       "      <td>minimax/music-2.0</td>\n",
       "      <td>audio</td>\n",
       "      <td>[]</td>\n",
       "      <td>[v2/generate/audio/minimax/generate]</td>\n",
       "      <td>MiniMax Music 2.0</td>\n",
       "      <td>Minimax AI</td>\n",
       "      <td>MiniMax Music 2.0 delivers superior music gene...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>386</th>\n",
       "      <td>elevenlabs/eleven_music</td>\n",
       "      <td>audio</td>\n",
       "      <td>[]</td>\n",
       "      <td>[/v2/generate/audio]</td>\n",
       "      <td>Eleven Music v1</td>\n",
       "      <td>Elevenlabs</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>https://docs.aimlapi.com/api-references/music-...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          id   type features  \\\n",
       "380             stable-audio  audio       []   \n",
       "381            minimax-music  audio       []   \n",
       "382            google/lyria2  audio       []   \n",
       "383                 music-01  audio       []   \n",
       "384        minimax/music-1.5  audio       []   \n",
       "385        minimax/music-2.0  audio       []   \n",
       "386  elevenlabs/eleven_music  audio       []   \n",
       "\n",
       "                                             endpoints               name  \\\n",
       "380                               [/v2/generate/audio]       Stable audio   \n",
       "381                               [/v2/generate/audio]      Minimax music   \n",
       "382                               [/v2/generate/audio]            Lyria 2   \n",
       "383  [v2/generate/audio/minimax/upload, v2/generate...           Music 01   \n",
       "384               [v2/generate/audio/minimax/generate]  MiniMax Music 1.5   \n",
       "385               [v2/generate/audio/minimax/generate]  MiniMax Music 2.0   \n",
       "386                               [/v2/generate/audio]    Eleven Music v1   \n",
       "\n",
       "        developer                                        description  \\\n",
       "380  Stability AI  Stable Audio generates high-quality audio from...   \n",
       "381    Minimax AI                                                      \n",
       "382        Google                                                      \n",
       "383    Minimax AI  MiniMax Music generates high-quality music fro...   \n",
       "384    Minimax AI  MiniMax Music 1.5 provides enhanced music gene...   \n",
       "385    Minimax AI  MiniMax Music 2.0 delivers superior music gene...   \n",
       "386    Elevenlabs                                                      \n",
       "\n",
       "     contextLength  maxTokens                                           url  \\\n",
       "380            NaN        NaN       https://aimlapi.com/models/stable-audio   \n",
       "381            NaN        NaN                                                 \n",
       "382            NaN        NaN                                                 \n",
       "383            NaN        NaN  https://aimlapi.com/models/minimax-music-api   \n",
       "384            NaN        NaN                                                 \n",
       "385            NaN        NaN                                                 \n",
       "386            NaN        NaN                                                 \n",
       "\n",
       "                                              docs_url  \n",
       "380  https://docs.aimlapi.com/api-references/music-...  \n",
       "381  https://docs.aimlapi.com/api-references/music-...  \n",
       "382  https://docs.aimlapi.com/api-references/music-...  \n",
       "383  https://docs.aimlapi.com/api-references/music-...  \n",
       "384                                                     \n",
       "385                                                     \n",
       "386  https://docs.aimlapi.com/api-references/music-...  "
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_models[380:420]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "fff20982-12ed-4644-8a28-1160134a57e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Aura is a real-time TTS model with human-like voices for conversational AI applications.'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_models.iloc[220][\"description\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85971702-7b39-47ce-9a77-7737a039e351",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
